{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"____ _ _ _ ____ _ _ | _ \\ __ _| |_ __ _| | __ _| | _____ / ___| ___ _ __(_)_ __ | |_ ___ | | | |/ _` | __/ _` | |/ _ ` | |/ / _ \\ \\___ \\ / __| '__| | '_ \\| __/ __| | |_| | (_| | || (_| | | (_| | < __/ ___) | (__| | | | |_) | |_\\__ \\ |____/ \\__,_|\\__\\__,_|_|\\__,_|_|\\_\\___| |____/ \\___|_| |_| .__/ \\__|___/ |_| datalake Datalake scripts How to use Installation With Python 3.6+: $ pip install datalake-scripts $ pip3 install datalake-scripts Using as a library see the following link Using as a CLI The cli can be used with: shell script $ ocd-dtl <command> <parameter> Check ocd-dtl -h for help, including the list of commands available. You can also use a script directly by using the following command: <script_name> <script_options> . /!\\ Make sure to use utf-8 without BOM when providing a file (-i option) Environment variables Authentication In case you don't want to enter credential for each commands and you are on a secured terminal, set those variables: OCD_DTL_USERNAME email address used to login on Datalake API/GUI. OCD_DTL_PASSWORD password used to login on Datalake API/GUI. They are independent and one can be used without the other if you wish. Throttling For throttling the request, those two environment variable can be used: OCD_DTL_QUOTA_TIME define, in seconds, the time before resetting the requests limit, default is 1 second . OCD_DTL_REQUESTS_PER_QUOTA_TIME define the number of request to do at maximum for the given time, default is 5 queries . Please don't exceed the quota marked here for each endpoint Cli parameters Parameters common and optional for all commands: --debug display more information for debugging purposes -e to change the environment {preprod, prod}, default is prod -o will set the output file as the API gives it. -q will quiet the verbosity of the program (but still show errors / warnings) For information about each command and more, please check the documentation directory","title":"Home"},{"location":"#datalake","text":"Datalake scripts","title":"datalake"},{"location":"#how-to-use","text":"","title":"How to use"},{"location":"#installation","text":"With Python 3.6+: $ pip install datalake-scripts $ pip3 install datalake-scripts","title":"Installation"},{"location":"#using-as-a-library","text":"see the following link","title":"Using as a library"},{"location":"#using-as-a-cli","text":"The cli can be used with: shell script $ ocd-dtl <command> <parameter> Check ocd-dtl -h for help, including the list of commands available. You can also use a script directly by using the following command: <script_name> <script_options> . /!\\ Make sure to use utf-8 without BOM when providing a file (-i option)","title":"Using as a CLI"},{"location":"#environment-variables","text":"","title":"Environment variables"},{"location":"#authentication","text":"In case you don't want to enter credential for each commands and you are on a secured terminal, set those variables: OCD_DTL_USERNAME email address used to login on Datalake API/GUI. OCD_DTL_PASSWORD password used to login on Datalake API/GUI. They are independent and one can be used without the other if you wish.","title":"Authentication"},{"location":"#throttling","text":"For throttling the request, those two environment variable can be used: OCD_DTL_QUOTA_TIME define, in seconds, the time before resetting the requests limit, default is 1 second . OCD_DTL_REQUESTS_PER_QUOTA_TIME define the number of request to do at maximum for the given time, default is 5 queries . Please don't exceed the quota marked here for each endpoint","title":"Throttling"},{"location":"#cli-parameters","text":"Parameters common and optional for all commands: --debug display more information for debugging purposes -e to change the environment {preprod, prod}, default is prod -o will set the output file as the API gives it. -q will quiet the verbosity of the program (but still show errors / warnings) For information about each command and more, please check the documentation directory","title":"Cli parameters"},{"location":"tutorial/","text":"____ _ _ _ | _ \\ __ _| |_ __ _| | __ _| | _____ | | | |/ _` | __/ _` | |/ _ ` | |/ / _ \\ | |_ | | (_| | || (_| | | (_| | < __/ |____/ \\__,_|\\__\\__,_|_|\\__,_|_|\\_\\___| Use this package as a Python library Using this library has multiple advantages, it first allows you to get started more quickly than by using the API directly. The library is also maintained directly by the developers of Datalake thus reducing the burden of keeping it compatible with the API over time. Finally, as it is open-source, you can reuse the functionalities developed by other Datalake users as well as helps improve this package further yourself. step 1: install datalake With Python 3.6+: $ pip install datalake-scripts or $ pip3 install datalake-scripts step 2: Create a Datalake instance You will need to create a Datalake instance once and reuse it: from datalake import Datalake dtl = Datalake ( username = 'username' , password = 'password' ) The credentials can be omitted and will then be asked in a prompt. You can also set them in your os environment variables: OCD_DTL_USERNAME email address used to login on Datalake API/GUI. OCD_DTL_PASSWORD password used to login on Datalake API/GUI. Usage: Code Sample Below are some examples to get you started Lookup a threat Bulk look up Bulk search Add a threat (with all details) Bulk add threats at once (atom values only) Add tags Edit score Advanced Search Sightings Lookup a threat from datalake import Datalake , AtomType , Output dtl = Datalake ( username = 'username' , password = 'password' ) dtl . Threats . lookup ( atom_value = 'mayoclinic.org' , atom_type = AtomType . DOMAIN , hashkey_only = False , output = Output . JSON ) Note that only the atom_value is required: dtl.Threats.lookup('mayoclinic.org') Bulk look up Compared to the lookup, the bulk_lookup method allows to lookup big batch of values faster as fewer API calls are made. However, fewer outputs types are supported (only json and csv as of now). from datalake import Datalake , AtomType , Output dtl = Datalake ( username = 'username' , password = 'password' ) threats = [ 'mayoclinic.org' , 'commentcamarche.net' , 'gawker.com' ] dtl . Threats . bulk_lookup ( atom_values = threats , atom_type = AtomType . DOMAIN , hashkey_only = False , output = Output . CSV , return_search_hashkey = False ) Bulk search A convenient download_sync method is provided: task = dtl . BulkSearch . create_task ( query_hash = '<some query hash>' ) csv = task . download_sync ( output = Output . CSV ) The following Output format are available: JSON JSON_ZIP CSV CSV_ZIP STIX STIX_ZIP The STIX and STIX_ZIP format are only available if when creating the task it is specified that it is for stix export, using the for_stix_export parameter task = dtl . BulkSearch . create_task ( for_stix_export = True , query_hash = '<some query hash>' ) stix = task . download_sync ( output = Output . STIX ) Note download_sync accepts a stream=True parameter that if passed change the return of the function. It is no longer the plain response body but the Response object from the requests library. This allow to retrieve the plain body as a stream. task.download_sync_stream_to_file('<absolute output path>', output=Output.JSON) is a helper function that do just that, storing the output in a file while keeping the RAM usage low and independent of the size of the bulksearch result. Depending of your use case, you can call an async version to parallelize the wait of bulk search for example: import asyncio from datalake import Datalake , Output dtl = Datalake ( username = 'username' , password = 'password' ) # Queuing multiple bulk searches at once saves a lot of time # However you will receive HTTP 400 error if you try to enqueue too many bulk search at once (more than 10) query_hashes_to_process = [ '7018d41944b71b04a9d3785b3741c842' , '207d02c81edde3c87f665451f04f9bd1' , '9f7a8fecb0a74e508d6873c4d6e0d614' , '8bd8f1b47ce1a76ac2a1dc9e91aa9a5e' , 'd3f8e2006554aaffa554714c614acd30' , ] coroutines = [] for query_hash in query_hashes_to_process : task = dtl . BulkSearch . create_task ( query_hash = query_hash ) coroutines . append ( task . download_async ( output = Output . JSON )) loop = asyncio . get_event_loop () future = asyncio . gather ( * coroutines ) results = loop . run_until_complete ( future ) result_per_query_hash = {} # Since results keep its order, we can easily attach back query_hash to its result for query_hash , result in zip ( query_hashes_to_process , results ): result_per_query_hash [ query_hash ] = result print ( result_per_query_hash ) # will output: { \"query_hash_1\" : { \"result of query_hash 1\" }, \"query_hash_2\" : { \"result of query_hash 2\" }, ... } Add a threat (with all details) You can call the add_threat function to add a single threat at a time and retrieve details from the newly submitted threat. from datalake import Datalake from datalake import ThreatType , OverrideType , IpAtom , EmailAtom , FileAtom , Hashes , EmailFlow , IpService dtl = Datalake ( username = 'username' , password = 'password' ) # Adding empty file hashes = Hashes ( md5 = 'd41d8cd98f00b204e9800998ecf8427e' , sha1 = 'da39a3ee5e6b4b0d3255bfef95601890afd80709' , sha256 = 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' ) empty_file = FileAtom ( hashes = hashes , filesize = 0 , filetype = 'txt' , filename = 'empty.txt' , external_analysis_link = [ 'https://www.computerhope.com/issues/ch001314.htm' ]) dtl . Threats . add_threat ( atom = empty_file , threat_types = [{ 'threat_type' : ThreatType . MALWARE , 'score' : 0 }], override_type = OverrideType . TEMPORARY , public = True , tags = [ 'empty_file' ]) # Adding Google DNS IP dns_service = IpService ( port = 53 , service_name = 'dns' , application = 'dns' , protocol = 'UDP' ) google_dns_ip = IpAtom ( ip_address = '52.48.79.33' , external_analysis_link = [ 'https://www.virustotal.com/gui/ip-address/8.8.8.8' ], ip_version = 4 , services = [ dns_service ], owner = 'Google' ) dtl . Threats . add_threat ( atom = google_dns_ip , threat_types = [{ 'threat_type' : ThreatType . MALWARE , 'score' : 0 }], override_type = OverrideType . TEMPORARY , public = True , tags = [ 'google_dns' ]) # Addding e-mail my_email = EmailAtom ( email = 'noreply@orangecyberdefense.com' , email_flow = EmailFlow . FROM , external_analysis_link = [ 'https://www.orangecyberdefense.com' ]) dtl . Threats . add_threat ( atom = my_email , threat_types = [{ 'threat_type' : ThreatType . SPAM , 'score' : 0 }], override_type = OverrideType . TEMPORARY , whitelist = True , public = True , tags = [ 'ocd' ]) The following positional arguments are required: atom : an instance of an Atom class, for example IpAtom The following keyword arguments are available: threat_types : A list of dictionaries containing a key named threat_type with a ThreatType value and a key named score with an integer value between 0 and 100 . Available ThreatType options are: DDOS, FRAUD, HACK, LEAK, MALWARE, PHISHING, SCAM, SCAN, SPAM . Defaults to None . override_type : an OverrideType. Available options are: PERMANENT : All values will override any values provided by both newer and older IOCs. Newer IOCs with override_type permanent can still override old permanent changes. TEMPORARY : All values should override any values provided by older IOCs, but not newer ones. LOCK : Will act like a permanent for three months, then like a temporary. whitelist : A boolean, if no threat_types are provided, this argument should be set to true. All score values will then be set to 0. If threat_types are provided along with whitelist set as True , will result in an error. Defaults to False . public : A boolean, sets whether the threats should be public or private. Defaults to True . tags : a List of strings. Will set the tags of the added threat(s). Bulk add threats at once (atom values only) You can call the add_threats function to add threats in bulk but you wil b from datalake import Datalake , ThreatType , OverrideType , AtomType dtl = Datalake ( username = 'username' , password = 'password' ) atom_list = [ '12.34.56.78' , '9.8.7.6' ] threat_types = [{ 'threat_type' : ThreatType . DDOS , 'score' : 20 }] dtl . Threats . add_threats ( atom_list , AtomType . IP , threat_types , OverrideType . TEMPORARY , external_analysis_link = [ 'https://someurl.com' ], tags = [ 'some_tag' ], public = False ) The following positional arguments are required: atom_list : a List of strings. Contains the list of threats to add. In our example it's a list of IPs. atom_type : an AtomType. Available options are: APK, AS, CC, CRYPTO, CVE, DOMAIN, EMAIL, FILE, FQDN, IBAN, IP, IP_RANGE, PATE, PHONE_NUMBER, REGKEY, SSL, URL The following keyword arguments are available: threat_types : A list of dictionaries containing a key named threat_type with a ThreatType value and a key named score with an integer value between 0 and 100 . Available ThreatType options are: DDOS, FRAUD, HACK, LEAK, MALWARE, PHISHING, SCAM, SCAN, SPAM . Defaults to None . override_type : an OverrideType. Available options are: PERMANENT : All values will override any values provided by both newer and older IOCs. Newer IOCs with override_type permanent can still override old permanent changes. TEMPORARY : All values should override any values provided by older IOCs, but not newer ones. LOCK : Will act like a permanent for three months, then like a temporary. whitelist : A boolean, if no threat_types are provided, this argument should be set to true. All score values will then be set to 0. If threat_types are provided along with whitelist set as True , will result in an error. Defaults to False . public : A boolean, sets whether the threats should be public or private. Defaults to True . tags : a List of strings. Will set the tags of the added threat(s). external_analysis_link : a List of strings. A link to an external resource providing more information about the threat. Add tags A quick and easy way to add tags to a threat from datalake import Datalake dtl = Datalake ( username = 'username' , password = 'password' ) hashkey = '00000001655688982ec8ba4058f02dd1' tags = [ 'green' , 'white' ] public = False dtl . Tags . add_to_threat ( hashkey , tags , public ) Edit score Mutliple threats can be edited at once, each threat type independently: from datalake import Datalake , ThreatType , OverrideType dtl = Datalake ( username = 'username' , password = 'password' ) hashkeys = [ '00000001655688982ec8ba4058f02dd1' , '00000001655688982ec8ba4058f02dd2' , ] threat_scores_list = [ { 'threat_type' : ThreatType . DDOS , 'score' : 5 }, { 'threat_type' : ThreatType . PHISHING , 'score' : 25 }, ] override_type = OverrideType . TEMPORARY dtl . Threats . edit_score_by_hashkeys ( hashkeys , threat_scores_list , override_type ) Query hashes can also be used with another function provided for that use: from datalake import Datalake , ThreatType , OverrideType dtl = Datalake ( username = 'username' , password = 'password' ) query_body_hash = '7018d41944b71b04a9d3785b3741c842' threat_scores_list = [ { 'threat_type' : ThreatType . DDOS , 'score' : 5 }, { 'threat_type' : ThreatType . PHISHING , 'score' : 25 }, ] override_type = OverrideType . TEMPORARY dtl . Threats . edit_score_by_query_body_hash ( query_body_hash , threat_scores_list , override_type ) Advanced Search The library can be used to execute advanced search if you have a query hash or a query to body, using advanced_search_from_query_hash or advanced_search_from_query_body . from datalake import Datalake , Output dtl = Datalake ( username = 'username' , password = 'password' ) query_body = { \"AND\" : [ { \"AND\" : [ { \"field\" : \"atom_type\" , \"multi_values\" : [ \"ip\" ], \"type\" : \"filter\" }, { \"field\" : \"risk\" , \"range\" : { \"gt\" : 60 }, \"type\" : \"filter\" } ] } ] } query_hash = 'cece3117abc823cee81e69c2143e6268' adv_search_hash_resp = dtl . AdvancedSearch . advanced_search_from_query_hash ( query_hash , limit = 20 , offset = 0 , ordering = [ 'first_seen' ], output = Output . JSON ) adv_search_body_resp = dtl . AdvancedSearch . advanced_search_from_query_body ( query_body , limit = 20 , offset = 0 , ordering = [ '-first_seen' ], output = Output . JSON ) Sightings Sightings can be submitted using the library using a list of atoms: from datalake import Datalake , IpAtom , EmailAtom , UrlAtom , FileAtom , Hashes , SightingType , Visibility , ThreatType import datetime dtl = Datalake ( username = 'username' , password = 'password' ) # building atoms hashes = Hashes ( md5 = 'd41d8cd98f00b204e9800998ecf8427e' , sha1 = 'da39a3ee5e6b4b0d3255bfef95601890afd80709' , sha256 = 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' ) f1 = FileAtom ( hashes = hashes ) ip1 = IpAtom ( '52.48.79.33' ) em1 = EmailAtom ( 'hacker@hacker.ha' ) url1 = UrlAtom ( 'http://notfishing.com' ) threat_types = [ ThreatType . PHISHING , ThreatType . SCAM ] # building sighting timestamps start = datetime . datetime . utcnow () - datetime . timedelta ( hours = 1 ) end = datetime . datetime . utcnow () # submit sighting dtl . Sightings . submit_sighting ( start_timestamp = start , end_timestamp = end , sighting_type = SightingType . POSITIVE , visibility = Visibility . PUBLIC , count = 1 , threat_types = threat_types , atoms = [ ip1 , f1 , em1 , url1 ], tags = [ 'some_tag' ], description = 'some_description' ) Or using a list of hashkeys: from datalake import Datalake , SightingType , Visibility , ThreatType import datetime threat_types = [ ThreatType . PHISHING , ThreatType . SCAM ] start = datetime . datetime . utcnow () - datetime . timedelta ( hours = 1 ) end = datetime . datetime . utcnow () dtl = Datalake ( username = 'username' , password = 'password' ) resp = dtl . Sightings . submit_sighting ( start , end , SightingType . POSITIVE , Visibility . PUBLIC , 1 , threat_types , hashkeys = [ 'mythreathashkeys' ]) The atom_type file provides multiple classes to build each type of atom type used by the API. The classes will provide you with hints on the value expected for each atom_type, most of which aren't mandatory. For sightings, we won't use most of the fields. You can verify the fields that are used for sighting in the docstrings of each class, inside your editor. API documentation For more information on the API used by this library, see the documentation","title":"Tutorial"},{"location":"tutorial/#use-this-package-as-a-python-library","text":"Using this library has multiple advantages, it first allows you to get started more quickly than by using the API directly. The library is also maintained directly by the developers of Datalake thus reducing the burden of keeping it compatible with the API over time. Finally, as it is open-source, you can reuse the functionalities developed by other Datalake users as well as helps improve this package further yourself.","title":"Use this package as a Python library"},{"location":"tutorial/#step-1-install-datalake","text":"With Python 3.6+: $ pip install datalake-scripts or $ pip3 install datalake-scripts","title":"step 1: install datalake"},{"location":"tutorial/#step-2-create-a-datalake-instance","text":"You will need to create a Datalake instance once and reuse it: from datalake import Datalake dtl = Datalake ( username = 'username' , password = 'password' ) The credentials can be omitted and will then be asked in a prompt. You can also set them in your os environment variables: OCD_DTL_USERNAME email address used to login on Datalake API/GUI. OCD_DTL_PASSWORD password used to login on Datalake API/GUI.","title":"step 2: Create a Datalake instance"},{"location":"tutorial/#usage-code-sample","text":"Below are some examples to get you started Lookup a threat Bulk look up Bulk search Add a threat (with all details) Bulk add threats at once (atom values only) Add tags Edit score Advanced Search Sightings","title":"Usage: Code Sample"},{"location":"tutorial/#lookup-a-threat","text":"from datalake import Datalake , AtomType , Output dtl = Datalake ( username = 'username' , password = 'password' ) dtl . Threats . lookup ( atom_value = 'mayoclinic.org' , atom_type = AtomType . DOMAIN , hashkey_only = False , output = Output . JSON ) Note that only the atom_value is required: dtl.Threats.lookup('mayoclinic.org')","title":"Lookup a threat"},{"location":"tutorial/#bulk-look-up","text":"Compared to the lookup, the bulk_lookup method allows to lookup big batch of values faster as fewer API calls are made. However, fewer outputs types are supported (only json and csv as of now). from datalake import Datalake , AtomType , Output dtl = Datalake ( username = 'username' , password = 'password' ) threats = [ 'mayoclinic.org' , 'commentcamarche.net' , 'gawker.com' ] dtl . Threats . bulk_lookup ( atom_values = threats , atom_type = AtomType . DOMAIN , hashkey_only = False , output = Output . CSV , return_search_hashkey = False )","title":"Bulk look up"},{"location":"tutorial/#bulk-search","text":"A convenient download_sync method is provided: task = dtl . BulkSearch . create_task ( query_hash = '<some query hash>' ) csv = task . download_sync ( output = Output . CSV ) The following Output format are available: JSON JSON_ZIP CSV CSV_ZIP STIX STIX_ZIP The STIX and STIX_ZIP format are only available if when creating the task it is specified that it is for stix export, using the for_stix_export parameter task = dtl . BulkSearch . create_task ( for_stix_export = True , query_hash = '<some query hash>' ) stix = task . download_sync ( output = Output . STIX ) Note download_sync accepts a stream=True parameter that if passed change the return of the function. It is no longer the plain response body but the Response object from the requests library. This allow to retrieve the plain body as a stream. task.download_sync_stream_to_file('<absolute output path>', output=Output.JSON) is a helper function that do just that, storing the output in a file while keeping the RAM usage low and independent of the size of the bulksearch result. Depending of your use case, you can call an async version to parallelize the wait of bulk search for example: import asyncio from datalake import Datalake , Output dtl = Datalake ( username = 'username' , password = 'password' ) # Queuing multiple bulk searches at once saves a lot of time # However you will receive HTTP 400 error if you try to enqueue too many bulk search at once (more than 10) query_hashes_to_process = [ '7018d41944b71b04a9d3785b3741c842' , '207d02c81edde3c87f665451f04f9bd1' , '9f7a8fecb0a74e508d6873c4d6e0d614' , '8bd8f1b47ce1a76ac2a1dc9e91aa9a5e' , 'd3f8e2006554aaffa554714c614acd30' , ] coroutines = [] for query_hash in query_hashes_to_process : task = dtl . BulkSearch . create_task ( query_hash = query_hash ) coroutines . append ( task . download_async ( output = Output . JSON )) loop = asyncio . get_event_loop () future = asyncio . gather ( * coroutines ) results = loop . run_until_complete ( future ) result_per_query_hash = {} # Since results keep its order, we can easily attach back query_hash to its result for query_hash , result in zip ( query_hashes_to_process , results ): result_per_query_hash [ query_hash ] = result print ( result_per_query_hash ) # will output: { \"query_hash_1\" : { \"result of query_hash 1\" }, \"query_hash_2\" : { \"result of query_hash 2\" }, ... }","title":"Bulk search"},{"location":"tutorial/#add-a-threat-with-all-details","text":"You can call the add_threat function to add a single threat at a time and retrieve details from the newly submitted threat. from datalake import Datalake from datalake import ThreatType , OverrideType , IpAtom , EmailAtom , FileAtom , Hashes , EmailFlow , IpService dtl = Datalake ( username = 'username' , password = 'password' ) # Adding empty file hashes = Hashes ( md5 = 'd41d8cd98f00b204e9800998ecf8427e' , sha1 = 'da39a3ee5e6b4b0d3255bfef95601890afd80709' , sha256 = 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' ) empty_file = FileAtom ( hashes = hashes , filesize = 0 , filetype = 'txt' , filename = 'empty.txt' , external_analysis_link = [ 'https://www.computerhope.com/issues/ch001314.htm' ]) dtl . Threats . add_threat ( atom = empty_file , threat_types = [{ 'threat_type' : ThreatType . MALWARE , 'score' : 0 }], override_type = OverrideType . TEMPORARY , public = True , tags = [ 'empty_file' ]) # Adding Google DNS IP dns_service = IpService ( port = 53 , service_name = 'dns' , application = 'dns' , protocol = 'UDP' ) google_dns_ip = IpAtom ( ip_address = '52.48.79.33' , external_analysis_link = [ 'https://www.virustotal.com/gui/ip-address/8.8.8.8' ], ip_version = 4 , services = [ dns_service ], owner = 'Google' ) dtl . Threats . add_threat ( atom = google_dns_ip , threat_types = [{ 'threat_type' : ThreatType . MALWARE , 'score' : 0 }], override_type = OverrideType . TEMPORARY , public = True , tags = [ 'google_dns' ]) # Addding e-mail my_email = EmailAtom ( email = 'noreply@orangecyberdefense.com' , email_flow = EmailFlow . FROM , external_analysis_link = [ 'https://www.orangecyberdefense.com' ]) dtl . Threats . add_threat ( atom = my_email , threat_types = [{ 'threat_type' : ThreatType . SPAM , 'score' : 0 }], override_type = OverrideType . TEMPORARY , whitelist = True , public = True , tags = [ 'ocd' ]) The following positional arguments are required: atom : an instance of an Atom class, for example IpAtom The following keyword arguments are available: threat_types : A list of dictionaries containing a key named threat_type with a ThreatType value and a key named score with an integer value between 0 and 100 . Available ThreatType options are: DDOS, FRAUD, HACK, LEAK, MALWARE, PHISHING, SCAM, SCAN, SPAM . Defaults to None . override_type : an OverrideType. Available options are: PERMANENT : All values will override any values provided by both newer and older IOCs. Newer IOCs with override_type permanent can still override old permanent changes. TEMPORARY : All values should override any values provided by older IOCs, but not newer ones. LOCK : Will act like a permanent for three months, then like a temporary. whitelist : A boolean, if no threat_types are provided, this argument should be set to true. All score values will then be set to 0. If threat_types are provided along with whitelist set as True , will result in an error. Defaults to False . public : A boolean, sets whether the threats should be public or private. Defaults to True . tags : a List of strings. Will set the tags of the added threat(s).","title":"Add a threat (with all details)"},{"location":"tutorial/#bulk-add-threats-at-once-atom-values-only","text":"You can call the add_threats function to add threats in bulk but you wil b from datalake import Datalake , ThreatType , OverrideType , AtomType dtl = Datalake ( username = 'username' , password = 'password' ) atom_list = [ '12.34.56.78' , '9.8.7.6' ] threat_types = [{ 'threat_type' : ThreatType . DDOS , 'score' : 20 }] dtl . Threats . add_threats ( atom_list , AtomType . IP , threat_types , OverrideType . TEMPORARY , external_analysis_link = [ 'https://someurl.com' ], tags = [ 'some_tag' ], public = False ) The following positional arguments are required: atom_list : a List of strings. Contains the list of threats to add. In our example it's a list of IPs. atom_type : an AtomType. Available options are: APK, AS, CC, CRYPTO, CVE, DOMAIN, EMAIL, FILE, FQDN, IBAN, IP, IP_RANGE, PATE, PHONE_NUMBER, REGKEY, SSL, URL The following keyword arguments are available: threat_types : A list of dictionaries containing a key named threat_type with a ThreatType value and a key named score with an integer value between 0 and 100 . Available ThreatType options are: DDOS, FRAUD, HACK, LEAK, MALWARE, PHISHING, SCAM, SCAN, SPAM . Defaults to None . override_type : an OverrideType. Available options are: PERMANENT : All values will override any values provided by both newer and older IOCs. Newer IOCs with override_type permanent can still override old permanent changes. TEMPORARY : All values should override any values provided by older IOCs, but not newer ones. LOCK : Will act like a permanent for three months, then like a temporary. whitelist : A boolean, if no threat_types are provided, this argument should be set to true. All score values will then be set to 0. If threat_types are provided along with whitelist set as True , will result in an error. Defaults to False . public : A boolean, sets whether the threats should be public or private. Defaults to True . tags : a List of strings. Will set the tags of the added threat(s). external_analysis_link : a List of strings. A link to an external resource providing more information about the threat.","title":"Bulk add threats at once (atom values only)"},{"location":"tutorial/#add-tags","text":"A quick and easy way to add tags to a threat from datalake import Datalake dtl = Datalake ( username = 'username' , password = 'password' ) hashkey = '00000001655688982ec8ba4058f02dd1' tags = [ 'green' , 'white' ] public = False dtl . Tags . add_to_threat ( hashkey , tags , public )","title":"Add tags"},{"location":"tutorial/#edit-score","text":"Mutliple threats can be edited at once, each threat type independently: from datalake import Datalake , ThreatType , OverrideType dtl = Datalake ( username = 'username' , password = 'password' ) hashkeys = [ '00000001655688982ec8ba4058f02dd1' , '00000001655688982ec8ba4058f02dd2' , ] threat_scores_list = [ { 'threat_type' : ThreatType . DDOS , 'score' : 5 }, { 'threat_type' : ThreatType . PHISHING , 'score' : 25 }, ] override_type = OverrideType . TEMPORARY dtl . Threats . edit_score_by_hashkeys ( hashkeys , threat_scores_list , override_type ) Query hashes can also be used with another function provided for that use: from datalake import Datalake , ThreatType , OverrideType dtl = Datalake ( username = 'username' , password = 'password' ) query_body_hash = '7018d41944b71b04a9d3785b3741c842' threat_scores_list = [ { 'threat_type' : ThreatType . DDOS , 'score' : 5 }, { 'threat_type' : ThreatType . PHISHING , 'score' : 25 }, ] override_type = OverrideType . TEMPORARY dtl . Threats . edit_score_by_query_body_hash ( query_body_hash , threat_scores_list , override_type )","title":"Edit score"},{"location":"tutorial/#advanced-search","text":"The library can be used to execute advanced search if you have a query hash or a query to body, using advanced_search_from_query_hash or advanced_search_from_query_body . from datalake import Datalake , Output dtl = Datalake ( username = 'username' , password = 'password' ) query_body = { \"AND\" : [ { \"AND\" : [ { \"field\" : \"atom_type\" , \"multi_values\" : [ \"ip\" ], \"type\" : \"filter\" }, { \"field\" : \"risk\" , \"range\" : { \"gt\" : 60 }, \"type\" : \"filter\" } ] } ] } query_hash = 'cece3117abc823cee81e69c2143e6268' adv_search_hash_resp = dtl . AdvancedSearch . advanced_search_from_query_hash ( query_hash , limit = 20 , offset = 0 , ordering = [ 'first_seen' ], output = Output . JSON ) adv_search_body_resp = dtl . AdvancedSearch . advanced_search_from_query_body ( query_body , limit = 20 , offset = 0 , ordering = [ '-first_seen' ], output = Output . JSON )","title":"Advanced Search"},{"location":"tutorial/#sightings","text":"Sightings can be submitted using the library using a list of atoms: from datalake import Datalake , IpAtom , EmailAtom , UrlAtom , FileAtom , Hashes , SightingType , Visibility , ThreatType import datetime dtl = Datalake ( username = 'username' , password = 'password' ) # building atoms hashes = Hashes ( md5 = 'd41d8cd98f00b204e9800998ecf8427e' , sha1 = 'da39a3ee5e6b4b0d3255bfef95601890afd80709' , sha256 = 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' ) f1 = FileAtom ( hashes = hashes ) ip1 = IpAtom ( '52.48.79.33' ) em1 = EmailAtom ( 'hacker@hacker.ha' ) url1 = UrlAtom ( 'http://notfishing.com' ) threat_types = [ ThreatType . PHISHING , ThreatType . SCAM ] # building sighting timestamps start = datetime . datetime . utcnow () - datetime . timedelta ( hours = 1 ) end = datetime . datetime . utcnow () # submit sighting dtl . Sightings . submit_sighting ( start_timestamp = start , end_timestamp = end , sighting_type = SightingType . POSITIVE , visibility = Visibility . PUBLIC , count = 1 , threat_types = threat_types , atoms = [ ip1 , f1 , em1 , url1 ], tags = [ 'some_tag' ], description = 'some_description' ) Or using a list of hashkeys: from datalake import Datalake , SightingType , Visibility , ThreatType import datetime threat_types = [ ThreatType . PHISHING , ThreatType . SCAM ] start = datetime . datetime . utcnow () - datetime . timedelta ( hours = 1 ) end = datetime . datetime . utcnow () dtl = Datalake ( username = 'username' , password = 'password' ) resp = dtl . Sightings . submit_sighting ( start , end , SightingType . POSITIVE , Visibility . PUBLIC , 1 , threat_types , hashkeys = [ 'mythreathashkeys' ]) The atom_type file provides multiple classes to build each type of atom type used by the API. The classes will provide you with hints on the value expected for each atom_type, most of which aren't mandatory. For sightings, we won't use most of the fields. You can verify the fields that are used for sighting in the docstrings of each class, inside your editor.","title":"Sightings"},{"location":"tutorial/#api-documentation","text":"For more information on the API used by this library, see the documentation","title":"API documentation"},{"location":"docs/Add_threats/","text":"Add new threats Examples To add new threats from file. From TXT file: 100.100.100.1 100.100.100.2 100.100.100.3 100.100.100.4 100.100.100.5 100.100.100.6 From CSV file: 2012 - 10 - 01 , 100.100.100.1 , random comments 2012 - 10 - 01 , 100.100.100.2 , random comments 2012 - 10 - 01 , 100.100.100.3 , random comments 2012 - 10 - 01 , 100.100.100.4 , random comments 2012 - 10 - 01 , 100.100.100.5 , random comments 2012 - 10 - 01 , 100.100.100.6 , random comments To create threats: ocd-dtl add_threats -o output_file.json -t ddos 50 scam 15 -i ip_list.txt -a IP --tag test0 test32 test320 ocd-dtl add_threats -o output_file.json -t ddos 50 scam 15 -i ip_list.csv -a IP --tag test0 test32 test320 --is_csv -d , -c 1 Parameters Required: * -i : indicate the file path * -a : indicate the atom type, in our example it is IP Use either: * -t : will set the score for the threat to the corresponding value. In our example we set ddos = 50 and scam = 15 . Can be ignored if the next paramaeter, -w , is set. * -w : If -t is not set this flag is required. Will set all the scores to 0 to whitelist the threat. Optional flags: * --no-bulk force an api call for each threats, useful to retrieve the details of the threats created. If a large number of threats has to be created and this flag is set, there is a risk of failure. Please make sure to use this flag only when creating a limited number of threats. * --tag will add all the following tags to the new threats, in our example test0 test32 test320 -p set the visibility to public. Default is private --is_csv flag is required if your input file is a csv file. -d to specify a custom delimiter -c to select the starting column (starting at 1 ) --link to provide a link i.e. a URL that will be filled in \"external_analysis_link\" --permanent to set override_type to permanent. For scores that should not be updated by the algorithm -o will set the output file as the API gives it . -e to change the environment {preprod, prod}, default is prod * Currently the result outputted with -o depends on the API endpoints call: * If --no-bulk isn't set, the output file will contain a json with the hashkey and the value of the threats created. If some threats failed to be created, the value and the hashkey will be recorded in the json. * If --no-bulk is set, the output file will contain a json with all the details about the created threats. Environment variables For the bulk mode, the following environment variable can be used OCD_DTL_MAX_BACK_OFF_TIME allow to set the maximum time period to wait between two api calls to check if the bulk submission is complete. default is 120 seconds . OCD_DTL_MAX_BULK_THREATS_TIME is the maximum time period, in seconds, to wait for the manual submission to be processed, after which the threats will be considered not successfully added. default is 600 seconds . OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT is the maximum bulk requests made in parallel, increasing this value may result in your personal queue limit to be reached. default is 10 .","title":"Add Threats"},{"location":"docs/Add_threats/#add-new-threats","text":"","title":"Add new threats"},{"location":"docs/Add_threats/#examples","text":"To add new threats from file. From TXT file: 100.100.100.1 100.100.100.2 100.100.100.3 100.100.100.4 100.100.100.5 100.100.100.6 From CSV file: 2012 - 10 - 01 , 100.100.100.1 , random comments 2012 - 10 - 01 , 100.100.100.2 , random comments 2012 - 10 - 01 , 100.100.100.3 , random comments 2012 - 10 - 01 , 100.100.100.4 , random comments 2012 - 10 - 01 , 100.100.100.5 , random comments 2012 - 10 - 01 , 100.100.100.6 , random comments To create threats: ocd-dtl add_threats -o output_file.json -t ddos 50 scam 15 -i ip_list.txt -a IP --tag test0 test32 test320 ocd-dtl add_threats -o output_file.json -t ddos 50 scam 15 -i ip_list.csv -a IP --tag test0 test32 test320 --is_csv -d , -c 1","title":"Examples"},{"location":"docs/Add_threats/#parameters","text":"Required: * -i : indicate the file path * -a : indicate the atom type, in our example it is IP Use either: * -t : will set the score for the threat to the corresponding value. In our example we set ddos = 50 and scam = 15 . Can be ignored if the next paramaeter, -w , is set. * -w : If -t is not set this flag is required. Will set all the scores to 0 to whitelist the threat. Optional flags: * --no-bulk force an api call for each threats, useful to retrieve the details of the threats created. If a large number of threats has to be created and this flag is set, there is a risk of failure. Please make sure to use this flag only when creating a limited number of threats. * --tag will add all the following tags to the new threats, in our example test0 test32 test320 -p set the visibility to public. Default is private --is_csv flag is required if your input file is a csv file. -d to specify a custom delimiter -c to select the starting column (starting at 1 ) --link to provide a link i.e. a URL that will be filled in \"external_analysis_link\" --permanent to set override_type to permanent. For scores that should not be updated by the algorithm -o will set the output file as the API gives it . -e to change the environment {preprod, prod}, default is prod * Currently the result outputted with -o depends on the API endpoints call: * If --no-bulk isn't set, the output file will contain a json with the hashkey and the value of the threats created. If some threats failed to be created, the value and the hashkey will be recorded in the json. * If --no-bulk is set, the output file will contain a json with all the details about the created threats.","title":"Parameters"},{"location":"docs/Add_threats/#environment-variables","text":"For the bulk mode, the following environment variable can be used OCD_DTL_MAX_BACK_OFF_TIME allow to set the maximum time period to wait between two api calls to check if the bulk submission is complete. default is 120 seconds . OCD_DTL_MAX_BULK_THREATS_TIME is the maximum time period, in seconds, to wait for the manual submission to be processed, after which the threats will be considered not successfully added. default is 600 seconds . OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT is the maximum bulk requests made in parallel, increasing this value may result in your personal queue limit to be reached. default is 10 .","title":"Environment variables"},{"location":"docs/Bulk_lookup_threats/","text":"Returns whether threats are in the database or not Using input files This command can read one or multiple files at the same time as atom input. To pass them, using -i or --input argument followed by the relative or absolute path. You can pass files as typed or untyped, it means that you will indicate which kind of atoms are inside the files. * typed files : <ATOM_TYPE>:<FILE_PATH> Here, the command will expect that each line represents a single atom of given type. For example, the following file contains only domains. threat1.com anotherthreat.fr en.wikipedia untyped files : <FILE_PATH> Here, the command will determinate line by line the atom type. For example, the following file contains multiple atom types. discover.me f0f5d30e91006c8ca7a528f26432ftt5 113.223.40.103 ```shell $ ocd-dtl bulk_lookup_threats -i ip:path/to/file/myiplist.txt -i file:path/to/file/myfilelist.txt --------- ip typed --------- --------- file typed ----------- $ ocd-dtl bulk_lookup_threats -i path/to/file/myatomlist.txt --------- untyped --------- ``` Passing atoms by CLI As input files, you can pass one or multiple typed or untyped atoms by CLI. typed atoms : Each atom has its own flag called exactly the same as the atom type. shell $ ocd-dtl bulk_lookup_threats --ip 113.223.40.103 --ip 5.78.23.158 --domain reverso.net --file f0f5d30e91006c8ca7a528f26432ftt5 untyped atoms : If you don't know the atom type, pass them as positional arguments, and the command will find out its type. shell $ ocd-dtl bulk_lookup_threats 113.223.40.103 5.78.23.158 reverso.net f0f5d30e91006c8ca7a528f26432ftt5 With all that in mind, you can combine them to fit your needs: $ ocd-dtl bulk_lookup_threats reverso.net 113 .223.40.103 --domain paiza.com --ip 45 .96.65.132 -i ip:path/to/file/myiplist.txt -i path/to/file/myfilelist.txt ----untyped domain ip----- ---typed domain--- ----typed ip---- -------typed file as ip-------- ---------untyped file--------- Parameters Optional arguments > -h, --help show this help message and exit -o [OUTPUT], --output [OUTPUT] file path from script -ot OUTPUT_TYPE, --output-type OUTPUT_TYPE set to the output type desired {json,csv}. Default is json -e {prod,preprod}, --env {prod,preprod} execute on specified environment (Default: prod) --debug -q, --quiet Silence the output to only show warnings/errors -ad, --atom-details returns threats full details -i INPUT, --input INPUT read threats to add from FILE. Typed atom flags > --apk APK --asn ASN --cc CC --crypto CRYPTO --cve CVE --domain DOMAIN --email EMAIL --file FILE --fqdn FQDN --iban IBAN --ip IP --ip_range IP_RANGE --paste PASTE --phone_number PHONE_NUMBER --regkey REGKEY --ssl SSL --url URL","title":"Bulk Lookup Threats"},{"location":"docs/Bulk_lookup_threats/#returns-whether-threats-are-in-the-database-or-not","text":"","title":"Returns whether threats are in the database or not"},{"location":"docs/Bulk_lookup_threats/#using-input-files","text":"This command can read one or multiple files at the same time as atom input. To pass them, using -i or --input argument followed by the relative or absolute path. You can pass files as typed or untyped, it means that you will indicate which kind of atoms are inside the files. * typed files : <ATOM_TYPE>:<FILE_PATH> Here, the command will expect that each line represents a single atom of given type. For example, the following file contains only domains. threat1.com anotherthreat.fr en.wikipedia untyped files : <FILE_PATH> Here, the command will determinate line by line the atom type. For example, the following file contains multiple atom types. discover.me f0f5d30e91006c8ca7a528f26432ftt5 113.223.40.103 ```shell $ ocd-dtl bulk_lookup_threats -i ip:path/to/file/myiplist.txt -i file:path/to/file/myfilelist.txt --------- ip typed --------- --------- file typed ----------- $ ocd-dtl bulk_lookup_threats -i path/to/file/myatomlist.txt --------- untyped --------- ```","title":"Using input files"},{"location":"docs/Bulk_lookup_threats/#passing-atoms-by-cli","text":"As input files, you can pass one or multiple typed or untyped atoms by CLI. typed atoms : Each atom has its own flag called exactly the same as the atom type. shell $ ocd-dtl bulk_lookup_threats --ip 113.223.40.103 --ip 5.78.23.158 --domain reverso.net --file f0f5d30e91006c8ca7a528f26432ftt5 untyped atoms : If you don't know the atom type, pass them as positional arguments, and the command will find out its type. shell $ ocd-dtl bulk_lookup_threats 113.223.40.103 5.78.23.158 reverso.net f0f5d30e91006c8ca7a528f26432ftt5 With all that in mind, you can combine them to fit your needs: $ ocd-dtl bulk_lookup_threats reverso.net 113 .223.40.103 --domain paiza.com --ip 45 .96.65.132 -i ip:path/to/file/myiplist.txt -i path/to/file/myfilelist.txt ----untyped domain ip----- ---typed domain--- ----typed ip---- -------typed file as ip-------- ---------untyped file---------","title":"Passing atoms by CLI"},{"location":"docs/Bulk_lookup_threats/#parameters","text":"Optional arguments > -h, --help show this help message and exit -o [OUTPUT], --output [OUTPUT] file path from script -ot OUTPUT_TYPE, --output-type OUTPUT_TYPE set to the output type desired {json,csv}. Default is json -e {prod,preprod}, --env {prod,preprod} execute on specified environment (Default: prod) --debug -q, --quiet Silence the output to only show warnings/errors -ad, --atom-details returns threats full details -i INPUT, --input INPUT read threats to add from FILE. Typed atom flags > --apk APK --asn ASN --cc CC --crypto CRYPTO --cve CVE --domain DOMAIN --email EMAIL --file FILE --fqdn FQDN --iban IBAN --ip IP --ip_range IP_RANGE --paste PASTE --phone_number PHONE_NUMBER --regkey REGKEY --ssl SSL --url URL","title":"Parameters"},{"location":"docs/CONTRIBUTING/","text":"Contribute Installation for local development You will need Python 3.6+ in order to execute the scripts. First install a virtual environment and then run the command pip install -r requirements.txt . Using a script The easiest way is to be in the virtual environment and run commands with: shell script ocd-dtl <command> <parameter> Check ocd-dtl -h for help, including the list of commands available. /!\\ Make sure to use utf-8 without BOM when providing a file (-i option) Adding a new script To add a new script, simply create a new file in ./src/scripts/{my_script_name.py} . And add a new function to the cli file . Tests Run tests with python -m pytest","title":"Contributing"},{"location":"docs/CONTRIBUTING/#contribute","text":"","title":"Contribute"},{"location":"docs/CONTRIBUTING/#installation-for-local-development","text":"You will need Python 3.6+ in order to execute the scripts. First install a virtual environment and then run the command pip install -r requirements.txt .","title":"Installation for local development"},{"location":"docs/CONTRIBUTING/#using-a-script","text":"The easiest way is to be in the virtual environment and run commands with: shell script ocd-dtl <command> <parameter> Check ocd-dtl -h for help, including the list of commands available. /!\\ Make sure to use utf-8 without BOM when providing a file (-i option)","title":"Using a script"},{"location":"docs/CONTRIBUTING/#adding-a-new-script","text":"To add a new script, simply create a new file in ./src/scripts/{my_script_name.py} . And add a new function to the cli file .","title":"Adding a new script"},{"location":"docs/CONTRIBUTING/#tests","text":"Run tests with python -m pytest","title":"Tests"},{"location":"docs/Edit_score/","text":"Edit the score of existing threats from their hashkeys Examples From TXT file hashkeys.txt : 9 f1a0af65610adc5cf10159f0413c499 afca2b3539110c70f53755e4eae5b4d6 88 d277709c94fe4380de77b974ac0c14 To edit them: ocd-dtl edit_score -i input.txt -o output.json -t malware 90 scam 12 Or directly from the cli: ocd-dtl edit_score 9f1a0af65610adc5cf10159f0413c499 afca2b3539110c70f53755e4eae5b4d6 88d277709c94fe4380de77b974ac0c14 -t malware 90 scam 12 Parameters <hashkeys> (positional argument) the haskeys to edit -i will be the input file At least one of them is required. -t the list of threat types and it's associated score like: ddos 50 scam 15 (see below for the authorized values). -w (Optional) will set all the scores to 0 like a whitelist. Override -t -o (Optional) will set the output file as the API gives it. -e (Optional) to change the environment {preprod, prod}, default is prod Accepted threat types for -t parameter, please use one of: ddos fraud hack leak malware phishing scam scan spam Followed by a number between 0 and 100 Environment variables The following environment variable can be used OCD_DTL_MAX_EDIT_SCORE_HASHKEYS Sets the number of hashkeys that can be edited with one function call. Default is 100.","title":"Edit Score"},{"location":"docs/Edit_score/#edit-the-score-of-existing-threats-from-their-hashkeys","text":"","title":"Edit the score of existing threats from their hashkeys"},{"location":"docs/Edit_score/#examples","text":"From TXT file hashkeys.txt : 9 f1a0af65610adc5cf10159f0413c499 afca2b3539110c70f53755e4eae5b4d6 88 d277709c94fe4380de77b974ac0c14 To edit them: ocd-dtl edit_score -i input.txt -o output.json -t malware 90 scam 12 Or directly from the cli: ocd-dtl edit_score 9f1a0af65610adc5cf10159f0413c499 afca2b3539110c70f53755e4eae5b4d6 88d277709c94fe4380de77b974ac0c14 -t malware 90 scam 12","title":"Examples"},{"location":"docs/Edit_score/#parameters","text":"<hashkeys> (positional argument) the haskeys to edit -i will be the input file At least one of them is required. -t the list of threat types and it's associated score like: ddos 50 scam 15 (see below for the authorized values). -w (Optional) will set all the scores to 0 like a whitelist. Override -t -o (Optional) will set the output file as the API gives it. -e (Optional) to change the environment {preprod, prod}, default is prod","title":"Parameters"},{"location":"docs/Edit_score/#accepted-threat-types","text":"for -t parameter, please use one of: ddos fraud hack leak malware phishing scam scan spam Followed by a number between 0 and 100","title":"Accepted threat types"},{"location":"docs/Edit_score/#environment-variables","text":"The following environment variable can be used OCD_DTL_MAX_EDIT_SCORE_HASHKEYS Sets the number of hashkeys that can be edited with one function call. Default is 100.","title":"Environment variables"},{"location":"docs/Get_threats/","text":"Get threats from their hashkeys Examples From TXT file hashkeys.txt : 9 f1a0af65610adc5cf10159f0413c499 afca2b3539110c70f53755e4eae5b4d6 88 d277709c94fe4380de77b974ac0c14 To retrieve them: ocd-dtl get_threats --input_file hashkeys.txt -o output.json Or directly from the cli: ocd-dtl get_threats 9f1a0af65610adc5cf10159f0413c499 afca2b3539110c70f53755e4eae5b4d6 88d277709c94fe4380de77b974ac0c14 -o output.json Parameters <hashkeys> (positional argument) the haskeys to query -i will be the input file At least one of them is required. If both parameters are provided, only hashkeys in the file will be retrieved. -o (Optional) will set the output file as the API gives it. -e (Optional) to change the environment {preprod, prod}, default is prod","title":"Get Threats"},{"location":"docs/Get_threats/#get-threats-from-their-hashkeys","text":"","title":"Get threats from their hashkeys"},{"location":"docs/Get_threats/#examples","text":"From TXT file hashkeys.txt : 9 f1a0af65610adc5cf10159f0413c499 afca2b3539110c70f53755e4eae5b4d6 88 d277709c94fe4380de77b974ac0c14 To retrieve them: ocd-dtl get_threats --input_file hashkeys.txt -o output.json Or directly from the cli: ocd-dtl get_threats 9f1a0af65610adc5cf10159f0413c499 afca2b3539110c70f53755e4eae5b4d6 88d277709c94fe4380de77b974ac0c14 -o output.json","title":"Examples"},{"location":"docs/Get_threats/#parameters","text":"<hashkeys> (positional argument) the haskeys to query -i will be the input file At least one of them is required. If both parameters are provided, only hashkeys in the file will be retrieved. -o (Optional) will set the output file as the API gives it. -e (Optional) to change the environment {preprod, prod}, default is prod","title":"Parameters"},{"location":"docs/Get_threats_from_query_hash/","text":"Get threats from a query hash This command allow to retrieve all threats at once without having to deal with a pagination. You need to have bulk search permission to use this endpoint You can see your set of permission on the GUI Or on the API using this endpoint: /v1/users/me/ Examples To retrieve hashkeys as a list : ocd-dtl get_threats_from_query_hash 4e26a3d4bbfd87375d04aaef983b6c8a --list -o output.text Will respond by: b7980ce58188a221abe23548826e9a1b 43f6b78e3b80397633d46a6207c1c6ec 61994647e96ef289342e288d3fc366ff Or make a more advanced query, that will return directly the API response (using json format): ocd-dtl get_threats_from_query_hash 46b384ce467f4d2ae8b74025f9b266ed --query_fields atom_value .android.developer .android.permissions -o output.json For atoms details, keep in mind to prefix them with a dot like for .android.developer If no value is present, an empty string will replace it Parameters <query_hash> (positional argument) the query_hash to query if the query hash is a valid local path to a (json) file, it will be used as query body instead --query_fields (Optional) fields to be retrieved from the threat (default: threat_hashkey) --list (Optional) Turn the output in a list (require query_fields to be a single element ) -o (Optional) will set the output file as the API gives it. -e (Optional) to change the environment {preprod, prod}, default is prod Environment variables OCD_DTL_MAX_BACK_OFF_TIME allow to set the maximum time period to wait between two api calls to check if the bulk search is ready. default is 120 seconds . OCD_DTL_MAX_BULK_SEARCH_TIME , the maximum time period, in seconds, to wait for the bulksearch to be ready, after which the bulk search will be considered failed. default is 3600 seconds .","title":"Get Threats From Query Hash"},{"location":"docs/Get_threats_from_query_hash/#get-threats-from-a-query-hash","text":"This command allow to retrieve all threats at once without having to deal with a pagination. You need to have bulk search permission to use this endpoint You can see your set of permission on the GUI Or on the API using this endpoint: /v1/users/me/","title":"Get threats from a query hash"},{"location":"docs/Get_threats_from_query_hash/#examples","text":"To retrieve hashkeys as a list : ocd-dtl get_threats_from_query_hash 4e26a3d4bbfd87375d04aaef983b6c8a --list -o output.text Will respond by: b7980ce58188a221abe23548826e9a1b 43f6b78e3b80397633d46a6207c1c6ec 61994647e96ef289342e288d3fc366ff Or make a more advanced query, that will return directly the API response (using json format): ocd-dtl get_threats_from_query_hash 46b384ce467f4d2ae8b74025f9b266ed --query_fields atom_value .android.developer .android.permissions -o output.json For atoms details, keep in mind to prefix them with a dot like for .android.developer If no value is present, an empty string will replace it","title":"Examples"},{"location":"docs/Get_threats_from_query_hash/#parameters","text":"<query_hash> (positional argument) the query_hash to query if the query hash is a valid local path to a (json) file, it will be used as query body instead --query_fields (Optional) fields to be retrieved from the threat (default: threat_hashkey) --list (Optional) Turn the output in a list (require query_fields to be a single element ) -o (Optional) will set the output file as the API gives it. -e (Optional) to change the environment {preprod, prod}, default is prod","title":"Parameters"},{"location":"docs/Get_threats_from_query_hash/#environment-variables","text":"OCD_DTL_MAX_BACK_OFF_TIME allow to set the maximum time period to wait between two api calls to check if the bulk search is ready. default is 120 seconds . OCD_DTL_MAX_BULK_SEARCH_TIME , the maximum time period, in seconds, to wait for the bulksearch to be ready, after which the bulk search will be considered failed. default is 3600 seconds .","title":"Environment variables"},{"location":"docs/Lookup_threats/","text":"Returns wether or not a threat is in the database Examples From TXT file input.txt : threat1.com anotherthreat.fr en.wikipedia From CSV file: # comment 2012 - 10 - 01 , threat1 . com , random comments 2012 - 10 - 01 , anotherthreat . fr , random comments 2012 - 10 - 01 , en . wikipedia , random comments To look them up: ocd-dtl lookup_threats -i input.txt -o output.csv -ot csv -a domain Or/And directly from the cli: ocd-dtl lookup_threats threat1.com anotherthreat.fr en.wikipedia -o output.json -a domain Parameters <atoms> (positional argument) the threats to lookup -i will be the input file -a will set the atom type, here domain (Optional) --is_csv set it to have a csv file as an input -d to have a special delimiter -c to select the column (starting at 1 ) -ho set it to False to get the complete result -o will set the output file as a csv per default. -ot set it to json or csv, to choose the output type. Default is json -td, --threat_details set if you also want to have access to the threat details in output file -e to change the environment {preprod, prod}, default is prod","title":"Lookup Threats"},{"location":"docs/Lookup_threats/#returns-wether-or-not-a-threat-is-in-the-database","text":"","title":"Returns wether or not a threat is in the database"},{"location":"docs/Lookup_threats/#examples","text":"From TXT file input.txt : threat1.com anotherthreat.fr en.wikipedia From CSV file: # comment 2012 - 10 - 01 , threat1 . com , random comments 2012 - 10 - 01 , anotherthreat . fr , random comments 2012 - 10 - 01 , en . wikipedia , random comments To look them up: ocd-dtl lookup_threats -i input.txt -o output.csv -ot csv -a domain Or/And directly from the cli: ocd-dtl lookup_threats threat1.com anotherthreat.fr en.wikipedia -o output.json -a domain","title":"Examples"},{"location":"docs/Lookup_threats/#parameters","text":"<atoms> (positional argument) the threats to lookup -i will be the input file -a will set the atom type, here domain (Optional) --is_csv set it to have a csv file as an input -d to have a special delimiter -c to select the column (starting at 1 ) -ho set it to False to get the complete result -o will set the output file as a csv per default. -ot set it to json or csv, to choose the output type. Default is json -td, --threat_details set if you also want to have access to the threat details in output file -e to change the environment {preprod, prod}, default is prod","title":"Parameters"},{"location":"docs/advanced_search/","text":"Advanced Search Examples With a query body To execute an advanced search from a query body stocked in a json file. query_body.json { \"AND\": [ { \"AND\": [ { \"field\": \"atom_type\", \"multi_values\": [ \"ip\" ], \"type\": \"filter\" }, { \"field\": \"risk\", \"range\": { \"gt\": 60 }, \"type\": \"filter\" } ] } ] } Run the following command ocd-dtl advanced_search -i query_body.json -o output.json With a query hash To execute an advanced search from a query hash run the following command. ocd-dtl advanced_search --query-hash cece3117abc823cee81e69c2143e6268 -o output.json Parameters Required: * -o : indicate the output file's path Use either: * -i : indicate the path of the file containing the query body * --query-hash : the query hash for your advanced search Optional flags: * --limit : defines how many items will be returned in one page slice. Accepted values: 0 to 5000, default is 20 * --offset : defines an index of the first requested item. Accepted values: 0 and bigger, default is 0 * --output-type : sets the output type desired {json, csv, stix, misp}. Default is json * --ordering : threat field to filter on. To sort the results by relevance (if any \"search\" is applied), just skip this field. To use the reversed order, use minus, i.e. --ordering=\"-last_updated\" .","title":"Advanced Search"},{"location":"docs/advanced_search/#advanced-search","text":"","title":"Advanced Search"},{"location":"docs/advanced_search/#examples","text":"","title":"Examples"},{"location":"docs/advanced_search/#with-a-query-body","text":"To execute an advanced search from a query body stocked in a json file. query_body.json { \"AND\": [ { \"AND\": [ { \"field\": \"atom_type\", \"multi_values\": [ \"ip\" ], \"type\": \"filter\" }, { \"field\": \"risk\", \"range\": { \"gt\": 60 }, \"type\": \"filter\" } ] } ] } Run the following command ocd-dtl advanced_search -i query_body.json -o output.json","title":"With a query body"},{"location":"docs/advanced_search/#with-a-query-hash","text":"To execute an advanced search from a query hash run the following command. ocd-dtl advanced_search --query-hash cece3117abc823cee81e69c2143e6268 -o output.json","title":"With a query hash"},{"location":"docs/advanced_search/#parameters","text":"Required: * -o : indicate the output file's path Use either: * -i : indicate the path of the file containing the query body * --query-hash : the query hash for your advanced search Optional flags: * --limit : defines how many items will be returned in one page slice. Accepted values: 0 to 5000, default is 20 * --offset : defines an index of the first requested item. Accepted values: 0 and bigger, default is 0 * --output-type : sets the output type desired {json, csv, stix, misp}. Default is json * --ordering : threat field to filter on. To sort the results by relevance (if any \"search\" is applied), just skip this field. To use the reversed order, use minus, i.e. --ordering=\"-last_updated\" .","title":"Parameters"},{"location":"reference/datalake/","text":"Module datalake None None View Source from .api_objects.bulk_search_task import BulkSearchTask , BulkSearchTaskState , BulkSearchFailedError , BulkSearchNotFound from .common.atom import AtomType , ThreatType , OverrideType , SightingType , Visibility from .common.atom_type import Atom , Hashes , FileAtom , AndroidApp , ApkAtom , AsAtom , CcAtom , CryptoAtom , CveAtom , Jarm , DomainAtom , EmailFlow , EmailAtom , FqdnAtom , IbanAtom , IpService , IpAtom , IpRangeAtom , PasteAtom , PhoneNumberAtom , RegKeyAtom , SslAtom , UrlAtom from .common.ouput import Output from .datalake import Datalake Sub-modules datalake.api_objects datalake.common datalake.datalake datalake.endpoints","title":"Index"},{"location":"reference/datalake/#module-datalake","text":"None None View Source from .api_objects.bulk_search_task import BulkSearchTask , BulkSearchTaskState , BulkSearchFailedError , BulkSearchNotFound from .common.atom import AtomType , ThreatType , OverrideType , SightingType , Visibility from .common.atom_type import Atom , Hashes , FileAtom , AndroidApp , ApkAtom , AsAtom , CcAtom , CryptoAtom , CveAtom , Jarm , DomainAtom , EmailFlow , EmailAtom , FqdnAtom , IbanAtom , IpService , IpAtom , IpRangeAtom , PasteAtom , PhoneNumberAtom , RegKeyAtom , SslAtom , UrlAtom from .common.ouput import Output from .datalake import Datalake","title":"Module datalake"},{"location":"reference/datalake/#sub-modules","text":"datalake.api_objects datalake.common datalake.datalake datalake.endpoints","title":"Sub-modules"},{"location":"reference/datalake/datalake/","text":"Module datalake.datalake None None View Source import logging from datalake import AtomType from datalake.common.config import Config from datalake.common.logger import configure_logging from datalake.common.token_manager import TokenManager from datalake.endpoints.threats import Threats from datalake.endpoints.bulk_search import BulkSearch from datalake.endpoints.tags import Tags from datalake.endpoints.advanced_search import AdvancedSearch from datalake.endpoints.sightings import Sightings class Datalake : \"\"\" Entrypoint to the Datalake library Usage: >>> dtl = Datalake(username='some username', password='some password') >>> dtl.Threats.lookup(atom_value='mayoclinic.org', atom_type=AtomType.DOMAIN, hashkey_only=False) \"\"\" def __init__ ( self , username : str = None , password : str = None , env = 'prod' , log_level = logging . WARNING ): configure_logging ( log_level ) endpoint_config = Config () . load_config () token_manager = TokenManager ( endpoint_config , environment = env , username = username , password = password ) self . Threats = Threats ( endpoint_config , env , token_manager ) self . BulkSearch = BulkSearch ( endpoint_config , env , token_manager ) self . Tags = Tags ( endpoint_config , env , token_manager ) self . AdvancedSearch = AdvancedSearch ( endpoint_config , env , token_manager ) self . Sightings = Sightings ( endpoint_config , env , token_manager ) Classes Datalake class Datalake ( username : str = None , password : str = None , env = 'prod' , log_level = 30 ) View Source class Datalake : \"\"\" Entrypoint to the Datalake library Usage: >>> dtl = Datalake(username='some username', password='some password') >>> dtl.Threats.lookup(atom_value='mayoclinic.org', atom_type=AtomType.DOMAIN, hashkey_only=False) \"\"\" def __init__ ( self , username : str = None , password : str = None , env = 'prod' , log_level = logging . WARNING ): configure_logging ( log_level ) endpoint_config = Config () . load_config () token_manager = TokenManager ( endpoint_config , environment = env , username = username , password = password ) self . Threats = Threats ( endpoint_config , env , token_manager ) self . BulkSearch = BulkSearch ( endpoint_config , env , token_manager ) self . Tags = Tags ( endpoint_config , env , token_manager ) self . AdvancedSearch = AdvancedSearch ( endpoint_config , env , token_manager ) self . Sightings = Sightings ( endpoint_config , env , token_manager )","title":"Datalake"},{"location":"reference/datalake/datalake/#module-datalakedatalake","text":"None None View Source import logging from datalake import AtomType from datalake.common.config import Config from datalake.common.logger import configure_logging from datalake.common.token_manager import TokenManager from datalake.endpoints.threats import Threats from datalake.endpoints.bulk_search import BulkSearch from datalake.endpoints.tags import Tags from datalake.endpoints.advanced_search import AdvancedSearch from datalake.endpoints.sightings import Sightings class Datalake : \"\"\" Entrypoint to the Datalake library Usage: >>> dtl = Datalake(username='some username', password='some password') >>> dtl.Threats.lookup(atom_value='mayoclinic.org', atom_type=AtomType.DOMAIN, hashkey_only=False) \"\"\" def __init__ ( self , username : str = None , password : str = None , env = 'prod' , log_level = logging . WARNING ): configure_logging ( log_level ) endpoint_config = Config () . load_config () token_manager = TokenManager ( endpoint_config , environment = env , username = username , password = password ) self . Threats = Threats ( endpoint_config , env , token_manager ) self . BulkSearch = BulkSearch ( endpoint_config , env , token_manager ) self . Tags = Tags ( endpoint_config , env , token_manager ) self . AdvancedSearch = AdvancedSearch ( endpoint_config , env , token_manager ) self . Sightings = Sightings ( endpoint_config , env , token_manager )","title":"Module datalake.datalake"},{"location":"reference/datalake/datalake/#classes","text":"","title":"Classes"},{"location":"reference/datalake/datalake/#datalake","text":"class Datalake ( username : str = None , password : str = None , env = 'prod' , log_level = 30 ) View Source class Datalake : \"\"\" Entrypoint to the Datalake library Usage: >>> dtl = Datalake(username='some username', password='some password') >>> dtl.Threats.lookup(atom_value='mayoclinic.org', atom_type=AtomType.DOMAIN, hashkey_only=False) \"\"\" def __init__ ( self , username : str = None , password : str = None , env = 'prod' , log_level = logging . WARNING ): configure_logging ( log_level ) endpoint_config = Config () . load_config () token_manager = TokenManager ( endpoint_config , environment = env , username = username , password = password ) self . Threats = Threats ( endpoint_config , env , token_manager ) self . BulkSearch = BulkSearch ( endpoint_config , env , token_manager ) self . Tags = Tags ( endpoint_config , env , token_manager ) self . AdvancedSearch = AdvancedSearch ( endpoint_config , env , token_manager ) self . Sightings = Sightings ( endpoint_config , env , token_manager )","title":"Datalake"},{"location":"reference/datalake/api_objects/","text":"Module datalake.api_objects None None Sub-modules datalake.api_objects.bulk_search_task","title":"Index"},{"location":"reference/datalake/api_objects/#module-datalakeapi_objects","text":"None None","title":"Module datalake.api_objects"},{"location":"reference/datalake/api_objects/#sub-modules","text":"datalake.api_objects.bulk_search_task","title":"Sub-modules"},{"location":"reference/datalake/api_objects/bulk_search_task/","text":"Module datalake.api_objects.bulk_search_task None None View Source import asyncio import datetime import os from enum import Enum from datalake.common.ouput import Output from datalake.common.utils import parse_api_timestamp class BulkSearchTaskState ( Enum ): NEW = 'NEW' QUEUED = 'QUEUED' IN_PROGRESS = 'IN_PROGRESS' DONE = 'DONE' CANCELLED = 'CANCELLED' FAILED_ERROR = 'FAILED_ERROR' FAILED_TIMEOUT = 'FAILED_TIMEOUT' BULK_SEARCH_FAILED_STATE = { BulkSearchTaskState . CANCELLED , BulkSearchTaskState . FAILED_TIMEOUT , BulkSearchTaskState . FAILED_ERROR } class BulkSearchFailedError ( Exception ): def __init__ ( self , failed_state : BulkSearchTaskState ): self . failed_state = failed_state class BulkSearchNotFound ( Exception ): pass class BulkSearchTask : \"\"\" Bulk Search Task as represented by the API This class is a thin wrapper around information returned by the API \"\"\" REQUEST_INTERVAL = float ( os . getenv ( 'OCD_DTL_MAX_BACK_OFF_TIME' , 10 )) STREAM_CHUNK_SIZE = 4096 def __init__ ( self , endpoint : \"BulkSearch\" , bulk_search : dict , bulk_search_hash : str , created_at : str , eta : str , file_delete_after : str , file_deleted : bool , file_size : int , finished_at : str , progress : int , queue_position : int , results : int , started_at : str , state : str , uuid : str , user : dict , ): \"\"\"Do not call this method directly, use BulkSearch.create_task instead\"\"\" self . _endpoint = endpoint # flatten bulk_search field self . advanced_query_hash = bulk_search [ 'advanced_query_hash' ] self . query_fields = bulk_search [ 'query_fields' ] self . bulk_search_hash = bulk_search_hash self . created_at = parse_api_timestamp ( created_at ) self . eta = parse_api_timestamp ( eta ) self . file_delete_after = file_delete_after self . file_deleted = file_deleted self . file_size = file_size self . finished_at = parse_api_timestamp ( finished_at ) self . progress = progress self . queue_position = queue_position self . results = results self . started_at = parse_api_timestamp ( started_at ) self . state = BulkSearchTaskState [ state ] self . user = user self . uuid = uuid def download ( self , output = Output . JSON , stream = False ): \"\"\" Download a bulk search task, if it is not ready to be downloaded, it'll return a ResponseNotReady error Use download_sync/download_async to automatically wait for the bulk search to be ready before downloading it \"\"\" return self . _endpoint . download ( self . uuid , output = output , stream = stream ) async def download_async ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ): \"\"\" Wait asynchronously for the bulk search to be ready then return its result Note that the requests library is used which is blocking. timeout parameter is in seconds. \"\"\" start = datetime . datetime . utcnow () while self . state != BulkSearchTaskState . DONE : if self . state in BULK_SEARCH_FAILED_STATE : raise BulkSearchFailedError ( self . state ) time_passed = datetime . datetime . utcnow () - start if time_passed . total_seconds () > timeout : raise TimeoutError () await asyncio . sleep ( self . REQUEST_INTERVAL ) self . update () return self . download ( output = output , stream = stream ) def download_sync ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ): \"\"\"Blocking version of download_async, easier to use but doesn't allow parallelization\"\"\" return asyncio . run ( self . download_async ( output , timeout , stream = stream )) def download_sync_stream_to_file ( self , output_path , output = Output . JSON , timeout = 15 * 60 ): \"\"\" Wrapper around download_sync that output the result to a file directly (reducing the memory usage). output_path should be an absolute path. \"\"\" raw_response = self . download_sync ( output = output , timeout = timeout , stream = True ) with open ( output_path , 'wb' ) as output_file : # Binary is required for zip types for chunk in raw_response . iter_content ( chunk_size = self . STREAM_CHUNK_SIZE ): output_file . write ( chunk ) def update ( self ): \"\"\"Query the API to refresh the tasks attributes\"\"\" updated_bs = self . _endpoint . get_task ( self . uuid ) self . __dict__ . update ( updated_bs . __dict__ ) # Avoid to return a new object Variables BULK_SEARCH_FAILED_STATE Classes BulkSearchFailedError class BulkSearchFailedError ( failed_state : datalake . api_objects . bulk_search_task . BulkSearchTaskState ) View Source class BulkSearchFailedError ( Exception ): def __init__ ( self , failed_state: BulkSearchTaskState ): self . failed_state = failed_state Ancestors (in MRO) builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self. BulkSearchNotFound class BulkSearchNotFound ( / , * args , ** kwargs ) View Source class BulkSearchNotFound ( Exception ): pass Ancestors (in MRO) builtins.Exception builtins.BaseException Class variables args Methods with_traceback def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self. BulkSearchTask class BulkSearchTask ( endpoint : 'BulkSearch' , bulk_search : dict , bulk_search_hash : str , created_at : str , eta : str , file_delete_after : str , file_deleted : bool , file_size : int , finished_at : str , progress : int , queue_position : int , results : int , started_at : str , state : str , uuid : str , user : dict ) View Source class BulkSearchTask : \"\"\" Bulk Search Task as represented by the API This class is a thin wrapper around information returned by the API \"\"\" REQUEST_INTERVAL = float ( os . getenv ( 'OCD_DTL_MAX_BACK_OFF_TIME' , 10 )) STREAM_CHUNK_SIZE = 4096 def __init__ ( self , endpoint : \"BulkSearch\" , bulk_search : dict , bulk_search_hash : str , created_at : str , eta : str , file_delete_after : str , file_deleted : bool , file_size : int , finished_at : str , progress : int , queue_position : int , results : int , started_at : str , state : str , uuid : str , user : dict , ) : \"\"\"Do not call this method directly, use BulkSearch.create_task instead\"\"\" self . _endpoint = endpoint # flatten bulk_search field self . advanced_query_hash = bulk_search [ 'advanced_query_hash' ] self . query_fields = bulk_search [ 'query_fields' ] self . bulk_search_hash = bulk_search_hash self . created_at = parse_api_timestamp ( created_at ) self . eta = parse_api_timestamp ( eta ) self . file_delete_after = file_delete_after self . file_deleted = file_deleted self . file_size = file_size self . finished_at = parse_api_timestamp ( finished_at ) self . progress = progress self . queue_position = queue_position self . results = results self . started_at = parse_api_timestamp ( started_at ) self . state = BulkSearchTaskState [ state ] self . user = user self . uuid = uuid def download ( self , output = Output . JSON , stream = False ) : \"\"\" Download a bulk search task, if it is not ready to be downloaded, it'll return a ResponseNotReady error Use download_sync/download_async to automatically wait for the bulk search to be ready before downloading it \"\"\" return self . _endpoint . download ( self . uuid , output = output , stream = stream ) async def download_async ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ) : \"\"\" Wait asynchronously for the bulk search to be ready then return its result Note that the requests library is used which is blocking. timeout parameter is in seconds. \"\"\" start = datetime . datetime . utcnow () while self . state != BulkSearchTaskState . DONE : if self . state in BULK_SEARCH_FAILED_STATE : raise BulkSearchFailedError ( self . state ) time_passed = datetime . datetime . utcnow () - start if time_passed . total_seconds () > timeout : raise TimeoutError () await asyncio . sleep ( self . REQUEST_INTERVAL ) self . update () return self . download ( output = output , stream = stream ) def download_sync ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ) : \"\"\"Blocking version of download_async, easier to use but doesn't allow parallelization\"\"\" return asyncio . run ( self . download_async ( output , timeout , stream = stream )) def download_sync_stream_to_file ( self , output_path , output = Output . JSON , timeout = 15 * 60 ) : \"\"\" Wrapper around download_sync that output the result to a file directly (reducing the memory usage). output_path should be an absolute path. \"\"\" raw_response = self . download_sync ( output = output , timeout = timeout , stream = True ) with open ( output_path , 'wb' ) as output_file : # Binary is required for zip types for chunk in raw_response . iter_content ( chunk_size = self . STREAM_CHUNK_SIZE ) : output_file . write ( chunk ) def update ( self ) : \"\"\"Query the API to refresh the tasks attributes\"\"\" updated_bs = self . _endpoint . get_task ( self . uuid ) self . __dict__ . update ( updated_bs . __dict__ ) # Avoid to return a new object Class variables REQUEST_INTERVAL STREAM_CHUNK_SIZE Methods download def download ( self , output = JSON , stream = False ) Download a bulk search task, if it is not ready to be downloaded, it'll return a ResponseNotReady error Use download_sync/download_async to automatically wait for the bulk search to be ready before downloading it View Source def download ( self , output = Output . JSON , stream = False ): \"\"\" Download a bulk search task, if it is not ready to be downloaded, it'll return a ResponseNotReady error Use download_sync/download_async to automatically wait for the bulk search to be ready before downloading it \"\"\" return self . _endpoint . download ( self . uuid , output = output , stream = stream ) download_async def download_async ( self , output = JSON , timeout = 900 , stream = False ) Wait asynchronously for the bulk search to be ready then return its result Note that the requests library is used which is blocking. timeout parameter is in seconds. View Source async def download_async ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ): \"\"\" Wait asynchronously for the bulk search to be ready then return its result Note that the requests library is used which is blocking. timeout parameter is in seconds. \"\"\" start = datetime . datetime . utcnow () while self . state != BulkSearchTaskState . DONE : if self . state in BULK_SEARCH_FAILED_STATE : raise BulkSearchFailedError ( self . state ) time_passed = datetime . datetime . utcnow () - start if time_passed . total_seconds () > timeout : raise TimeoutError () await asyncio . sleep ( self . REQUEST_INTERVAL ) self . update () return self . download ( output = output , stream = stream ) download_sync def download_sync ( self , output = JSON , timeout = 900 , stream = False ) Blocking version of download_async, easier to use but doesn't allow parallelization View Source def download_sync ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ): \"\"\"Blocking version of download_async, easier to use but doesn't allow parallelization\"\"\" return asyncio . run ( self . download_async ( output , timeout , stream = stream )) download_sync_stream_to_file def download_sync_stream_to_file ( self , output_path , output = JSON , timeout = 900 ) Wrapper around download_sync that output the result to a file directly (reducing the memory usage). output_path should be an absolute path. View Source def download_sync_stream_to_file ( self , output_path , output = Output . JSON , timeout = 15 * 60 ): \"\"\" Wrapper around download_sync that output the result to a file directly (reducing the memory usage). output_path should be an absolute path. \"\"\" raw_response = self . download_sync ( output = output , timeout = timeout , stream = True ) with open ( output_path , 'wb' ) as output_file : # Binary is required for zip types for chunk in raw_response . iter_content ( chunk_size = self . STREAM_CHUNK_SIZE ): output_file . write ( chunk ) update def update ( self ) Query the API to refresh the tasks attributes View Source def update ( self ) : \"\"\" Query the API to refresh the tasks attributes \"\"\" updated_bs = self . _endpoint . get_task ( self . uuid ) self . __dict__ . update ( updated_bs . __dict__ ) # Avoid to return a new object BulkSearchTaskState class BulkSearchTaskState ( / , * args , ** kwargs ) View Source class BulkSearchTaskState ( Enum ): NEW = 'NEW' QUEUED = 'QUEUED' IN_PROGRESS = 'IN_PROGRESS' DONE = 'DONE' CANCELLED = 'CANCELLED' FAILED_ERROR = 'FAILED_ERROR' FAILED_TIMEOUT = 'FAILED_TIMEOUT' Ancestors (in MRO) enum.Enum Class variables CANCELLED DONE FAILED_ERROR FAILED_TIMEOUT IN_PROGRESS NEW QUEUED name value","title":"Bulk Search Task"},{"location":"reference/datalake/api_objects/bulk_search_task/#module-datalakeapi_objectsbulk_search_task","text":"None None View Source import asyncio import datetime import os from enum import Enum from datalake.common.ouput import Output from datalake.common.utils import parse_api_timestamp class BulkSearchTaskState ( Enum ): NEW = 'NEW' QUEUED = 'QUEUED' IN_PROGRESS = 'IN_PROGRESS' DONE = 'DONE' CANCELLED = 'CANCELLED' FAILED_ERROR = 'FAILED_ERROR' FAILED_TIMEOUT = 'FAILED_TIMEOUT' BULK_SEARCH_FAILED_STATE = { BulkSearchTaskState . CANCELLED , BulkSearchTaskState . FAILED_TIMEOUT , BulkSearchTaskState . FAILED_ERROR } class BulkSearchFailedError ( Exception ): def __init__ ( self , failed_state : BulkSearchTaskState ): self . failed_state = failed_state class BulkSearchNotFound ( Exception ): pass class BulkSearchTask : \"\"\" Bulk Search Task as represented by the API This class is a thin wrapper around information returned by the API \"\"\" REQUEST_INTERVAL = float ( os . getenv ( 'OCD_DTL_MAX_BACK_OFF_TIME' , 10 )) STREAM_CHUNK_SIZE = 4096 def __init__ ( self , endpoint : \"BulkSearch\" , bulk_search : dict , bulk_search_hash : str , created_at : str , eta : str , file_delete_after : str , file_deleted : bool , file_size : int , finished_at : str , progress : int , queue_position : int , results : int , started_at : str , state : str , uuid : str , user : dict , ): \"\"\"Do not call this method directly, use BulkSearch.create_task instead\"\"\" self . _endpoint = endpoint # flatten bulk_search field self . advanced_query_hash = bulk_search [ 'advanced_query_hash' ] self . query_fields = bulk_search [ 'query_fields' ] self . bulk_search_hash = bulk_search_hash self . created_at = parse_api_timestamp ( created_at ) self . eta = parse_api_timestamp ( eta ) self . file_delete_after = file_delete_after self . file_deleted = file_deleted self . file_size = file_size self . finished_at = parse_api_timestamp ( finished_at ) self . progress = progress self . queue_position = queue_position self . results = results self . started_at = parse_api_timestamp ( started_at ) self . state = BulkSearchTaskState [ state ] self . user = user self . uuid = uuid def download ( self , output = Output . JSON , stream = False ): \"\"\" Download a bulk search task, if it is not ready to be downloaded, it'll return a ResponseNotReady error Use download_sync/download_async to automatically wait for the bulk search to be ready before downloading it \"\"\" return self . _endpoint . download ( self . uuid , output = output , stream = stream ) async def download_async ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ): \"\"\" Wait asynchronously for the bulk search to be ready then return its result Note that the requests library is used which is blocking. timeout parameter is in seconds. \"\"\" start = datetime . datetime . utcnow () while self . state != BulkSearchTaskState . DONE : if self . state in BULK_SEARCH_FAILED_STATE : raise BulkSearchFailedError ( self . state ) time_passed = datetime . datetime . utcnow () - start if time_passed . total_seconds () > timeout : raise TimeoutError () await asyncio . sleep ( self . REQUEST_INTERVAL ) self . update () return self . download ( output = output , stream = stream ) def download_sync ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ): \"\"\"Blocking version of download_async, easier to use but doesn't allow parallelization\"\"\" return asyncio . run ( self . download_async ( output , timeout , stream = stream )) def download_sync_stream_to_file ( self , output_path , output = Output . JSON , timeout = 15 * 60 ): \"\"\" Wrapper around download_sync that output the result to a file directly (reducing the memory usage). output_path should be an absolute path. \"\"\" raw_response = self . download_sync ( output = output , timeout = timeout , stream = True ) with open ( output_path , 'wb' ) as output_file : # Binary is required for zip types for chunk in raw_response . iter_content ( chunk_size = self . STREAM_CHUNK_SIZE ): output_file . write ( chunk ) def update ( self ): \"\"\"Query the API to refresh the tasks attributes\"\"\" updated_bs = self . _endpoint . get_task ( self . uuid ) self . __dict__ . update ( updated_bs . __dict__ ) # Avoid to return a new object","title":"Module datalake.api_objects.bulk_search_task"},{"location":"reference/datalake/api_objects/bulk_search_task/#variables","text":"BULK_SEARCH_FAILED_STATE","title":"Variables"},{"location":"reference/datalake/api_objects/bulk_search_task/#classes","text":"","title":"Classes"},{"location":"reference/datalake/api_objects/bulk_search_task/#bulksearchfailederror","text":"class BulkSearchFailedError ( failed_state : datalake . api_objects . bulk_search_task . BulkSearchTaskState ) View Source class BulkSearchFailedError ( Exception ): def __init__ ( self , failed_state: BulkSearchTaskState ): self . failed_state = failed_state","title":"BulkSearchFailedError"},{"location":"reference/datalake/api_objects/bulk_search_task/#ancestors-in-mro","text":"builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/datalake/api_objects/bulk_search_task/#class-variables","text":"args","title":"Class variables"},{"location":"reference/datalake/api_objects/bulk_search_task/#methods","text":"","title":"Methods"},{"location":"reference/datalake/api_objects/bulk_search_task/#with_traceback","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/datalake/api_objects/bulk_search_task/#bulksearchnotfound","text":"class BulkSearchNotFound ( / , * args , ** kwargs ) View Source class BulkSearchNotFound ( Exception ): pass","title":"BulkSearchNotFound"},{"location":"reference/datalake/api_objects/bulk_search_task/#ancestors-in-mro_1","text":"builtins.Exception builtins.BaseException","title":"Ancestors (in MRO)"},{"location":"reference/datalake/api_objects/bulk_search_task/#class-variables_1","text":"args","title":"Class variables"},{"location":"reference/datalake/api_objects/bulk_search_task/#methods_1","text":"","title":"Methods"},{"location":"reference/datalake/api_objects/bulk_search_task/#with_traceback_1","text":"def with_traceback ( ... ) Exception.with_traceback(tb) -- set self. traceback to tb and return self.","title":"with_traceback"},{"location":"reference/datalake/api_objects/bulk_search_task/#bulksearchtask","text":"class BulkSearchTask ( endpoint : 'BulkSearch' , bulk_search : dict , bulk_search_hash : str , created_at : str , eta : str , file_delete_after : str , file_deleted : bool , file_size : int , finished_at : str , progress : int , queue_position : int , results : int , started_at : str , state : str , uuid : str , user : dict ) View Source class BulkSearchTask : \"\"\" Bulk Search Task as represented by the API This class is a thin wrapper around information returned by the API \"\"\" REQUEST_INTERVAL = float ( os . getenv ( 'OCD_DTL_MAX_BACK_OFF_TIME' , 10 )) STREAM_CHUNK_SIZE = 4096 def __init__ ( self , endpoint : \"BulkSearch\" , bulk_search : dict , bulk_search_hash : str , created_at : str , eta : str , file_delete_after : str , file_deleted : bool , file_size : int , finished_at : str , progress : int , queue_position : int , results : int , started_at : str , state : str , uuid : str , user : dict , ) : \"\"\"Do not call this method directly, use BulkSearch.create_task instead\"\"\" self . _endpoint = endpoint # flatten bulk_search field self . advanced_query_hash = bulk_search [ 'advanced_query_hash' ] self . query_fields = bulk_search [ 'query_fields' ] self . bulk_search_hash = bulk_search_hash self . created_at = parse_api_timestamp ( created_at ) self . eta = parse_api_timestamp ( eta ) self . file_delete_after = file_delete_after self . file_deleted = file_deleted self . file_size = file_size self . finished_at = parse_api_timestamp ( finished_at ) self . progress = progress self . queue_position = queue_position self . results = results self . started_at = parse_api_timestamp ( started_at ) self . state = BulkSearchTaskState [ state ] self . user = user self . uuid = uuid def download ( self , output = Output . JSON , stream = False ) : \"\"\" Download a bulk search task, if it is not ready to be downloaded, it'll return a ResponseNotReady error Use download_sync/download_async to automatically wait for the bulk search to be ready before downloading it \"\"\" return self . _endpoint . download ( self . uuid , output = output , stream = stream ) async def download_async ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ) : \"\"\" Wait asynchronously for the bulk search to be ready then return its result Note that the requests library is used which is blocking. timeout parameter is in seconds. \"\"\" start = datetime . datetime . utcnow () while self . state != BulkSearchTaskState . DONE : if self . state in BULK_SEARCH_FAILED_STATE : raise BulkSearchFailedError ( self . state ) time_passed = datetime . datetime . utcnow () - start if time_passed . total_seconds () > timeout : raise TimeoutError () await asyncio . sleep ( self . REQUEST_INTERVAL ) self . update () return self . download ( output = output , stream = stream ) def download_sync ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ) : \"\"\"Blocking version of download_async, easier to use but doesn't allow parallelization\"\"\" return asyncio . run ( self . download_async ( output , timeout , stream = stream )) def download_sync_stream_to_file ( self , output_path , output = Output . JSON , timeout = 15 * 60 ) : \"\"\" Wrapper around download_sync that output the result to a file directly (reducing the memory usage). output_path should be an absolute path. \"\"\" raw_response = self . download_sync ( output = output , timeout = timeout , stream = True ) with open ( output_path , 'wb' ) as output_file : # Binary is required for zip types for chunk in raw_response . iter_content ( chunk_size = self . STREAM_CHUNK_SIZE ) : output_file . write ( chunk ) def update ( self ) : \"\"\"Query the API to refresh the tasks attributes\"\"\" updated_bs = self . _endpoint . get_task ( self . uuid ) self . __dict__ . update ( updated_bs . __dict__ ) # Avoid to return a new object","title":"BulkSearchTask"},{"location":"reference/datalake/api_objects/bulk_search_task/#class-variables_2","text":"REQUEST_INTERVAL STREAM_CHUNK_SIZE","title":"Class variables"},{"location":"reference/datalake/api_objects/bulk_search_task/#methods_2","text":"","title":"Methods"},{"location":"reference/datalake/api_objects/bulk_search_task/#download","text":"def download ( self , output = JSON , stream = False ) Download a bulk search task, if it is not ready to be downloaded, it'll return a ResponseNotReady error Use download_sync/download_async to automatically wait for the bulk search to be ready before downloading it View Source def download ( self , output = Output . JSON , stream = False ): \"\"\" Download a bulk search task, if it is not ready to be downloaded, it'll return a ResponseNotReady error Use download_sync/download_async to automatically wait for the bulk search to be ready before downloading it \"\"\" return self . _endpoint . download ( self . uuid , output = output , stream = stream )","title":"download"},{"location":"reference/datalake/api_objects/bulk_search_task/#download_async","text":"def download_async ( self , output = JSON , timeout = 900 , stream = False ) Wait asynchronously for the bulk search to be ready then return its result Note that the requests library is used which is blocking. timeout parameter is in seconds. View Source async def download_async ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ): \"\"\" Wait asynchronously for the bulk search to be ready then return its result Note that the requests library is used which is blocking. timeout parameter is in seconds. \"\"\" start = datetime . datetime . utcnow () while self . state != BulkSearchTaskState . DONE : if self . state in BULK_SEARCH_FAILED_STATE : raise BulkSearchFailedError ( self . state ) time_passed = datetime . datetime . utcnow () - start if time_passed . total_seconds () > timeout : raise TimeoutError () await asyncio . sleep ( self . REQUEST_INTERVAL ) self . update () return self . download ( output = output , stream = stream )","title":"download_async"},{"location":"reference/datalake/api_objects/bulk_search_task/#download_sync","text":"def download_sync ( self , output = JSON , timeout = 900 , stream = False ) Blocking version of download_async, easier to use but doesn't allow parallelization View Source def download_sync ( self , output = Output . JSON , timeout = 15 * 60 , stream = False ): \"\"\"Blocking version of download_async, easier to use but doesn't allow parallelization\"\"\" return asyncio . run ( self . download_async ( output , timeout , stream = stream ))","title":"download_sync"},{"location":"reference/datalake/api_objects/bulk_search_task/#download_sync_stream_to_file","text":"def download_sync_stream_to_file ( self , output_path , output = JSON , timeout = 900 ) Wrapper around download_sync that output the result to a file directly (reducing the memory usage). output_path should be an absolute path. View Source def download_sync_stream_to_file ( self , output_path , output = Output . JSON , timeout = 15 * 60 ): \"\"\" Wrapper around download_sync that output the result to a file directly (reducing the memory usage). output_path should be an absolute path. \"\"\" raw_response = self . download_sync ( output = output , timeout = timeout , stream = True ) with open ( output_path , 'wb' ) as output_file : # Binary is required for zip types for chunk in raw_response . iter_content ( chunk_size = self . STREAM_CHUNK_SIZE ): output_file . write ( chunk )","title":"download_sync_stream_to_file"},{"location":"reference/datalake/api_objects/bulk_search_task/#update","text":"def update ( self ) Query the API to refresh the tasks attributes View Source def update ( self ) : \"\"\" Query the API to refresh the tasks attributes \"\"\" updated_bs = self . _endpoint . get_task ( self . uuid ) self . __dict__ . update ( updated_bs . __dict__ ) # Avoid to return a new object","title":"update"},{"location":"reference/datalake/api_objects/bulk_search_task/#bulksearchtaskstate","text":"class BulkSearchTaskState ( / , * args , ** kwargs ) View Source class BulkSearchTaskState ( Enum ): NEW = 'NEW' QUEUED = 'QUEUED' IN_PROGRESS = 'IN_PROGRESS' DONE = 'DONE' CANCELLED = 'CANCELLED' FAILED_ERROR = 'FAILED_ERROR' FAILED_TIMEOUT = 'FAILED_TIMEOUT'","title":"BulkSearchTaskState"},{"location":"reference/datalake/api_objects/bulk_search_task/#ancestors-in-mro_2","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/datalake/api_objects/bulk_search_task/#class-variables_3","text":"CANCELLED DONE FAILED_ERROR FAILED_TIMEOUT IN_PROGRESS NEW QUEUED name value","title":"Class variables"},{"location":"reference/datalake/common/","text":"Module datalake.common None None View Source Sub-modules datalake.common.atom datalake.common.atom_type datalake.common.config datalake.common.logger datalake.common.ouput datalake.common.throttler datalake.common.token_manager datalake.common.utils datalake.common.warn","title":"Index"},{"location":"reference/datalake/common/#module-datalakecommon","text":"None None View Source","title":"Module datalake.common"},{"location":"reference/datalake/common/#sub-modules","text":"datalake.common.atom datalake.common.atom_type datalake.common.config datalake.common.logger datalake.common.ouput datalake.common.throttler datalake.common.token_manager datalake.common.utils datalake.common.warn","title":"Sub-modules"},{"location":"reference/datalake/common/atom/","text":"Module datalake.common.atom Enums for atom properties None View Source \"\"\" Enums for atom properties \"\"\" from enum import Enum from typing import Dict , Union class AtomType ( Enum ): APK = 'apk' AS = 'as' CC = 'cc' CRYPTO = 'crypto' CVE = 'cve' DOMAIN = 'domain' EMAIL = 'email' FILE = 'file' FQDN = 'fqdn' IBAN = 'iban' IP = 'ip' IP_RANGE = 'ip_range' PASTE = 'paste' PHONE_NUMBER = 'phone_number' REGKEY = 'regkey' SSL = 'ssl' URL = 'url' class ThreatType ( Enum ): DDOS = 'ddos' FRAUD = 'fraud' HACK = 'hack' LEAK = 'leak' MALWARE = 'malware' PHISHING = 'phishing' SCAM = 'scam' SCAN = 'scan' SPAM = 'spam' ScoreMap = Dict [ str , Union [ int , ThreatType ]] # Should be replaced by a TypedDict when we drop python 3.7 support \"\"\" Group a threat type with a score using the following keys: score -> int (from 0 to 100 included) threat_type -> ThreatType \"\"\" class OverrideType ( Enum ): PERMANENT = 'permanent' TEMPORARY = 'temporary' LOCK = 'lock' class SightingType ( Enum ): POSITIVE = 'positive' NEGATIVE = 'negative' NEUTRAL = 'neutral' class Visibility ( Enum ): PUBLIC = 'PUBLIC' ORGANIZATION = 'ORGANIZATION' Variables ScoreMap Group a threat type with a score using the following keys: score -> int (from 0 to 100 included) threat_type -> ThreatType Classes AtomType class AtomType ( / , * args , ** kwargs ) View Source class AtomType ( Enum ) : APK = ' apk ' AS = ' as ' CC = ' cc ' CRYPTO = ' crypto ' CVE = ' cve ' DOMAIN = ' domain ' EMAIL = ' email ' FILE = ' file ' FQDN = ' fqdn ' IBAN = ' iban ' IP = ' ip ' IP_RANGE = ' ip_range ' PASTE = ' paste ' PHONE_NUMBER = ' phone_number ' REGKEY = ' regkey ' SSL = ' ssl ' URL = ' url ' Ancestors (in MRO) enum.Enum Class variables APK AS CC CRYPTO CVE DOMAIN EMAIL FILE FQDN IBAN IP IP_RANGE PASTE PHONE_NUMBER REGKEY SSL URL name value OverrideType class OverrideType ( / , * args , ** kwargs ) View Source class OverrideType ( Enum ): PERMANENT = 'permanent' TEMPORARY = 'temporary' LOCK = 'lock' Ancestors (in MRO) enum.Enum Class variables LOCK PERMANENT TEMPORARY name value SightingType class SightingType ( / , * args , ** kwargs ) View Source class SightingType ( Enum ): POSITIVE = 'positive' NEGATIVE = 'negative' NEUTRAL = 'neutral' Ancestors (in MRO) enum.Enum Class variables NEGATIVE NEUTRAL POSITIVE name value ThreatType class ThreatType ( / , * args , ** kwargs ) View Source class ThreatType ( Enum ): DDOS = 'ddos' FRAUD = 'fraud' HACK = 'hack' LEAK = 'leak' MALWARE = 'malware' PHISHING = 'phishing' SCAM = 'scam' SCAN = 'scan' SPAM = 'spam' Ancestors (in MRO) enum.Enum Class variables DDOS FRAUD HACK LEAK MALWARE PHISHING SCAM SCAN SPAM name value Visibility class Visibility ( / , * args , ** kwargs ) View Source class Visibility ( Enum ): PUBLIC = 'PUBLIC' ORGANIZATION = 'ORGANIZATION' Ancestors (in MRO) enum.Enum Class variables ORGANIZATION PUBLIC name value","title":"Atom"},{"location":"reference/datalake/common/atom/#module-datalakecommonatom","text":"Enums for atom properties None View Source \"\"\" Enums for atom properties \"\"\" from enum import Enum from typing import Dict , Union class AtomType ( Enum ): APK = 'apk' AS = 'as' CC = 'cc' CRYPTO = 'crypto' CVE = 'cve' DOMAIN = 'domain' EMAIL = 'email' FILE = 'file' FQDN = 'fqdn' IBAN = 'iban' IP = 'ip' IP_RANGE = 'ip_range' PASTE = 'paste' PHONE_NUMBER = 'phone_number' REGKEY = 'regkey' SSL = 'ssl' URL = 'url' class ThreatType ( Enum ): DDOS = 'ddos' FRAUD = 'fraud' HACK = 'hack' LEAK = 'leak' MALWARE = 'malware' PHISHING = 'phishing' SCAM = 'scam' SCAN = 'scan' SPAM = 'spam' ScoreMap = Dict [ str , Union [ int , ThreatType ]] # Should be replaced by a TypedDict when we drop python 3.7 support \"\"\" Group a threat type with a score using the following keys: score -> int (from 0 to 100 included) threat_type -> ThreatType \"\"\" class OverrideType ( Enum ): PERMANENT = 'permanent' TEMPORARY = 'temporary' LOCK = 'lock' class SightingType ( Enum ): POSITIVE = 'positive' NEGATIVE = 'negative' NEUTRAL = 'neutral' class Visibility ( Enum ): PUBLIC = 'PUBLIC' ORGANIZATION = 'ORGANIZATION'","title":"Module datalake.common.atom"},{"location":"reference/datalake/common/atom/#variables","text":"ScoreMap Group a threat type with a score using the following keys: score -> int (from 0 to 100 included) threat_type -> ThreatType","title":"Variables"},{"location":"reference/datalake/common/atom/#classes","text":"","title":"Classes"},{"location":"reference/datalake/common/atom/#atomtype","text":"class AtomType ( / , * args , ** kwargs ) View Source class AtomType ( Enum ) : APK = ' apk ' AS = ' as ' CC = ' cc ' CRYPTO = ' crypto ' CVE = ' cve ' DOMAIN = ' domain ' EMAIL = ' email ' FILE = ' file ' FQDN = ' fqdn ' IBAN = ' iban ' IP = ' ip ' IP_RANGE = ' ip_range ' PASTE = ' paste ' PHONE_NUMBER = ' phone_number ' REGKEY = ' regkey ' SSL = ' ssl ' URL = ' url '","title":"AtomType"},{"location":"reference/datalake/common/atom/#ancestors-in-mro","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom/#class-variables","text":"APK AS CC CRYPTO CVE DOMAIN EMAIL FILE FQDN IBAN IP IP_RANGE PASTE PHONE_NUMBER REGKEY SSL URL name value","title":"Class variables"},{"location":"reference/datalake/common/atom/#overridetype","text":"class OverrideType ( / , * args , ** kwargs ) View Source class OverrideType ( Enum ): PERMANENT = 'permanent' TEMPORARY = 'temporary' LOCK = 'lock'","title":"OverrideType"},{"location":"reference/datalake/common/atom/#ancestors-in-mro_1","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom/#class-variables_1","text":"LOCK PERMANENT TEMPORARY name value","title":"Class variables"},{"location":"reference/datalake/common/atom/#sightingtype","text":"class SightingType ( / , * args , ** kwargs ) View Source class SightingType ( Enum ): POSITIVE = 'positive' NEGATIVE = 'negative' NEUTRAL = 'neutral'","title":"SightingType"},{"location":"reference/datalake/common/atom/#ancestors-in-mro_2","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom/#class-variables_2","text":"NEGATIVE NEUTRAL POSITIVE name value","title":"Class variables"},{"location":"reference/datalake/common/atom/#threattype","text":"class ThreatType ( / , * args , ** kwargs ) View Source class ThreatType ( Enum ): DDOS = 'ddos' FRAUD = 'fraud' HACK = 'hack' LEAK = 'leak' MALWARE = 'malware' PHISHING = 'phishing' SCAM = 'scam' SCAN = 'scan' SPAM = 'spam'","title":"ThreatType"},{"location":"reference/datalake/common/atom/#ancestors-in-mro_3","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom/#class-variables_3","text":"DDOS FRAUD HACK LEAK MALWARE PHISHING SCAM SCAN SPAM name value","title":"Class variables"},{"location":"reference/datalake/common/atom/#visibility","text":"class Visibility ( / , * args , ** kwargs ) View Source class Visibility ( Enum ): PUBLIC = 'PUBLIC' ORGANIZATION = 'ORGANIZATION'","title":"Visibility"},{"location":"reference/datalake/common/atom/#ancestors-in-mro_4","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom/#class-variables_4","text":"ORGANIZATION PUBLIC name value","title":"Class variables"},{"location":"reference/datalake/common/atom_type/","text":"Module datalake.common.atom_type None None View Source from abc import abstractmethod from dataclasses import dataclass , asdict from datalake.common.warn import Warn from typing import List , Dict @dataclass class Atom : \"\"\" Base class for atom types. \"\"\" pass def _factory ( self , data ): return dict ( x for x in data if x [ 1 ] is not None ) @abstractmethod def _get_sightings_allowed_keys ( self ): pass @abstractmethod def _get_sightings_prefix ( self ): pass def _sightings_factory ( self , data ): allowed = self . _get_sightings_allowed_keys () fact_dict = {} removed = False for x in data : if x [ 1 ] is not None : if x [ 0 ] in allowed : fact_dict [ x [ 0 ]] = x [ 1 ] else : removed = True if removed : Warn . warning ( \"Some keys aren't allowed for sightings and thus will be removed if you have set them. Check \" \"the classes for information on which keys are allowed. To stop this warning from showing, \" \"please set the IGNORE_SIGHTING_BUILDER_WARNING environment variable to True\" ) return fact_dict def generate_atom_json ( self , for_sightings = False ): \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API. \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' { prefix } _list' : [ asdict ( self , dict_factory = self . _sightings_factory )]} return { f ' { prefix } _content' : asdict ( self , dict_factory = self . _factory )} @dataclass class Hashes : \"\"\" At least one value is required \"\"\" md5 : str = None sha1 : str = None sha256 : str = None sha512 : str = None ssdeep : str = None @dataclass class FileAtom ( Atom ): \"\"\" Allowed sighting key: ['hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" hashes : Hashes external_analysis_link : List [ str ] = None filesize : int = None filetype : str = None file_url : str = None mimetype : str = None filename : str = None filepath : str = None def _get_sightings_allowed_keys ( self ): return [ 'hashes' , 'md5' , 'sha1' , 'sha256' , 'sha512' ] def _get_sightings_prefix ( self ): return 'file' @dataclass class AndroidApp : package_name : str app_name : str = None developer : str = None version_name : str = None permissions : str = None @dataclass class ApkAtom ( Atom ): \"\"\" Allowed sighting key: ['android', 'package_name', 'version_name', 'hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" android : AndroidApp hashes : Hashes external_analysis_link : List [ str ] = None filesize : int = None filetype : str = None file_url : str = None mimetype : str = None filename : str = None filepath : str = None def _get_sightings_allowed_keys ( self ): return [ 'android' , 'package_name' , 'version_name' , 'hashes' , 'md5' , 'sha1' , 'sha256' , 'sha512' ] def _get_sightings_prefix ( self ): return 'apk' @dataclass class AsAtom ( Atom ): \"\"\" Allowed sighting key: ['asn'] \"\"\" asn : int external_analysis_link : List [ str ] = None allocation_date : str = None country : str = None malware_family : str = None owner : str = None registry : str = None def _get_sightings_allowed_keys ( self ): return [ 'asn' ] def _get_sightings_prefix ( self ): return 'as' @dataclass class CcAtom ( Atom ): \"\"\" Allowed sighting key: ['number'] number: minLength = 8 maxLength = 19 \"\"\" number : str external_analysis_link : List [ str ] = None bin : int = None brand : str = None cvx : int = None description : str = None expiry_date : str = None def _get_sightings_allowed_keys ( self ): return [ 'number' ] def _get_sightings_prefix ( self ): return 'cc' @dataclass class CryptoAtom ( Atom ): \"\"\" Allowed sighting key: ['crypto_address', 'crypto_network'] \"\"\" \"\"\" first_used and last_used expect the following datetime format: '%Y-%m-%dT%H:%M:%SZ' \"\"\" crypto_address : str crypto_network : str external_analysis_link : List [ str ] = None number_of_transactions : int = None total_received : float = None total_sent : float = None first_used : str = None last_used : str = None def _get_sightings_allowed_keys ( self ): return [ 'crypto_address' , 'crypto_network' ] def _get_sightings_prefix ( self ): return 'crypto' @dataclass class CveAtom ( Atom ): \"\"\" Allowed sighting key: ['cve_id'] \"\"\" \"\"\" published_at expect the following datetime format: '%Y-%m-%dT%H:%M:%SZ' \"\"\" cve_id : str cvss : int = None cwe : str = None external_analysis_link : List [ str ] = None published_at : str = None def _get_sightings_allowed_keys ( self ): return [ 'cve_id' ] def _get_sightings_prefix ( self ): return 'cve' @dataclass class Jarm : calculated_at : str = None fingerprint : str = None malicious : bool = None malware_family : str = None @dataclass class DomainAtom ( Atom ): \"\"\" Allowed sighting key: ['domain'] \"\"\" domain : str external_analysis_link : List [ str ] = None malware_family : str = None jarm : Jarm = None def _get_sightings_allowed_keys ( self ): return [ 'domain' ] def _get_sightings_prefix ( self ): return 'domain' class EmailFlow : TO = 'to' FROM = 'from' @dataclass class EmailAtom ( Atom ): \"\"\" Allowed sighting key: ['email'] \"\"\" email : str email_flow : EmailFlow = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ): return [ 'email' ] def _get_sightings_prefix ( self ): return 'email' @dataclass class FqdnAtom ( Atom ): \"\"\" Allowed sighting key: ['fqdn'] \"\"\" fqdn : str jarm : Jarm = None malware_family : str = None port : List [ int ] = None ns : List [ str ] = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ): return [ 'fqdn' ] def _get_sightings_prefix ( self ): return 'fqdn' @dataclass class IbanAtom ( Atom ): \"\"\" Allowed sighting key: ['iban'] \"\"\" iban : str holder_name : str = None holder_address : str = None external_analysis_link : List [ str ] = None bic : str = None bank_name : str = None bank_address : str = None def _get_sightings_allowed_keys ( self ): return [ 'iban' ] def _get_sightings_prefix ( self ): return 'iban' @dataclass class IpService : port : int service_name : str application : str = None protocol : str = None @dataclass class IpAtom ( Atom ): \"\"\" Allowed sighting key: ['ip_address'] \"\"\" ip_address : str external_analysis_link : List [ str ] = None hostname : str = None ip_version : int = None jarm : Jarm = None malware_family : str = None owner : str = None peer_asns : List [ int ] = None services : List [ IpService ] = None def _get_sightings_allowed_keys ( self ): return [ 'ip_address' ] def _get_sightings_prefix ( self ): return 'ip' @dataclass class IpRangeAtom ( Atom ): \"\"\" Allowed sighting key: ['cidr'] \"\"\" cidr : str country : str = None allocation_date : str = None external_analysis_link : List [ str ] = None owner : str = None owner_description : str = None registry : str = None def _get_sightings_allowed_keys ( self ): return [ 'cidr' ] def _get_sightings_prefix ( self ): return 'ip_range' @dataclass class PasteAtom ( Atom ): \"\"\" Allowed sighting key: ['url'] \"\"\" url : str author : str = None title : str = None content : str = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ): return [ 'url' ] def _get_sightings_prefix ( self ): return 'paste' @dataclass class PhoneNumberAtom ( Atom ): \"\"\" Allowed sighting key: ['international_phone_number', 'national_phone_number'] \"\"\" \"\"\" country: minLength = 2 maxLength = 2 \"\"\" company : str = None country : str = None external_analysis_link : List [ str ] = None international_phone_number : str = None national_phone_number : str = None def _get_sightings_allowed_keys ( self ): return [ 'international_phone_number' , 'national_phone_number' ] def _get_sightings_prefix ( self ): return 'phone_number' @dataclass class RegKeyAtom ( Atom ): \"\"\" Allowed sighting key: ['path'] \"\"\" path : str regkey_value : str = None hive : str = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ): return [ 'path' ] def _get_sightings_prefix ( self ): return 'regkey' @dataclass class SslAtom ( Atom ): \"\"\" Allowed sighting key: ['hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" hashes : Hashes issuer : str = None public_key : str = None serial_number : str = None signature : str = None signature_algorithm : str = None subject : str = None valid_not_after : str = None valid_not_before : str = None external_analysis_link : str = None def _get_sightings_allowed_keys ( self ): return [ 'hashes' , 'md5' , 'sha1' , 'sha256' , 'sha512' ] def _get_sightings_prefix ( self ): return 'ssl' @dataclass class UrlAtom ( Atom ): \"\"\" Allowed sighting key: ['url'] \"\"\" url : str malware_family : str = None jarm : Jarm = None http_headers : Dict [ str , str ] = None http_code : int = None external_analysis_link : List [ str ] = None reason : str = None def _get_sightings_allowed_keys ( self ): return [ 'url' ] def _get_sightings_prefix ( self ): return 'url' Classes AndroidApp class AndroidApp ( package_name : str , app_name : str = None , developer : str = None , version_name : str = None , permissions : str = None ) View Source @dataclass class AndroidApp : package_name : str app_name : str = None developer : str = None version_name : str = None permissions : str = None Class variables app_name developer permissions version_name ApkAtom class ApkAtom ( android : datalake . common . atom_type . AndroidApp , hashes : datalake . common . atom_type . Hashes , external_analysis_link : List [ str ] = None , filesize : int = None , filetype : str = None , file_url : str = None , mimetype : str = None , filename : str = None , filepath : str = None ) View Source @dataclass class ApkAtom ( Atom ) : \"\"\" Allowed sighting key: ['android', 'package_name', 'version_name', 'hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" android : AndroidApp hashes : Hashes external_analysis_link : List [ str ] = None filesize : int = None filetype : str = None file_url : str = None mimetype : str = None filename : str = None filepath : str = None def _get_sightings_allowed_keys ( self ) : return [ 'android', 'package_name', 'version_name', 'hashes', 'md5', 'sha1', 'sha256', 'sha512' ] def _get_sightings_prefix ( self ) : return 'apk' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables external_analysis_link file_url filename filepath filesize filetype mimetype Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } AsAtom class AsAtom ( asn : int , external_analysis_link : List [ str ] = None , allocation_date : str = None , country : str = None , malware_family : str = None , owner : str = None , registry : str = None ) View Source @dataclass class AsAtom ( Atom ) : \"\"\" Allowed sighting key: ['asn'] \"\"\" asn : int external_analysis_link : List [ str ] = None allocation_date : str = None country : str = None malware_family : str = None owner : str = None registry : str = None def _get_sightings_allowed_keys ( self ) : return [ 'asn' ] def _get_sightings_prefix ( self ) : return 'as' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables allocation_date country external_analysis_link malware_family owner registry Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } Atom class Atom ( ) View Source @ dataclass class Atom : \"\"\" Base class for atom types. \"\"\" pass def _factory ( self , data ): return dict ( x for x in data if x [ 1 ] is not None ) @ abstractmethod def _get_sightings_allowed_keys ( self ): pass @ abstractmethod def _get_sightings_prefix ( self ): pass def _sightings_factory ( self , data ): allowed = self . _get_sightings_allowed_keys () fact_dict = {} removed = False for x in data : if x [ 1 ] is not None : if x [ 0 ] in allowed : fact_dict [ x [ 0 ]] = x [ 1 ] else : removed = True if removed : Warn . warning ( \"Some keys aren't allowed for sightings and thus will be removed if you have set them. Check \" \"the classes for information on which keys are allowed. To stop this warning from showing, \" \"please set the IGNORE_SIGHTING_BUILDER_WARNING environment variable to True\" ) return fact_dict def generate_atom_json ( self , for_sightings = False ): \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API. \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f '{prefix}_list' : [ asdict ( self , dict_factory = self . _sightings_factory )]} return { f '{prefix}_content' : asdict ( self , dict_factory = self . _factory )} Descendants datalake.common.atom_type.FileAtom datalake.common.atom_type.ApkAtom datalake.common.atom_type.AsAtom datalake.common.atom_type.CcAtom datalake.common.atom_type.CryptoAtom datalake.common.atom_type.CveAtom datalake.common.atom_type.DomainAtom datalake.common.atom_type.EmailAtom datalake.common.atom_type.FqdnAtom datalake.common.atom_type.IbanAtom datalake.common.atom_type.IpAtom datalake.common.atom_type.IpRangeAtom datalake.common.atom_type.PasteAtom datalake.common.atom_type.PhoneNumberAtom datalake.common.atom_type.RegKeyAtom datalake.common.atom_type.SslAtom datalake.common.atom_type.UrlAtom Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } CcAtom class CcAtom ( number : str , external_analysis_link : List [ str ] = None , bin : int = None , brand : str = None , cvx : int = None , description : str = None , expiry_date : str = None ) View Source @dataclass class CcAtom ( Atom ) : \"\"\" Allowed sighting key: ['number'] number: minLength = 8 maxLength = 19 \"\"\" number : str external_analysis_link : List [ str ] = None bin : int = None brand : str = None cvx : int = None description : str = None expiry_date : str = None def _get_sightings_allowed_keys ( self ) : return [ 'number' ] def _get_sightings_prefix ( self ) : return 'cc' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables bin brand cvx description expiry_date external_analysis_link Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } CryptoAtom class CryptoAtom ( crypto_address : str , crypto_network : str , external_analysis_link : List [ str ] = None , number_of_transactions : int = None , total_received : float = None , total_sent : float = None , first_used : str = None , last_used : str = None ) View Source @dataclass class CryptoAtom ( Atom ) : \"\"\" Allowed sighting key: ['crypto_address', 'crypto_network'] \"\"\" \"\"\" first_used and last_used expect the following datetime format: '%Y-%m-%dT%H:%M:%SZ' \"\"\" crypto_address : str crypto_network : str external_analysis_link : List [ str ] = None number_of_transactions : int = None total_received : float = None total_sent : float = None first_used : str = None last_used : str = None def _get_sightings_allowed_keys ( self ) : return [ 'crypto_address', 'crypto_network' ] def _get_sightings_prefix ( self ) : return 'crypto' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables external_analysis_link first_used last_used number_of_transactions total_received total_sent Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } CveAtom class CveAtom ( cve_id : str , cvss : int = None , cwe : str = None , external_analysis_link : List [ str ] = None , published_at : str = None ) View Source @dataclass class CveAtom ( Atom ) : \"\"\" Allowed sighting key: ['cve_id'] \"\"\" \"\"\" published_at expect the following datetime format: '%Y-%m-%dT%H:%M:%SZ' \"\"\" cve_id : str cvss : int = None cwe : str = None external_analysis_link : List [ str ] = None published_at : str = None def _get_sightings_allowed_keys ( self ) : return [ 'cve_id' ] def _get_sightings_prefix ( self ) : return 'cve' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables cvss cwe external_analysis_link published_at Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } DomainAtom class DomainAtom ( domain : str , external_analysis_link : List [ str ] = None , malware_family : str = None , jarm : datalake . common . atom_type . Jarm = None ) View Source @dataclass class DomainAtom ( Atom ) : \"\"\" Allowed sighting key: ['domain'] \"\"\" domain : str external_analysis_link : List [ str ] = None malware_family : str = None jarm : Jarm = None def _get_sightings_allowed_keys ( self ) : return [ 'domain' ] def _get_sightings_prefix ( self ) : return 'domain' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables external_analysis_link jarm malware_family Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } EmailAtom class EmailAtom ( email : str , email_flow : datalake . common . atom_type . EmailFlow = None , external_analysis_link : List [ str ] = None ) View Source @dataclass class EmailAtom ( Atom ) : \"\"\" Allowed sighting key: ['email'] \"\"\" email : str email_flow : EmailFlow = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ) : return [ 'email' ] def _get_sightings_prefix ( self ) : return 'email' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables email_flow external_analysis_link Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } EmailFlow class EmailFlow ( / , * args , ** kwargs ) View Source class EmailFlow: TO = 'to' FROM = 'from' Class variables FROM TO FileAtom class FileAtom ( hashes : datalake . common . atom_type . Hashes , external_analysis_link : List [ str ] = None , filesize : int = None , filetype : str = None , file_url : str = None , mimetype : str = None , filename : str = None , filepath : str = None ) View Source @dataclass class FileAtom ( Atom ) : \"\"\" Allowed sighting key: ['hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" hashes : Hashes external_analysis_link : List [ str ] = None filesize : int = None filetype : str = None file_url : str = None mimetype : str = None filename : str = None filepath : str = None def _get_sightings_allowed_keys ( self ) : return [ 'hashes', 'md5', 'sha1', 'sha256', 'sha512' ] def _get_sightings_prefix ( self ) : return 'file' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables external_analysis_link file_url filename filepath filesize filetype mimetype Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } FqdnAtom class FqdnAtom ( fqdn : str , jarm : datalake . common . atom_type . Jarm = None , malware_family : str = None , port : List [ int ] = None , ns : List [ str ] = None , external_analysis_link : List [ str ] = None ) View Source @dataclass class FqdnAtom ( Atom ) : \"\"\" Allowed sighting key: ['fqdn'] \"\"\" fqdn : str jarm : Jarm = None malware_family : str = None port : List [ int ] = None ns : List [ str ] = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ) : return [ 'fqdn' ] def _get_sightings_prefix ( self ) : return 'fqdn' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables external_analysis_link jarm malware_family ns port Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } Hashes class Hashes ( md5 : str = None , sha1 : str = None , sha256 : str = None , sha512 : str = None , ssdeep : str = None ) View Source @dataclass class Hashes : \"\"\" At least one value is required \"\"\" md5 : str = None sha1 : str = None sha256 : str = None sha512 : str = None ssdeep : str = None Class variables md5 sha1 sha256 sha512 ssdeep IbanAtom class IbanAtom ( iban : str , holder_name : str = None , holder_address : str = None , external_analysis_link : List [ str ] = None , bic : str = None , bank_name : str = None , bank_address : str = None ) View Source @dataclass class IbanAtom ( Atom ) : \"\"\" Allowed sighting key: ['iban'] \"\"\" iban : str holder_name : str = None holder_address : str = None external_analysis_link : List [ str ] = None bic : str = None bank_name : str = None bank_address : str = None def _get_sightings_allowed_keys ( self ) : return [ 'iban' ] def _get_sightings_prefix ( self ) : return 'iban' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables bank_address bank_name bic external_analysis_link holder_address holder_name Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } IpAtom class IpAtom ( ip_address : str , external_analysis_link : List [ str ] = None , hostname : str = None , ip_version : int = None , jarm : datalake . common . atom_type . Jarm = None , malware_family : str = None , owner : str = None , peer_asns : List [ int ] = None , services : List [ datalake . common . atom_type . IpService ] = None ) View Source @dataclass class IpAtom ( Atom ) : \"\"\" Allowed sighting key: ['ip_address'] \"\"\" ip_address : str external_analysis_link : List [ str ] = None hostname : str = None ip_version : int = None jarm : Jarm = None malware_family : str = None owner : str = None peer_asns : List [ int ] = None services : List [ IpService ] = None def _get_sightings_allowed_keys ( self ) : return [ 'ip_address' ] def _get_sightings_prefix ( self ) : return 'ip' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables external_analysis_link hostname ip_version jarm malware_family owner peer_asns services Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } IpRangeAtom class IpRangeAtom ( cidr : str , country : str = None , allocation_date : str = None , external_analysis_link : List [ str ] = None , owner : str = None , owner_description : str = None , registry : str = None ) View Source @dataclass class IpRangeAtom ( Atom ) : \"\"\" Allowed sighting key: ['cidr'] \"\"\" cidr : str country : str = None allocation_date : str = None external_analysis_link : List [ str ] = None owner : str = None owner_description : str = None registry : str = None def _get_sightings_allowed_keys ( self ) : return [ 'cidr' ] def _get_sightings_prefix ( self ) : return 'ip_range' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables allocation_date country external_analysis_link owner owner_description registry Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } IpService class IpService ( port : int , service_name : str , application : str = None , protocol : str = None ) View Source @dataclass class IpService : port : int service_name : str application : str = None protocol : str = None Class variables application protocol Jarm class Jarm ( calculated_at : str = None , fingerprint : str = None , malicious : bool = None , malware_family : str = None ) View Source @dataclass class Jarm : calculated_at : str = None fingerprint : str = None malicious : bool = None malware_family : str = None Class variables calculated_at fingerprint malicious malware_family PasteAtom class PasteAtom ( url : str , author : str = None , title : str = None , content : str = None , external_analysis_link : List [ str ] = None ) View Source @dataclass class PasteAtom ( Atom ) : \"\"\" Allowed sighting key: ['url'] \"\"\" url : str author : str = None title : str = None content : str = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ) : return [ 'url' ] def _get_sightings_prefix ( self ) : return 'paste' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables author content external_analysis_link title Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } PhoneNumberAtom class PhoneNumberAtom ( company : str = None , country : str = None , external_analysis_link : List [ str ] = None , international_phone_number : str = None , national_phone_number : str = None ) View Source @dataclass class PhoneNumberAtom ( Atom ) : \"\"\" Allowed sighting key: ['international_phone_number', 'national_phone_number'] \"\"\" \"\"\" country: minLength = 2 maxLength = 2 \"\"\" company : str = None country : str = None external_analysis_link : List [ str ] = None international_phone_number : str = None national_phone_number : str = None def _get_sightings_allowed_keys ( self ) : return [ 'international_phone_number', 'national_phone_number' ] def _get_sightings_prefix ( self ) : return 'phone_number' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables company country external_analysis_link international_phone_number national_phone_number Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } RegKeyAtom class RegKeyAtom ( path : str , regkey_value : str = None , hive : str = None , external_analysis_link : List [ str ] = None ) View Source @dataclass class RegKeyAtom ( Atom ) : \"\"\" Allowed sighting key: ['path'] \"\"\" path : str regkey_value : str = None hive : str = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ) : return [ 'path' ] def _get_sightings_prefix ( self ) : return 'regkey' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables external_analysis_link hive regkey_value Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } SslAtom class SslAtom ( hashes : datalake . common . atom_type . Hashes , issuer : str = None , public_key : str = None , serial_number : str = None , signature : str = None , signature_algorithm : str = None , subject : str = None , valid_not_after : str = None , valid_not_before : str = None , external_analysis_link : str = None ) View Source @dataclass class SslAtom ( Atom ) : \"\"\" Allowed sighting key: ['hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" hashes : Hashes issuer : str = None public_key : str = None serial_number : str = None signature : str = None signature_algorithm : str = None subject : str = None valid_not_after : str = None valid_not_before : str = None external_analysis_link : str = None def _get_sightings_allowed_keys ( self ) : return [ 'hashes', 'md5', 'sha1', 'sha256', 'sha512' ] def _get_sightings_prefix ( self ) : return 'ssl' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables external_analysis_link issuer public_key serial_number signature signature_algorithm subject valid_not_after valid_not_before Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) } UrlAtom class UrlAtom ( url : str , malware_family : str = None , jarm : datalake . common . atom_type . Jarm = None , http_headers : Dict [ str , str ] = None , http_code : int = None , external_analysis_link : List [ str ] = None , reason : str = None ) View Source @dataclass class UrlAtom ( Atom ) : \"\"\" Allowed sighting key: ['url'] \"\"\" url : str malware_family : str = None jarm : Jarm = None http_headers : Dict [ str, str ] = None http_code : int = None external_analysis_link : List [ str ] = None reason : str = None def _get_sightings_allowed_keys ( self ) : return [ 'url' ] def _get_sightings_prefix ( self ) : return 'url' Ancestors (in MRO) datalake.common.atom_type.Atom Class variables external_analysis_link http_code http_headers jarm malware_family reason Methods generate_atom_json def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"Atom Type"},{"location":"reference/datalake/common/atom_type/#module-datalakecommonatom_type","text":"None None View Source from abc import abstractmethod from dataclasses import dataclass , asdict from datalake.common.warn import Warn from typing import List , Dict @dataclass class Atom : \"\"\" Base class for atom types. \"\"\" pass def _factory ( self , data ): return dict ( x for x in data if x [ 1 ] is not None ) @abstractmethod def _get_sightings_allowed_keys ( self ): pass @abstractmethod def _get_sightings_prefix ( self ): pass def _sightings_factory ( self , data ): allowed = self . _get_sightings_allowed_keys () fact_dict = {} removed = False for x in data : if x [ 1 ] is not None : if x [ 0 ] in allowed : fact_dict [ x [ 0 ]] = x [ 1 ] else : removed = True if removed : Warn . warning ( \"Some keys aren't allowed for sightings and thus will be removed if you have set them. Check \" \"the classes for information on which keys are allowed. To stop this warning from showing, \" \"please set the IGNORE_SIGHTING_BUILDER_WARNING environment variable to True\" ) return fact_dict def generate_atom_json ( self , for_sightings = False ): \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API. \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' { prefix } _list' : [ asdict ( self , dict_factory = self . _sightings_factory )]} return { f ' { prefix } _content' : asdict ( self , dict_factory = self . _factory )} @dataclass class Hashes : \"\"\" At least one value is required \"\"\" md5 : str = None sha1 : str = None sha256 : str = None sha512 : str = None ssdeep : str = None @dataclass class FileAtom ( Atom ): \"\"\" Allowed sighting key: ['hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" hashes : Hashes external_analysis_link : List [ str ] = None filesize : int = None filetype : str = None file_url : str = None mimetype : str = None filename : str = None filepath : str = None def _get_sightings_allowed_keys ( self ): return [ 'hashes' , 'md5' , 'sha1' , 'sha256' , 'sha512' ] def _get_sightings_prefix ( self ): return 'file' @dataclass class AndroidApp : package_name : str app_name : str = None developer : str = None version_name : str = None permissions : str = None @dataclass class ApkAtom ( Atom ): \"\"\" Allowed sighting key: ['android', 'package_name', 'version_name', 'hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" android : AndroidApp hashes : Hashes external_analysis_link : List [ str ] = None filesize : int = None filetype : str = None file_url : str = None mimetype : str = None filename : str = None filepath : str = None def _get_sightings_allowed_keys ( self ): return [ 'android' , 'package_name' , 'version_name' , 'hashes' , 'md5' , 'sha1' , 'sha256' , 'sha512' ] def _get_sightings_prefix ( self ): return 'apk' @dataclass class AsAtom ( Atom ): \"\"\" Allowed sighting key: ['asn'] \"\"\" asn : int external_analysis_link : List [ str ] = None allocation_date : str = None country : str = None malware_family : str = None owner : str = None registry : str = None def _get_sightings_allowed_keys ( self ): return [ 'asn' ] def _get_sightings_prefix ( self ): return 'as' @dataclass class CcAtom ( Atom ): \"\"\" Allowed sighting key: ['number'] number: minLength = 8 maxLength = 19 \"\"\" number : str external_analysis_link : List [ str ] = None bin : int = None brand : str = None cvx : int = None description : str = None expiry_date : str = None def _get_sightings_allowed_keys ( self ): return [ 'number' ] def _get_sightings_prefix ( self ): return 'cc' @dataclass class CryptoAtom ( Atom ): \"\"\" Allowed sighting key: ['crypto_address', 'crypto_network'] \"\"\" \"\"\" first_used and last_used expect the following datetime format: '%Y-%m-%dT%H:%M:%SZ' \"\"\" crypto_address : str crypto_network : str external_analysis_link : List [ str ] = None number_of_transactions : int = None total_received : float = None total_sent : float = None first_used : str = None last_used : str = None def _get_sightings_allowed_keys ( self ): return [ 'crypto_address' , 'crypto_network' ] def _get_sightings_prefix ( self ): return 'crypto' @dataclass class CveAtom ( Atom ): \"\"\" Allowed sighting key: ['cve_id'] \"\"\" \"\"\" published_at expect the following datetime format: '%Y-%m-%dT%H:%M:%SZ' \"\"\" cve_id : str cvss : int = None cwe : str = None external_analysis_link : List [ str ] = None published_at : str = None def _get_sightings_allowed_keys ( self ): return [ 'cve_id' ] def _get_sightings_prefix ( self ): return 'cve' @dataclass class Jarm : calculated_at : str = None fingerprint : str = None malicious : bool = None malware_family : str = None @dataclass class DomainAtom ( Atom ): \"\"\" Allowed sighting key: ['domain'] \"\"\" domain : str external_analysis_link : List [ str ] = None malware_family : str = None jarm : Jarm = None def _get_sightings_allowed_keys ( self ): return [ 'domain' ] def _get_sightings_prefix ( self ): return 'domain' class EmailFlow : TO = 'to' FROM = 'from' @dataclass class EmailAtom ( Atom ): \"\"\" Allowed sighting key: ['email'] \"\"\" email : str email_flow : EmailFlow = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ): return [ 'email' ] def _get_sightings_prefix ( self ): return 'email' @dataclass class FqdnAtom ( Atom ): \"\"\" Allowed sighting key: ['fqdn'] \"\"\" fqdn : str jarm : Jarm = None malware_family : str = None port : List [ int ] = None ns : List [ str ] = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ): return [ 'fqdn' ] def _get_sightings_prefix ( self ): return 'fqdn' @dataclass class IbanAtom ( Atom ): \"\"\" Allowed sighting key: ['iban'] \"\"\" iban : str holder_name : str = None holder_address : str = None external_analysis_link : List [ str ] = None bic : str = None bank_name : str = None bank_address : str = None def _get_sightings_allowed_keys ( self ): return [ 'iban' ] def _get_sightings_prefix ( self ): return 'iban' @dataclass class IpService : port : int service_name : str application : str = None protocol : str = None @dataclass class IpAtom ( Atom ): \"\"\" Allowed sighting key: ['ip_address'] \"\"\" ip_address : str external_analysis_link : List [ str ] = None hostname : str = None ip_version : int = None jarm : Jarm = None malware_family : str = None owner : str = None peer_asns : List [ int ] = None services : List [ IpService ] = None def _get_sightings_allowed_keys ( self ): return [ 'ip_address' ] def _get_sightings_prefix ( self ): return 'ip' @dataclass class IpRangeAtom ( Atom ): \"\"\" Allowed sighting key: ['cidr'] \"\"\" cidr : str country : str = None allocation_date : str = None external_analysis_link : List [ str ] = None owner : str = None owner_description : str = None registry : str = None def _get_sightings_allowed_keys ( self ): return [ 'cidr' ] def _get_sightings_prefix ( self ): return 'ip_range' @dataclass class PasteAtom ( Atom ): \"\"\" Allowed sighting key: ['url'] \"\"\" url : str author : str = None title : str = None content : str = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ): return [ 'url' ] def _get_sightings_prefix ( self ): return 'paste' @dataclass class PhoneNumberAtom ( Atom ): \"\"\" Allowed sighting key: ['international_phone_number', 'national_phone_number'] \"\"\" \"\"\" country: minLength = 2 maxLength = 2 \"\"\" company : str = None country : str = None external_analysis_link : List [ str ] = None international_phone_number : str = None national_phone_number : str = None def _get_sightings_allowed_keys ( self ): return [ 'international_phone_number' , 'national_phone_number' ] def _get_sightings_prefix ( self ): return 'phone_number' @dataclass class RegKeyAtom ( Atom ): \"\"\" Allowed sighting key: ['path'] \"\"\" path : str regkey_value : str = None hive : str = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ): return [ 'path' ] def _get_sightings_prefix ( self ): return 'regkey' @dataclass class SslAtom ( Atom ): \"\"\" Allowed sighting key: ['hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" hashes : Hashes issuer : str = None public_key : str = None serial_number : str = None signature : str = None signature_algorithm : str = None subject : str = None valid_not_after : str = None valid_not_before : str = None external_analysis_link : str = None def _get_sightings_allowed_keys ( self ): return [ 'hashes' , 'md5' , 'sha1' , 'sha256' , 'sha512' ] def _get_sightings_prefix ( self ): return 'ssl' @dataclass class UrlAtom ( Atom ): \"\"\" Allowed sighting key: ['url'] \"\"\" url : str malware_family : str = None jarm : Jarm = None http_headers : Dict [ str , str ] = None http_code : int = None external_analysis_link : List [ str ] = None reason : str = None def _get_sightings_allowed_keys ( self ): return [ 'url' ] def _get_sightings_prefix ( self ): return 'url'","title":"Module datalake.common.atom_type"},{"location":"reference/datalake/common/atom_type/#classes","text":"","title":"Classes"},{"location":"reference/datalake/common/atom_type/#androidapp","text":"class AndroidApp ( package_name : str , app_name : str = None , developer : str = None , version_name : str = None , permissions : str = None ) View Source @dataclass class AndroidApp : package_name : str app_name : str = None developer : str = None version_name : str = None permissions : str = None","title":"AndroidApp"},{"location":"reference/datalake/common/atom_type/#class-variables","text":"app_name developer permissions version_name","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#apkatom","text":"class ApkAtom ( android : datalake . common . atom_type . AndroidApp , hashes : datalake . common . atom_type . Hashes , external_analysis_link : List [ str ] = None , filesize : int = None , filetype : str = None , file_url : str = None , mimetype : str = None , filename : str = None , filepath : str = None ) View Source @dataclass class ApkAtom ( Atom ) : \"\"\" Allowed sighting key: ['android', 'package_name', 'version_name', 'hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" android : AndroidApp hashes : Hashes external_analysis_link : List [ str ] = None filesize : int = None filetype : str = None file_url : str = None mimetype : str = None filename : str = None filepath : str = None def _get_sightings_allowed_keys ( self ) : return [ 'android', 'package_name', 'version_name', 'hashes', 'md5', 'sha1', 'sha256', 'sha512' ] def _get_sightings_prefix ( self ) : return 'apk'","title":"ApkAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_1","text":"external_analysis_link file_url filename filepath filesize filetype mimetype","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#asatom","text":"class AsAtom ( asn : int , external_analysis_link : List [ str ] = None , allocation_date : str = None , country : str = None , malware_family : str = None , owner : str = None , registry : str = None ) View Source @dataclass class AsAtom ( Atom ) : \"\"\" Allowed sighting key: ['asn'] \"\"\" asn : int external_analysis_link : List [ str ] = None allocation_date : str = None country : str = None malware_family : str = None owner : str = None registry : str = None def _get_sightings_allowed_keys ( self ) : return [ 'asn' ] def _get_sightings_prefix ( self ) : return 'as'","title":"AsAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_1","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_2","text":"allocation_date country external_analysis_link malware_family owner registry","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_1","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_1","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#atom","text":"class Atom ( ) View Source @ dataclass class Atom : \"\"\" Base class for atom types. \"\"\" pass def _factory ( self , data ): return dict ( x for x in data if x [ 1 ] is not None ) @ abstractmethod def _get_sightings_allowed_keys ( self ): pass @ abstractmethod def _get_sightings_prefix ( self ): pass def _sightings_factory ( self , data ): allowed = self . _get_sightings_allowed_keys () fact_dict = {} removed = False for x in data : if x [ 1 ] is not None : if x [ 0 ] in allowed : fact_dict [ x [ 0 ]] = x [ 1 ] else : removed = True if removed : Warn . warning ( \"Some keys aren't allowed for sightings and thus will be removed if you have set them. Check \" \"the classes for information on which keys are allowed. To stop this warning from showing, \" \"please set the IGNORE_SIGHTING_BUILDER_WARNING environment variable to True\" ) return fact_dict def generate_atom_json ( self , for_sightings = False ): \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API. \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f '{prefix}_list' : [ asdict ( self , dict_factory = self . _sightings_factory )]} return { f '{prefix}_content' : asdict ( self , dict_factory = self . _factory )}","title":"Atom"},{"location":"reference/datalake/common/atom_type/#descendants","text":"datalake.common.atom_type.FileAtom datalake.common.atom_type.ApkAtom datalake.common.atom_type.AsAtom datalake.common.atom_type.CcAtom datalake.common.atom_type.CryptoAtom datalake.common.atom_type.CveAtom datalake.common.atom_type.DomainAtom datalake.common.atom_type.EmailAtom datalake.common.atom_type.FqdnAtom datalake.common.atom_type.IbanAtom datalake.common.atom_type.IpAtom datalake.common.atom_type.IpRangeAtom datalake.common.atom_type.PasteAtom datalake.common.atom_type.PhoneNumberAtom datalake.common.atom_type.RegKeyAtom datalake.common.atom_type.SslAtom datalake.common.atom_type.UrlAtom","title":"Descendants"},{"location":"reference/datalake/common/atom_type/#methods_2","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_2","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#ccatom","text":"class CcAtom ( number : str , external_analysis_link : List [ str ] = None , bin : int = None , brand : str = None , cvx : int = None , description : str = None , expiry_date : str = None ) View Source @dataclass class CcAtom ( Atom ) : \"\"\" Allowed sighting key: ['number'] number: minLength = 8 maxLength = 19 \"\"\" number : str external_analysis_link : List [ str ] = None bin : int = None brand : str = None cvx : int = None description : str = None expiry_date : str = None def _get_sightings_allowed_keys ( self ) : return [ 'number' ] def _get_sightings_prefix ( self ) : return 'cc'","title":"CcAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_2","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_3","text":"bin brand cvx description expiry_date external_analysis_link","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_3","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_3","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#cryptoatom","text":"class CryptoAtom ( crypto_address : str , crypto_network : str , external_analysis_link : List [ str ] = None , number_of_transactions : int = None , total_received : float = None , total_sent : float = None , first_used : str = None , last_used : str = None ) View Source @dataclass class CryptoAtom ( Atom ) : \"\"\" Allowed sighting key: ['crypto_address', 'crypto_network'] \"\"\" \"\"\" first_used and last_used expect the following datetime format: '%Y-%m-%dT%H:%M:%SZ' \"\"\" crypto_address : str crypto_network : str external_analysis_link : List [ str ] = None number_of_transactions : int = None total_received : float = None total_sent : float = None first_used : str = None last_used : str = None def _get_sightings_allowed_keys ( self ) : return [ 'crypto_address', 'crypto_network' ] def _get_sightings_prefix ( self ) : return 'crypto'","title":"CryptoAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_3","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_4","text":"external_analysis_link first_used last_used number_of_transactions total_received total_sent","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_4","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_4","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#cveatom","text":"class CveAtom ( cve_id : str , cvss : int = None , cwe : str = None , external_analysis_link : List [ str ] = None , published_at : str = None ) View Source @dataclass class CveAtom ( Atom ) : \"\"\" Allowed sighting key: ['cve_id'] \"\"\" \"\"\" published_at expect the following datetime format: '%Y-%m-%dT%H:%M:%SZ' \"\"\" cve_id : str cvss : int = None cwe : str = None external_analysis_link : List [ str ] = None published_at : str = None def _get_sightings_allowed_keys ( self ) : return [ 'cve_id' ] def _get_sightings_prefix ( self ) : return 'cve'","title":"CveAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_4","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_5","text":"cvss cwe external_analysis_link published_at","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_5","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_5","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#domainatom","text":"class DomainAtom ( domain : str , external_analysis_link : List [ str ] = None , malware_family : str = None , jarm : datalake . common . atom_type . Jarm = None ) View Source @dataclass class DomainAtom ( Atom ) : \"\"\" Allowed sighting key: ['domain'] \"\"\" domain : str external_analysis_link : List [ str ] = None malware_family : str = None jarm : Jarm = None def _get_sightings_allowed_keys ( self ) : return [ 'domain' ] def _get_sightings_prefix ( self ) : return 'domain'","title":"DomainAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_5","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_6","text":"external_analysis_link jarm malware_family","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_6","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_6","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#emailatom","text":"class EmailAtom ( email : str , email_flow : datalake . common . atom_type . EmailFlow = None , external_analysis_link : List [ str ] = None ) View Source @dataclass class EmailAtom ( Atom ) : \"\"\" Allowed sighting key: ['email'] \"\"\" email : str email_flow : EmailFlow = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ) : return [ 'email' ] def _get_sightings_prefix ( self ) : return 'email'","title":"EmailAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_6","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_7","text":"email_flow external_analysis_link","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_7","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_7","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#emailflow","text":"class EmailFlow ( / , * args , ** kwargs ) View Source class EmailFlow: TO = 'to' FROM = 'from'","title":"EmailFlow"},{"location":"reference/datalake/common/atom_type/#class-variables_8","text":"FROM TO","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#fileatom","text":"class FileAtom ( hashes : datalake . common . atom_type . Hashes , external_analysis_link : List [ str ] = None , filesize : int = None , filetype : str = None , file_url : str = None , mimetype : str = None , filename : str = None , filepath : str = None ) View Source @dataclass class FileAtom ( Atom ) : \"\"\" Allowed sighting key: ['hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" hashes : Hashes external_analysis_link : List [ str ] = None filesize : int = None filetype : str = None file_url : str = None mimetype : str = None filename : str = None filepath : str = None def _get_sightings_allowed_keys ( self ) : return [ 'hashes', 'md5', 'sha1', 'sha256', 'sha512' ] def _get_sightings_prefix ( self ) : return 'file'","title":"FileAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_7","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_9","text":"external_analysis_link file_url filename filepath filesize filetype mimetype","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_8","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_8","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#fqdnatom","text":"class FqdnAtom ( fqdn : str , jarm : datalake . common . atom_type . Jarm = None , malware_family : str = None , port : List [ int ] = None , ns : List [ str ] = None , external_analysis_link : List [ str ] = None ) View Source @dataclass class FqdnAtom ( Atom ) : \"\"\" Allowed sighting key: ['fqdn'] \"\"\" fqdn : str jarm : Jarm = None malware_family : str = None port : List [ int ] = None ns : List [ str ] = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ) : return [ 'fqdn' ] def _get_sightings_prefix ( self ) : return 'fqdn'","title":"FqdnAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_8","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_10","text":"external_analysis_link jarm malware_family ns port","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_9","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_9","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#hashes","text":"class Hashes ( md5 : str = None , sha1 : str = None , sha256 : str = None , sha512 : str = None , ssdeep : str = None ) View Source @dataclass class Hashes : \"\"\" At least one value is required \"\"\" md5 : str = None sha1 : str = None sha256 : str = None sha512 : str = None ssdeep : str = None","title":"Hashes"},{"location":"reference/datalake/common/atom_type/#class-variables_11","text":"md5 sha1 sha256 sha512 ssdeep","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#ibanatom","text":"class IbanAtom ( iban : str , holder_name : str = None , holder_address : str = None , external_analysis_link : List [ str ] = None , bic : str = None , bank_name : str = None , bank_address : str = None ) View Source @dataclass class IbanAtom ( Atom ) : \"\"\" Allowed sighting key: ['iban'] \"\"\" iban : str holder_name : str = None holder_address : str = None external_analysis_link : List [ str ] = None bic : str = None bank_name : str = None bank_address : str = None def _get_sightings_allowed_keys ( self ) : return [ 'iban' ] def _get_sightings_prefix ( self ) : return 'iban'","title":"IbanAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_9","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_12","text":"bank_address bank_name bic external_analysis_link holder_address holder_name","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_10","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_10","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#ipatom","text":"class IpAtom ( ip_address : str , external_analysis_link : List [ str ] = None , hostname : str = None , ip_version : int = None , jarm : datalake . common . atom_type . Jarm = None , malware_family : str = None , owner : str = None , peer_asns : List [ int ] = None , services : List [ datalake . common . atom_type . IpService ] = None ) View Source @dataclass class IpAtom ( Atom ) : \"\"\" Allowed sighting key: ['ip_address'] \"\"\" ip_address : str external_analysis_link : List [ str ] = None hostname : str = None ip_version : int = None jarm : Jarm = None malware_family : str = None owner : str = None peer_asns : List [ int ] = None services : List [ IpService ] = None def _get_sightings_allowed_keys ( self ) : return [ 'ip_address' ] def _get_sightings_prefix ( self ) : return 'ip'","title":"IpAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_10","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_13","text":"external_analysis_link hostname ip_version jarm malware_family owner peer_asns services","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_11","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_11","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#iprangeatom","text":"class IpRangeAtom ( cidr : str , country : str = None , allocation_date : str = None , external_analysis_link : List [ str ] = None , owner : str = None , owner_description : str = None , registry : str = None ) View Source @dataclass class IpRangeAtom ( Atom ) : \"\"\" Allowed sighting key: ['cidr'] \"\"\" cidr : str country : str = None allocation_date : str = None external_analysis_link : List [ str ] = None owner : str = None owner_description : str = None registry : str = None def _get_sightings_allowed_keys ( self ) : return [ 'cidr' ] def _get_sightings_prefix ( self ) : return 'ip_range'","title":"IpRangeAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_11","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_14","text":"allocation_date country external_analysis_link owner owner_description registry","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_12","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_12","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#ipservice","text":"class IpService ( port : int , service_name : str , application : str = None , protocol : str = None ) View Source @dataclass class IpService : port : int service_name : str application : str = None protocol : str = None","title":"IpService"},{"location":"reference/datalake/common/atom_type/#class-variables_15","text":"application protocol","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#jarm","text":"class Jarm ( calculated_at : str = None , fingerprint : str = None , malicious : bool = None , malware_family : str = None ) View Source @dataclass class Jarm : calculated_at : str = None fingerprint : str = None malicious : bool = None malware_family : str = None","title":"Jarm"},{"location":"reference/datalake/common/atom_type/#class-variables_16","text":"calculated_at fingerprint malicious malware_family","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#pasteatom","text":"class PasteAtom ( url : str , author : str = None , title : str = None , content : str = None , external_analysis_link : List [ str ] = None ) View Source @dataclass class PasteAtom ( Atom ) : \"\"\" Allowed sighting key: ['url'] \"\"\" url : str author : str = None title : str = None content : str = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ) : return [ 'url' ] def _get_sightings_prefix ( self ) : return 'paste'","title":"PasteAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_12","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_17","text":"author content external_analysis_link title","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_13","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_13","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#phonenumberatom","text":"class PhoneNumberAtom ( company : str = None , country : str = None , external_analysis_link : List [ str ] = None , international_phone_number : str = None , national_phone_number : str = None ) View Source @dataclass class PhoneNumberAtom ( Atom ) : \"\"\" Allowed sighting key: ['international_phone_number', 'national_phone_number'] \"\"\" \"\"\" country: minLength = 2 maxLength = 2 \"\"\" company : str = None country : str = None external_analysis_link : List [ str ] = None international_phone_number : str = None national_phone_number : str = None def _get_sightings_allowed_keys ( self ) : return [ 'international_phone_number', 'national_phone_number' ] def _get_sightings_prefix ( self ) : return 'phone_number'","title":"PhoneNumberAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_13","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_18","text":"company country external_analysis_link international_phone_number national_phone_number","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_14","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_14","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#regkeyatom","text":"class RegKeyAtom ( path : str , regkey_value : str = None , hive : str = None , external_analysis_link : List [ str ] = None ) View Source @dataclass class RegKeyAtom ( Atom ) : \"\"\" Allowed sighting key: ['path'] \"\"\" path : str regkey_value : str = None hive : str = None external_analysis_link : List [ str ] = None def _get_sightings_allowed_keys ( self ) : return [ 'path' ] def _get_sightings_prefix ( self ) : return 'regkey'","title":"RegKeyAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_14","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_19","text":"external_analysis_link hive regkey_value","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_15","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_15","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#sslatom","text":"class SslAtom ( hashes : datalake . common . atom_type . Hashes , issuer : str = None , public_key : str = None , serial_number : str = None , signature : str = None , signature_algorithm : str = None , subject : str = None , valid_not_after : str = None , valid_not_before : str = None , external_analysis_link : str = None ) View Source @dataclass class SslAtom ( Atom ) : \"\"\" Allowed sighting key: ['hashes', 'md5', 'sha1', 'sha256', 'sha512'] \"\"\" hashes : Hashes issuer : str = None public_key : str = None serial_number : str = None signature : str = None signature_algorithm : str = None subject : str = None valid_not_after : str = None valid_not_before : str = None external_analysis_link : str = None def _get_sightings_allowed_keys ( self ) : return [ 'hashes', 'md5', 'sha1', 'sha256', 'sha512' ] def _get_sightings_prefix ( self ) : return 'ssl'","title":"SslAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_15","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_20","text":"external_analysis_link issuer public_key serial_number signature signature_algorithm subject valid_not_after valid_not_before","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_16","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_16","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/atom_type/#urlatom","text":"class UrlAtom ( url : str , malware_family : str = None , jarm : datalake . common . atom_type . Jarm = None , http_headers : Dict [ str , str ] = None , http_code : int = None , external_analysis_link : List [ str ] = None , reason : str = None ) View Source @dataclass class UrlAtom ( Atom ) : \"\"\" Allowed sighting key: ['url'] \"\"\" url : str malware_family : str = None jarm : Jarm = None http_headers : Dict [ str, str ] = None http_code : int = None external_analysis_link : List [ str ] = None reason : str = None def _get_sightings_allowed_keys ( self ) : return [ 'url' ] def _get_sightings_prefix ( self ) : return 'url'","title":"UrlAtom"},{"location":"reference/datalake/common/atom_type/#ancestors-in-mro_16","text":"datalake.common.atom_type.Atom","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/atom_type/#class-variables_21","text":"external_analysis_link http_code http_headers jarm malware_family reason","title":"Class variables"},{"location":"reference/datalake/common/atom_type/#methods_17","text":"","title":"Methods"},{"location":"reference/datalake/common/atom_type/#generate_atom_json_17","text":"def generate_atom_json ( self , for_sightings = False ) Utility method to returns a filtered json from the data given to the atom class for the API. View Source def generate_atom_json ( self , for_sightings = False ) : \"\"\" Utility method to returns a filtered json from the data given to the atom class for the API . \"\"\" prefix = self . _get_sightings_prefix () if for_sightings : return { f ' {prefix}_list ' : [ asdict ( self , dict_factory = self . _sightings_factory ) ]} return { f ' {prefix}_content ' : asdict ( self , dict_factory = self . _factory ) }","title":"generate_atom_json"},{"location":"reference/datalake/common/config/","text":"Module datalake.common.config None None View Source import json import os FOLDER_ABSOLUTE_PATH = os . path . normpath ( os . path . dirname ( os . path . abspath ( __file__ ))) class Config : _CONFIG_ENDPOINTS = os . path . join ( FOLDER_ABSOLUTE_PATH , '..' , 'config' , 'endpoints.json' ) def load_config ( self ): \"\"\" Load correct config and generate first tokens :return (dict, str, list<str, str>) \"\"\" with open ( self . _CONFIG_ENDPOINTS , 'r' ) as config : return json . load ( config ) Variables FOLDER_ABSOLUTE_PATH Classes Config class Config ( / , * args , ** kwargs ) View Source class Config : _CONFIG_ENDPOINTS = os . path . join ( FOLDER_ABSOLUTE_PATH , '..' , 'config' , 'endpoints.json' ) def load_config ( self ): \"\"\" Load correct config and generate first tokens :return (dict, str, list<str, str>) \"\"\" with open ( self . _CONFIG_ENDPOINTS , 'r' ) as config : return json . load ( config ) Methods load_config def load_config ( self ) Load correct config and generate first tokens :return (dict, str, list ) View Source def load_config ( self ): \"\"\" Load correct config and generate first tokens :return (dict, str, list<str, str>) \"\"\" with open ( self . _CONFIG_ENDPOINTS , 'r' ) as config : return json . load ( config )","title":"Config"},{"location":"reference/datalake/common/config/#module-datalakecommonconfig","text":"None None View Source import json import os FOLDER_ABSOLUTE_PATH = os . path . normpath ( os . path . dirname ( os . path . abspath ( __file__ ))) class Config : _CONFIG_ENDPOINTS = os . path . join ( FOLDER_ABSOLUTE_PATH , '..' , 'config' , 'endpoints.json' ) def load_config ( self ): \"\"\" Load correct config and generate first tokens :return (dict, str, list<str, str>) \"\"\" with open ( self . _CONFIG_ENDPOINTS , 'r' ) as config : return json . load ( config )","title":"Module datalake.common.config"},{"location":"reference/datalake/common/config/#variables","text":"FOLDER_ABSOLUTE_PATH","title":"Variables"},{"location":"reference/datalake/common/config/#classes","text":"","title":"Classes"},{"location":"reference/datalake/common/config/#config","text":"class Config ( / , * args , ** kwargs ) View Source class Config : _CONFIG_ENDPOINTS = os . path . join ( FOLDER_ABSOLUTE_PATH , '..' , 'config' , 'endpoints.json' ) def load_config ( self ): \"\"\" Load correct config and generate first tokens :return (dict, str, list<str, str>) \"\"\" with open ( self . _CONFIG_ENDPOINTS , 'r' ) as config : return json . load ( config )","title":"Config"},{"location":"reference/datalake/common/config/#methods","text":"","title":"Methods"},{"location":"reference/datalake/common/config/#load_config","text":"def load_config ( self ) Load correct config and generate first tokens :return (dict, str, list ) View Source def load_config ( self ): \"\"\" Load correct config and generate first tokens :return (dict, str, list<str, str>) \"\"\" with open ( self . _CONFIG_ENDPOINTS , 'r' ) as config : return json . load ( config )","title":"load_config"},{"location":"reference/datalake/common/logger/","text":"Module datalake.common.logger None None View Source import logging logger = logging . getLogger ( \"OCD_DTL\" ) DEBUG_FORMAT = ' %(name)s : %(levelname)-4s %(message)s ' INFO_FORMAT = ' %(message)s ' def configure_logging ( loglevel : int ): handler = logging . StreamHandler () if loglevel == logging . DEBUG : formatter = logging . Formatter ( DEBUG_FORMAT ) else : formatter = logging . Formatter ( INFO_FORMAT ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) logger . setLevel ( loglevel ) Variables DEBUG_FORMAT INFO_FORMAT logger Functions configure_logging def configure_logging ( loglevel : int ) View Source def configure_logging ( loglevel : int ) : handler = logging . StreamHandler () if loglevel == logging . DEBUG : formatter = logging . Formatter ( DEBUG_FORMAT ) else : formatter = logging . Formatter ( INFO_FORMAT ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) logger . setLevel ( loglevel )","title":"Logger"},{"location":"reference/datalake/common/logger/#module-datalakecommonlogger","text":"None None View Source import logging logger = logging . getLogger ( \"OCD_DTL\" ) DEBUG_FORMAT = ' %(name)s : %(levelname)-4s %(message)s ' INFO_FORMAT = ' %(message)s ' def configure_logging ( loglevel : int ): handler = logging . StreamHandler () if loglevel == logging . DEBUG : formatter = logging . Formatter ( DEBUG_FORMAT ) else : formatter = logging . Formatter ( INFO_FORMAT ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) logger . setLevel ( loglevel )","title":"Module datalake.common.logger"},{"location":"reference/datalake/common/logger/#variables","text":"DEBUG_FORMAT INFO_FORMAT logger","title":"Variables"},{"location":"reference/datalake/common/logger/#functions","text":"","title":"Functions"},{"location":"reference/datalake/common/logger/#configure_logging","text":"def configure_logging ( loglevel : int ) View Source def configure_logging ( loglevel : int ) : handler = logging . StreamHandler () if loglevel == logging . DEBUG : formatter = logging . Formatter ( DEBUG_FORMAT ) else : formatter = logging . Formatter ( INFO_FORMAT ) handler . setFormatter ( formatter ) logger . addHandler ( handler ) logger . setLevel ( loglevel )","title":"configure_logging"},{"location":"reference/datalake/common/ouput/","text":"Module datalake.common.ouput None None View Source from enum import Enum from typing import Set , Union from requests import Response class Output ( Enum ): JSON = 'application/json' CSV = 'text/csv' MISP = 'application/x-misp+json' STIX = 'application/stix+json' JSON_ZIP = 'application/zip' CSV_ZIP = 'text/x-csv-zip' STIX_ZIP = 'text/x-stix-zip' def __str__ ( self ): return self . name def __repr__ ( self ): return str ( self ) def parse_response ( response : Response ) -> Union [ str , dict ]: \"\"\"Parse a Request.Response depending of the Content-Type returned\"\"\" content_type = response . headers . get ( 'Content-Type' , Output . JSON . value ) content_type = content_type . split ( ';' )[ 0 ] # we don't care about extra info on the content if content_type in { output . value for output in [ Output . CSV , Output . CSV_ZIP , Output . JSON_ZIP ]}: return response . text else : return response . json () def display_outputs ( outputs ): return ', ' . join ( sorted ([ str ( output ) for output in outputs ])) def output_supported ( outputs : Set [ Output ]): def inner_decorator ( function ): def wrapper ( * args , ** kwargs ): if kwargs . get ( 'output' ) and kwargs [ 'output' ] not in outputs : raise ValueError ( f ' { kwargs [ \"output\" ] } output type is not supported. ' f 'Outputs supported are: { display_outputs ( outputs ) } ' ) return function ( * args , ** kwargs ) return wrapper return inner_decorator Functions display_outputs def display_outputs ( outputs ) View Source def display_outputs ( outputs ) : return ' , ' . join ( sorted ( [ str ( output ) for output in outputs ] )) output_supported def output_supported ( outputs : Set [ datalake . common . ouput . Output ] ) View Source def output_supported ( outputs : Set [ Output ] ) : def inner_decorator ( function ) : def wrapper ( * args , ** kwargs ) : if kwargs . get ( 'output' ) and kwargs [ 'output' ] not in outputs : raise ValueError ( f '{kwargs[\"output\"]} output type is not supported. ' f 'Outputs supported are: {display_outputs(outputs)}' ) return function ( * args , ** kwargs ) return wrapper return inner_decorator parse_response def parse_response ( response : requests . models . Response ) -> Union [ str , dict ] Parse a Request.Response depending of the Content-Type returned View Source def parse_response ( response : Response ) - > Union [ str , dict ] : \"\"\"Parse a Request.Response depending of the Content-Type returned\"\"\" content_type = response . headers . get ( 'Content-Type' , Output . JSON . value ) content_type = content_type . split ( ';' ) [ 0 ] # we don ' t care about extra info on the content if content_type in { output.value for output in [ Output.CSV , Output.CSV_ZIP , Output.JSON_ZIP ] } : return response . text else : return response . json () Classes Output class Output ( / , * args , ** kwargs ) View Source class Output ( Enum ): JSON = 'application/json' CSV = 'text/csv' MISP = 'application/x-misp+json' STIX = 'application/stix+json' JSON_ZIP = 'application/zip' CSV_ZIP = 'text/x-csv-zip' STIX_ZIP = 'text/x-stix-zip' def __str__ ( self ): return self . name def __repr__ ( self ): return str ( self ) Ancestors (in MRO) enum.Enum Class variables CSV CSV_ZIP JSON JSON_ZIP MISP STIX STIX_ZIP name value","title":"Ouput"},{"location":"reference/datalake/common/ouput/#module-datalakecommonouput","text":"None None View Source from enum import Enum from typing import Set , Union from requests import Response class Output ( Enum ): JSON = 'application/json' CSV = 'text/csv' MISP = 'application/x-misp+json' STIX = 'application/stix+json' JSON_ZIP = 'application/zip' CSV_ZIP = 'text/x-csv-zip' STIX_ZIP = 'text/x-stix-zip' def __str__ ( self ): return self . name def __repr__ ( self ): return str ( self ) def parse_response ( response : Response ) -> Union [ str , dict ]: \"\"\"Parse a Request.Response depending of the Content-Type returned\"\"\" content_type = response . headers . get ( 'Content-Type' , Output . JSON . value ) content_type = content_type . split ( ';' )[ 0 ] # we don't care about extra info on the content if content_type in { output . value for output in [ Output . CSV , Output . CSV_ZIP , Output . JSON_ZIP ]}: return response . text else : return response . json () def display_outputs ( outputs ): return ', ' . join ( sorted ([ str ( output ) for output in outputs ])) def output_supported ( outputs : Set [ Output ]): def inner_decorator ( function ): def wrapper ( * args , ** kwargs ): if kwargs . get ( 'output' ) and kwargs [ 'output' ] not in outputs : raise ValueError ( f ' { kwargs [ \"output\" ] } output type is not supported. ' f 'Outputs supported are: { display_outputs ( outputs ) } ' ) return function ( * args , ** kwargs ) return wrapper return inner_decorator","title":"Module datalake.common.ouput"},{"location":"reference/datalake/common/ouput/#functions","text":"","title":"Functions"},{"location":"reference/datalake/common/ouput/#display_outputs","text":"def display_outputs ( outputs ) View Source def display_outputs ( outputs ) : return ' , ' . join ( sorted ( [ str ( output ) for output in outputs ] ))","title":"display_outputs"},{"location":"reference/datalake/common/ouput/#output_supported","text":"def output_supported ( outputs : Set [ datalake . common . ouput . Output ] ) View Source def output_supported ( outputs : Set [ Output ] ) : def inner_decorator ( function ) : def wrapper ( * args , ** kwargs ) : if kwargs . get ( 'output' ) and kwargs [ 'output' ] not in outputs : raise ValueError ( f '{kwargs[\"output\"]} output type is not supported. ' f 'Outputs supported are: {display_outputs(outputs)}' ) return function ( * args , ** kwargs ) return wrapper return inner_decorator","title":"output_supported"},{"location":"reference/datalake/common/ouput/#parse_response","text":"def parse_response ( response : requests . models . Response ) -> Union [ str , dict ] Parse a Request.Response depending of the Content-Type returned View Source def parse_response ( response : Response ) - > Union [ str , dict ] : \"\"\"Parse a Request.Response depending of the Content-Type returned\"\"\" content_type = response . headers . get ( 'Content-Type' , Output . JSON . value ) content_type = content_type . split ( ';' ) [ 0 ] # we don ' t care about extra info on the content if content_type in { output.value for output in [ Output.CSV , Output.CSV_ZIP , Output.JSON_ZIP ] } : return response . text else : return response . json ()","title":"parse_response"},{"location":"reference/datalake/common/ouput/#classes","text":"","title":"Classes"},{"location":"reference/datalake/common/ouput/#output","text":"class Output ( / , * args , ** kwargs ) View Source class Output ( Enum ): JSON = 'application/json' CSV = 'text/csv' MISP = 'application/x-misp+json' STIX = 'application/stix+json' JSON_ZIP = 'application/zip' CSV_ZIP = 'text/x-csv-zip' STIX_ZIP = 'text/x-stix-zip' def __str__ ( self ): return self . name def __repr__ ( self ): return str ( self )","title":"Output"},{"location":"reference/datalake/common/ouput/#ancestors-in-mro","text":"enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/datalake/common/ouput/#class-variables","text":"CSV CSV_ZIP JSON JSON_ZIP MISP STIX STIX_ZIP name value","title":"Class variables"},{"location":"reference/datalake/common/throttler/","text":"Module datalake.common.throttler None None View Source import random from time import sleep , time from typing import Callable , Dict , List THROTTLE_QUEUE : Dict [ Callable [[], None ], List [ float ]] = {} def throttle ( * , period : int , call_per_period : int ): \"\"\" Wrapper to throttle the number of time a function can be called :param period: time in seconds :param call_per_period: number of time the function can be called, if not the wrapper will wait \"\"\" def inner_decorator ( f ): def wrapped ( * args , ** kwargs ): previous_call = THROTTLE_QUEUE . get ( f , []) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don't allow the function to be called if len ( previous_call ) >= call_per_period : time_since_first_call = ( call_time - previous_call [ 0 ]) sleep ( max ( period - time_since_first_call , 0.1 )) # Wait until a call has been made 'period' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response return wrapped return inner_decorator if __name__ == '__main__' : \"\"\"Example on how to use the throttle wrapper\"\"\" random . seed ( 0 ) PERIOD = 1 CALL_PER_PERIOD = 2 @throttle ( period = PERIOD , call_per_period = CALL_PER_PERIOD ) def f (): sleep ( random . randint ( 0 , 2 )) return \"hello world\" start = time () total_calls = 10 for i in range ( total_calls ): print ( f ' { f () } { i } { time () } ' ) assert time () > start + (( total_calls / CALL_PER_PERIOD ) - 1 ) * PERIOD Variables THROTTLE_QUEUE Functions throttle def throttle ( * , period : int , call_per_period : int ) Wrapper to throttle the number of time a function can be called Parameters: Name Type Description Default period None time in seconds None call_per_period None number of time the function can be called, if not the wrapper will wait None View Source def throttle ( * , period : int , call_per_period : int ) : \"\"\" Wrapper to throttle the number of time a function can be called :param period: time in seconds :param call_per_period: number of time the function can be called, if not the wrapper will wait \"\"\" def inner_decorator ( f ) : def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response return wrapped return inner_decorator","title":"Throttler"},{"location":"reference/datalake/common/throttler/#module-datalakecommonthrottler","text":"None None View Source import random from time import sleep , time from typing import Callable , Dict , List THROTTLE_QUEUE : Dict [ Callable [[], None ], List [ float ]] = {} def throttle ( * , period : int , call_per_period : int ): \"\"\" Wrapper to throttle the number of time a function can be called :param period: time in seconds :param call_per_period: number of time the function can be called, if not the wrapper will wait \"\"\" def inner_decorator ( f ): def wrapped ( * args , ** kwargs ): previous_call = THROTTLE_QUEUE . get ( f , []) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don't allow the function to be called if len ( previous_call ) >= call_per_period : time_since_first_call = ( call_time - previous_call [ 0 ]) sleep ( max ( period - time_since_first_call , 0.1 )) # Wait until a call has been made 'period' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response return wrapped return inner_decorator if __name__ == '__main__' : \"\"\"Example on how to use the throttle wrapper\"\"\" random . seed ( 0 ) PERIOD = 1 CALL_PER_PERIOD = 2 @throttle ( period = PERIOD , call_per_period = CALL_PER_PERIOD ) def f (): sleep ( random . randint ( 0 , 2 )) return \"hello world\" start = time () total_calls = 10 for i in range ( total_calls ): print ( f ' { f () } { i } { time () } ' ) assert time () > start + (( total_calls / CALL_PER_PERIOD ) - 1 ) * PERIOD","title":"Module datalake.common.throttler"},{"location":"reference/datalake/common/throttler/#variables","text":"THROTTLE_QUEUE","title":"Variables"},{"location":"reference/datalake/common/throttler/#functions","text":"","title":"Functions"},{"location":"reference/datalake/common/throttler/#throttle","text":"def throttle ( * , period : int , call_per_period : int ) Wrapper to throttle the number of time a function can be called Parameters: Name Type Description Default period None time in seconds None call_per_period None number of time the function can be called, if not the wrapper will wait None View Source def throttle ( * , period : int , call_per_period : int ) : \"\"\" Wrapper to throttle the number of time a function can be called :param period: time in seconds :param call_per_period: number of time the function can be called, if not the wrapper will wait \"\"\" def inner_decorator ( f ) : def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response return wrapped return inner_decorator","title":"throttle"},{"location":"reference/datalake/common/token_manager/","text":"Module datalake.common.token_manager Token manager will manage tokens for the scripts. None View Source \"\"\" Token manager will manage tokens for the scripts. \"\"\" import json import os from getpass import getpass from urllib.parse import urljoin import requests from datalake.common.logger import logger class TokenManager : \"\"\" Use it to generate token access to the API \"\"\" def __init__ ( self , endpoint_config : dict , * , environment : str , username = None , password = None ): \"\"\"environment can be either prod or preprod\"\"\" base_url = urljoin ( endpoint_config [ 'main' ][ environment ], endpoint_config [ 'api_version' ]) endpoints = endpoint_config [ 'endpoints' ] self . url_token = urljoin ( base_url , endpoints [ 'token' ], allow_fragments = True ) self . url_refresh = urljoin ( base_url , endpoints [ 'refresh_token' ], allow_fragments = True ) self . username = username self . password = password self . access_token = None self . refresh_token = None self . get_token () def get_token ( self ): \"\"\" Generate token from user input, with email and password \"\"\" self . username = self . username or os . getenv ( 'OCD_DTL_USERNAME' ) or input ( 'Email: ' ) self . password = self . password or os . getenv ( 'OCD_DTL_PASSWORD' ) or getpass () print () data = { 'email' : self . username , 'password' : self . password } response = requests . post ( url = self . url_token , json = data ) json_response = json . loads ( response . text ) try : self . access_token = f 'Token { json_response [ \"access_token\" ] } ' self . refresh_token = f 'Token { json_response [ \"refresh_token\" ] } ' except KeyError : logger . error ( f 'An error occurred while retrieving an access token, for URL: { self . url_token } \\n ' f 'response of the API: { response . text } ' ) raise ValueError ( f 'Could not login: { response . text } ' ) def fetch_new_token ( self ): logger . debug ( 'Token will be refreshed' ) headers = { 'Authorization' : self . refresh_token } response = requests . post ( url = self . url_refresh , headers = headers ) json_response = response . json () if response . status_code == 401 and json_response . get ( 'messages' ) == 'Token has expired' : logger . debug ( 'Refreshing the refresh token' ) # Refresh token is also expired, we need to restart the authentication from scratch self . get_token () elif 'access_token' in json_response : self . access_token = f 'Token { json_response [ \"access_token\" ] } ' else : # an error occurred logger . error ( f 'An error occurred while refreshing the refresh token, for URL: { self . url_refresh } \\n ' f 'response of the API: { response . text } ' ) raise ValueError ( f 'Could not refresh the token: { response . text } ' ) def process_auth_error ( self , error_msg : str ): \"\"\" Allow to update token when API response is either Missing Authorization Header or Token has expired. \"\"\" if error_msg in ( 'Missing Authorization Header' , 'Bad Authorization header. Expected value \\' Token <JWT> \\' ' ): self . get_token () elif error_msg == 'Token has expired' : self . fetch_new_token () else : raise ValueError ( f 'Unexpected msg: { error_msg } ' ) Classes TokenManager class TokenManager ( endpoint_config : dict , * , environment : str , username = None , password = None ) View Source class TokenManager : \"\"\" Use it to generate token access to the API \"\"\" def __init__ ( self , endpoint_config : dict , * , environment : str , username = None , password = None ) : \"\"\"environment can be either prod or preprod\"\"\" base_url = urljoin ( endpoint_config [ 'main' ][ environment ] , endpoint_config [ 'api_version' ] ) endpoints = endpoint_config [ 'endpoints' ] self . url_token = urljoin ( base_url , endpoints [ 'token' ] , allow_fragments = True ) self . url_refresh = urljoin ( base_url , endpoints [ 'refresh_token' ] , allow_fragments = True ) self . username = username self . password = password self . access_token = None self . refresh_token = None self . get_token () def get_token ( self ) : \"\"\" Generate token from user input, with email and password \"\"\" self . username = self . username or os . getenv ( 'OCD_DTL_USERNAME' ) or input ( 'Email: ' ) self . password = self . password or os . getenv ( 'OCD_DTL_PASSWORD' ) or getpass () print () data = { 'email' : self . username , 'password' : self . password } response = requests . post ( url = self . url_token , json = data ) json_response = json . loads ( response . text ) try : self . access_token = f 'Token {json_response[\"access_token\"]}' self . refresh_token = f 'Token {json_response[\"refresh_token\"]}' except KeyError : logger . error ( f 'An error occurred while retrieving an access token, for URL: {self.url_token}\\n' f 'response of the API: {response.text}' ) raise ValueError ( f 'Could not login: {response.text}' ) def fetch_new_token ( self ) : logger . debug ( 'Token will be refreshed' ) headers = { 'Authorization' : self . refresh_token } response = requests . post ( url = self . url_refresh , headers = headers ) json_response = response . json () if response . status_code == 401 and json_response . get ( 'messages' ) == 'Token has expired' : logger . debug ( 'Refreshing the refresh token' ) # Refresh token is also expired , we need to restart the authentication from scratch self . get_token () elif 'access_token' in json_response : self . access_token = f 'Token {json_response[\"access_token\"]}' else : # an error occurred logger . error ( f 'An error occurred while refreshing the refresh token, for URL: {self.url_refresh}\\n' f 'response of the API: {response.text}' ) raise ValueError ( f 'Could not refresh the token: {response.text}' ) def process_auth_error ( self , error_msg : str ) : \"\"\" Allow to update token when API response is either Missing Authorization Header or Token has expired. \"\"\" if error_msg in ( 'Missing Authorization Header' , 'Bad Authorization header. Expected value \\' Token < JWT > \\ '' ) : self . get_token () elif error_msg == 'Token has expired' : self . fetch_new_token () else : raise ValueError ( f 'Unexpected msg: {error_msg}' ) Methods fetch_new_token def fetch_new_token ( self ) View Source def fetch_new_token ( self ) : logger . debug ( ' Token will be refreshed ' ) headers = { ' Authorization ' : self . refresh_token } response = requests . post ( url = self . url_refresh , headers = headers ) json_response = response . json () if response . status_code == 401 and json_response . get ( ' messages ' ) == ' Token has expired ' : logger . debug ( ' Refreshing the refresh token ' ) # Refresh token is also expired , we need to restart the authentication from scratch self . get_token () elif ' access_token ' in json_response : self . access_token = f ' Token {json_response[\"access_token\"]} ' else : # an error occurred logger . error ( f ' An error occurred while refreshing the refresh token, for URL: {self.url_refresh} \\n ' f ' response of the API: {response.text} ' ) raise ValueError ( f ' Could not refresh the token: {response.text} ' ) get_token def get_token ( self ) Generate token from user input, with email and password View Source def get_token ( self ): \"\"\" Generate token from user input, with email and password \"\"\" self . username = self . username or os . getenv ( 'OCD_DTL_USERNAME' ) or input ( 'Email: ' ) self . password = self . password or os . getenv ( 'OCD_DTL_PASSWORD' ) or getpass () print () data = { 'email' : self . username , 'password' : self . password } response = requests . post ( url = self . url_token , json = data ) json_response = json . loads ( response . text ) try : self . access_token = f 'Token {json_response[\"access_token\"]}' self . refresh_token = f 'Token {json_response[\"refresh_token\"]}' except KeyError : logger . error ( f 'An error occurred while retrieving an access token, for URL: {self.url_token} \\n ' f 'response of the API: {response.text}' ) raise ValueError ( f 'Could not login: {response.text}' ) process_auth_error def process_auth_error ( self , error_msg : str ) Allow to update token when API response is either Missing Authorization Header or Token has expired. View Source def process_auth_error ( self , error_msg : str ) : \"\"\" Allow to update token when API response is either Missing Authorization Header or Token has expired . \"\"\" if error_msg in ( ' Missing Authorization Header ' , ' Bad Authorization header. Expected value \\ ' Token < JWT > \\ '' ) : self . get_token () elif error_msg == ' Token has expired ' : self . fetch_new_token () else : raise ValueError ( f ' Unexpected msg: {error_msg} ' )","title":"Token Manager"},{"location":"reference/datalake/common/token_manager/#module-datalakecommontoken_manager","text":"Token manager will manage tokens for the scripts. None View Source \"\"\" Token manager will manage tokens for the scripts. \"\"\" import json import os from getpass import getpass from urllib.parse import urljoin import requests from datalake.common.logger import logger class TokenManager : \"\"\" Use it to generate token access to the API \"\"\" def __init__ ( self , endpoint_config : dict , * , environment : str , username = None , password = None ): \"\"\"environment can be either prod or preprod\"\"\" base_url = urljoin ( endpoint_config [ 'main' ][ environment ], endpoint_config [ 'api_version' ]) endpoints = endpoint_config [ 'endpoints' ] self . url_token = urljoin ( base_url , endpoints [ 'token' ], allow_fragments = True ) self . url_refresh = urljoin ( base_url , endpoints [ 'refresh_token' ], allow_fragments = True ) self . username = username self . password = password self . access_token = None self . refresh_token = None self . get_token () def get_token ( self ): \"\"\" Generate token from user input, with email and password \"\"\" self . username = self . username or os . getenv ( 'OCD_DTL_USERNAME' ) or input ( 'Email: ' ) self . password = self . password or os . getenv ( 'OCD_DTL_PASSWORD' ) or getpass () print () data = { 'email' : self . username , 'password' : self . password } response = requests . post ( url = self . url_token , json = data ) json_response = json . loads ( response . text ) try : self . access_token = f 'Token { json_response [ \"access_token\" ] } ' self . refresh_token = f 'Token { json_response [ \"refresh_token\" ] } ' except KeyError : logger . error ( f 'An error occurred while retrieving an access token, for URL: { self . url_token } \\n ' f 'response of the API: { response . text } ' ) raise ValueError ( f 'Could not login: { response . text } ' ) def fetch_new_token ( self ): logger . debug ( 'Token will be refreshed' ) headers = { 'Authorization' : self . refresh_token } response = requests . post ( url = self . url_refresh , headers = headers ) json_response = response . json () if response . status_code == 401 and json_response . get ( 'messages' ) == 'Token has expired' : logger . debug ( 'Refreshing the refresh token' ) # Refresh token is also expired, we need to restart the authentication from scratch self . get_token () elif 'access_token' in json_response : self . access_token = f 'Token { json_response [ \"access_token\" ] } ' else : # an error occurred logger . error ( f 'An error occurred while refreshing the refresh token, for URL: { self . url_refresh } \\n ' f 'response of the API: { response . text } ' ) raise ValueError ( f 'Could not refresh the token: { response . text } ' ) def process_auth_error ( self , error_msg : str ): \"\"\" Allow to update token when API response is either Missing Authorization Header or Token has expired. \"\"\" if error_msg in ( 'Missing Authorization Header' , 'Bad Authorization header. Expected value \\' Token <JWT> \\' ' ): self . get_token () elif error_msg == 'Token has expired' : self . fetch_new_token () else : raise ValueError ( f 'Unexpected msg: { error_msg } ' )","title":"Module datalake.common.token_manager"},{"location":"reference/datalake/common/token_manager/#classes","text":"","title":"Classes"},{"location":"reference/datalake/common/token_manager/#tokenmanager","text":"class TokenManager ( endpoint_config : dict , * , environment : str , username = None , password = None ) View Source class TokenManager : \"\"\" Use it to generate token access to the API \"\"\" def __init__ ( self , endpoint_config : dict , * , environment : str , username = None , password = None ) : \"\"\"environment can be either prod or preprod\"\"\" base_url = urljoin ( endpoint_config [ 'main' ][ environment ] , endpoint_config [ 'api_version' ] ) endpoints = endpoint_config [ 'endpoints' ] self . url_token = urljoin ( base_url , endpoints [ 'token' ] , allow_fragments = True ) self . url_refresh = urljoin ( base_url , endpoints [ 'refresh_token' ] , allow_fragments = True ) self . username = username self . password = password self . access_token = None self . refresh_token = None self . get_token () def get_token ( self ) : \"\"\" Generate token from user input, with email and password \"\"\" self . username = self . username or os . getenv ( 'OCD_DTL_USERNAME' ) or input ( 'Email: ' ) self . password = self . password or os . getenv ( 'OCD_DTL_PASSWORD' ) or getpass () print () data = { 'email' : self . username , 'password' : self . password } response = requests . post ( url = self . url_token , json = data ) json_response = json . loads ( response . text ) try : self . access_token = f 'Token {json_response[\"access_token\"]}' self . refresh_token = f 'Token {json_response[\"refresh_token\"]}' except KeyError : logger . error ( f 'An error occurred while retrieving an access token, for URL: {self.url_token}\\n' f 'response of the API: {response.text}' ) raise ValueError ( f 'Could not login: {response.text}' ) def fetch_new_token ( self ) : logger . debug ( 'Token will be refreshed' ) headers = { 'Authorization' : self . refresh_token } response = requests . post ( url = self . url_refresh , headers = headers ) json_response = response . json () if response . status_code == 401 and json_response . get ( 'messages' ) == 'Token has expired' : logger . debug ( 'Refreshing the refresh token' ) # Refresh token is also expired , we need to restart the authentication from scratch self . get_token () elif 'access_token' in json_response : self . access_token = f 'Token {json_response[\"access_token\"]}' else : # an error occurred logger . error ( f 'An error occurred while refreshing the refresh token, for URL: {self.url_refresh}\\n' f 'response of the API: {response.text}' ) raise ValueError ( f 'Could not refresh the token: {response.text}' ) def process_auth_error ( self , error_msg : str ) : \"\"\" Allow to update token when API response is either Missing Authorization Header or Token has expired. \"\"\" if error_msg in ( 'Missing Authorization Header' , 'Bad Authorization header. Expected value \\' Token < JWT > \\ '' ) : self . get_token () elif error_msg == 'Token has expired' : self . fetch_new_token () else : raise ValueError ( f 'Unexpected msg: {error_msg}' )","title":"TokenManager"},{"location":"reference/datalake/common/token_manager/#methods","text":"","title":"Methods"},{"location":"reference/datalake/common/token_manager/#fetch_new_token","text":"def fetch_new_token ( self ) View Source def fetch_new_token ( self ) : logger . debug ( ' Token will be refreshed ' ) headers = { ' Authorization ' : self . refresh_token } response = requests . post ( url = self . url_refresh , headers = headers ) json_response = response . json () if response . status_code == 401 and json_response . get ( ' messages ' ) == ' Token has expired ' : logger . debug ( ' Refreshing the refresh token ' ) # Refresh token is also expired , we need to restart the authentication from scratch self . get_token () elif ' access_token ' in json_response : self . access_token = f ' Token {json_response[\"access_token\"]} ' else : # an error occurred logger . error ( f ' An error occurred while refreshing the refresh token, for URL: {self.url_refresh} \\n ' f ' response of the API: {response.text} ' ) raise ValueError ( f ' Could not refresh the token: {response.text} ' )","title":"fetch_new_token"},{"location":"reference/datalake/common/token_manager/#get_token","text":"def get_token ( self ) Generate token from user input, with email and password View Source def get_token ( self ): \"\"\" Generate token from user input, with email and password \"\"\" self . username = self . username or os . getenv ( 'OCD_DTL_USERNAME' ) or input ( 'Email: ' ) self . password = self . password or os . getenv ( 'OCD_DTL_PASSWORD' ) or getpass () print () data = { 'email' : self . username , 'password' : self . password } response = requests . post ( url = self . url_token , json = data ) json_response = json . loads ( response . text ) try : self . access_token = f 'Token {json_response[\"access_token\"]}' self . refresh_token = f 'Token {json_response[\"refresh_token\"]}' except KeyError : logger . error ( f 'An error occurred while retrieving an access token, for URL: {self.url_token} \\n ' f 'response of the API: {response.text}' ) raise ValueError ( f 'Could not login: {response.text}' )","title":"get_token"},{"location":"reference/datalake/common/token_manager/#process_auth_error","text":"def process_auth_error ( self , error_msg : str ) Allow to update token when API response is either Missing Authorization Header or Token has expired. View Source def process_auth_error ( self , error_msg : str ) : \"\"\" Allow to update token when API response is either Missing Authorization Header or Token has expired . \"\"\" if error_msg in ( ' Missing Authorization Header ' , ' Bad Authorization header. Expected value \\ ' Token < JWT > \\ '' ) : self . get_token () elif error_msg == ' Token has expired ' : self . fetch_new_token () else : raise ValueError ( f ' Unexpected msg: {error_msg} ' )","title":"process_auth_error"},{"location":"reference/datalake/common/utils/","text":"Module datalake.common.utils None None View Source import datetime from collections import defaultdict from typing import Dict , Optional , Generator def join_dicts ( * dicts : dict ) -> Dict [ str , list ]: \"\"\"Takes two or more dictionaries and join them\"\"\" if len ( dicts ) == 0 : return {} if len ( dicts ) == 1 : return dicts [ 0 ] out = defaultdict ( list ) for d in dicts : for key , val in d . items (): out [ key ] . extend ( val ) return out def aggregate_csv_or_json_api_response ( aggregated_response , response ): if isinstance ( response , dict ): # json response aggregated_response = join_dicts ( aggregated_response , response ) else : # csv response csv_lines = response . strip () . split ( ' \\n ' ) if not aggregated_response : aggregated_response = [ csv_lines [ 0 ]] aggregated_response += csv_lines [ 1 :] return aggregated_response def parse_api_timestamp ( timestamp : str ) -> Optional [ datetime . datetime ]: if timestamp : timestamp = timestamp . split ( '+' )[ 0 ] # Discard the offset as it should be 0 and is not supported by python 3.6 return datetime . datetime . strptime ( timestamp , '%Y-%m- %d T%H:%M:%S. %f ' ) return None def split_list ( list_to_split : list , slice_size : int ) -> Generator [ list , None , None ]: for i in range ( 0 , len ( list_to_split ), slice_size ): yield list_to_split [ i : i + slice_size ] Functions aggregate_csv_or_json_api_response def aggregate_csv_or_json_api_response ( aggregated_response , response ) View Source def aggregate_csv_or_json_api_response ( aggregated_response , response ) : if isinstance ( response , dict ) : # json response aggregated_response = join_dicts ( aggregated_response , response ) else : # csv response csv_lines = response . strip (). split ( '\\n' ) if not aggregated_response: aggregated_response = [ csv_lines [ 0 ]] aggregated_response += csv_lines [ 1 : ] return aggregated_response join_dicts def join_dicts ( * dicts : dict ) -> Dict [ str , list ] Takes two or more dictionaries and join them View Source def join_dicts ( * dicts : dict ) -> Dict [ str, list ] : \"\"\"Takes two or more dictionaries and join them\"\"\" if len ( dicts ) == 0 : return {} if len ( dicts ) == 1 : return dicts [ 0 ] out = defaultdict ( list ) for d in dicts : for key , val in d . items () : out [ key ] . extend ( val ) return out parse_api_timestamp def parse_api_timestamp ( timestamp : str ) -> Optional [ datetime . datetime ] View Source def parse_api_timestamp ( timestamp : str ) -> Optional [ datetime . datetime ] : if timestamp : timestamp = timestamp . split ( '+' )[ 0 ] # Discard the offset as it should be 0 and is not supported by python 3.6 return datetime . datetime . strptime ( timestamp , '%Y-%m-%dT%H:%M:%S.%f' ) return None split_list def split_list ( list_to_split : list , slice_size : int ) -> Generator [ list , NoneType , NoneType ] View Source def split_list ( list_to_split : list , slice_size : int ) -> Generator [ list , None , None ] : for i in range ( 0 , len ( list_to_split ), slice_size ) : yield list_to_split [ i : i + slice_size ]","title":"Utils"},{"location":"reference/datalake/common/utils/#module-datalakecommonutils","text":"None None View Source import datetime from collections import defaultdict from typing import Dict , Optional , Generator def join_dicts ( * dicts : dict ) -> Dict [ str , list ]: \"\"\"Takes two or more dictionaries and join them\"\"\" if len ( dicts ) == 0 : return {} if len ( dicts ) == 1 : return dicts [ 0 ] out = defaultdict ( list ) for d in dicts : for key , val in d . items (): out [ key ] . extend ( val ) return out def aggregate_csv_or_json_api_response ( aggregated_response , response ): if isinstance ( response , dict ): # json response aggregated_response = join_dicts ( aggregated_response , response ) else : # csv response csv_lines = response . strip () . split ( ' \\n ' ) if not aggregated_response : aggregated_response = [ csv_lines [ 0 ]] aggregated_response += csv_lines [ 1 :] return aggregated_response def parse_api_timestamp ( timestamp : str ) -> Optional [ datetime . datetime ]: if timestamp : timestamp = timestamp . split ( '+' )[ 0 ] # Discard the offset as it should be 0 and is not supported by python 3.6 return datetime . datetime . strptime ( timestamp , '%Y-%m- %d T%H:%M:%S. %f ' ) return None def split_list ( list_to_split : list , slice_size : int ) -> Generator [ list , None , None ]: for i in range ( 0 , len ( list_to_split ), slice_size ): yield list_to_split [ i : i + slice_size ]","title":"Module datalake.common.utils"},{"location":"reference/datalake/common/utils/#functions","text":"","title":"Functions"},{"location":"reference/datalake/common/utils/#aggregate_csv_or_json_api_response","text":"def aggregate_csv_or_json_api_response ( aggregated_response , response ) View Source def aggregate_csv_or_json_api_response ( aggregated_response , response ) : if isinstance ( response , dict ) : # json response aggregated_response = join_dicts ( aggregated_response , response ) else : # csv response csv_lines = response . strip (). split ( '\\n' ) if not aggregated_response: aggregated_response = [ csv_lines [ 0 ]] aggregated_response += csv_lines [ 1 : ] return aggregated_response","title":"aggregate_csv_or_json_api_response"},{"location":"reference/datalake/common/utils/#join_dicts","text":"def join_dicts ( * dicts : dict ) -> Dict [ str , list ] Takes two or more dictionaries and join them View Source def join_dicts ( * dicts : dict ) -> Dict [ str, list ] : \"\"\"Takes two or more dictionaries and join them\"\"\" if len ( dicts ) == 0 : return {} if len ( dicts ) == 1 : return dicts [ 0 ] out = defaultdict ( list ) for d in dicts : for key , val in d . items () : out [ key ] . extend ( val ) return out","title":"join_dicts"},{"location":"reference/datalake/common/utils/#parse_api_timestamp","text":"def parse_api_timestamp ( timestamp : str ) -> Optional [ datetime . datetime ] View Source def parse_api_timestamp ( timestamp : str ) -> Optional [ datetime . datetime ] : if timestamp : timestamp = timestamp . split ( '+' )[ 0 ] # Discard the offset as it should be 0 and is not supported by python 3.6 return datetime . datetime . strptime ( timestamp , '%Y-%m-%dT%H:%M:%S.%f' ) return None","title":"parse_api_timestamp"},{"location":"reference/datalake/common/utils/#split_list","text":"def split_list ( list_to_split : list , slice_size : int ) -> Generator [ list , NoneType , NoneType ] View Source def split_list ( list_to_split : list , slice_size : int ) -> Generator [ list , None , None ] : for i in range ( 0 , len ( list_to_split ), slice_size ) : yield list_to_split [ i : i + slice_size ]","title":"split_list"},{"location":"reference/datalake/common/warn/","text":"Module datalake.common.warn None None View Source import os import warnings class Warn : @staticmethod def warning ( message ): if not os . environ . get ( 'IGNORE_SIGHTING_BUILDER_WARNING' , 'False' ) . lower () in ( 'true' , '1' , 't' ): warnings . warn ( message ) Classes Warn class Warn ( / , * args , ** kwargs ) View Source class Warn : @staticmethod def warning ( message ) : if not os . environ . get ( 'IGNORE_SIGHTING_BUILDER_WARNING' , 'False' ). lower () in ( 'true' , '1' , 't' ) : warnings . warn ( message ) Static methods warning def warning ( message ) View Source @staticmethod def warning ( message ) : if not os . environ . get ( 'IGNORE_SIGHTING_BUILDER_WARNING' , 'False' ). lower () in ( 'true' , '1' , 't' ) : warnings . warn ( message )","title":"Warn"},{"location":"reference/datalake/common/warn/#module-datalakecommonwarn","text":"None None View Source import os import warnings class Warn : @staticmethod def warning ( message ): if not os . environ . get ( 'IGNORE_SIGHTING_BUILDER_WARNING' , 'False' ) . lower () in ( 'true' , '1' , 't' ): warnings . warn ( message )","title":"Module datalake.common.warn"},{"location":"reference/datalake/common/warn/#classes","text":"","title":"Classes"},{"location":"reference/datalake/common/warn/#warn","text":"class Warn ( / , * args , ** kwargs ) View Source class Warn : @staticmethod def warning ( message ) : if not os . environ . get ( 'IGNORE_SIGHTING_BUILDER_WARNING' , 'False' ). lower () in ( 'true' , '1' , 't' ) : warnings . warn ( message )","title":"Warn"},{"location":"reference/datalake/common/warn/#static-methods","text":"","title":"Static methods"},{"location":"reference/datalake/common/warn/#warning","text":"def warning ( message ) View Source @staticmethod def warning ( message ) : if not os . environ . get ( 'IGNORE_SIGHTING_BUILDER_WARNING' , 'False' ). lower () in ( 'true' , '1' , 't' ) : warnings . warn ( message )","title":"warning"},{"location":"reference/datalake/endpoints/","text":"Module datalake.endpoints None None View Source from datalake.endpoints.endpoint import Endpoint Sub-modules datalake.endpoints.advanced_search datalake.endpoints.bulk_search datalake.endpoints.endpoint datalake.endpoints.sightings datalake.endpoints.tags datalake.endpoints.threats","title":"Index"},{"location":"reference/datalake/endpoints/#module-datalakeendpoints","text":"None None View Source from datalake.endpoints.endpoint import Endpoint","title":"Module datalake.endpoints"},{"location":"reference/datalake/endpoints/#sub-modules","text":"datalake.endpoints.advanced_search datalake.endpoints.bulk_search datalake.endpoints.endpoint datalake.endpoints.sightings datalake.endpoints.tags datalake.endpoints.threats","title":"Sub-modules"},{"location":"reference/datalake/endpoints/advanced_search/","text":"Module datalake.endpoints.advanced_search None None View Source from datalake.endpoints.endpoint import Endpoint from datalake.common.ouput import parse_response , Output , output_supported from requests.sessions import PreparedRequest from typing import List class AdvancedSearch ( Endpoint ): ordering_list = [ 'first_seen' , '-first_seen' , 'last_updated' , '-last_updated' , 'events_count' , '-events_count' , 'sources_count' , '-sources_count' ] @output_supported ({ Output . JSON , Output . STIX , Output . MISP , Output . CSV }) def advanced_search_from_query_body ( self , query_body : dict , limit : int = 20 , offset : int = 0 , ordering : List [ str ] = None , output = Output . JSON ): if not query_body : raise ValueError ( \"query_body is required\" ) if ordering and not set ( ordering ) . issubset ( self . ordering_list ): raise ValueError ( f 'ordering needs to be a list of at least one of the following str : { \", \" . join ( self . ordering_list ) } ' ) body = { 'query_body' : query_body , 'limit' : limit , 'offset' : offset , } if ordering : body [ 'ordering' ] = ordering url = self . _build_url_for_endpoint ( 'advanced-search' ) response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ( output = output )) return parse_response ( response ) @output_supported ({ Output . JSON , Output . STIX , Output . MISP , Output . CSV }) def advanced_search_from_query_hash ( self , query_hash , limit : int = 20 , offset : int = 0 , ordering : List [ str ] = None , output = Output . JSON ): if not query_hash : raise ValueError ( \"query_hash is required\" ) if ordering and not set ( ordering ) . issubset ( self . ordering_list ): raise ValueError ( f 'ordering needs to be a list of at least one of the following str : { \", \" . join ( self . ordering_list ) } ' ) url = self . _build_url_for_endpoint ( 'advanced-search-hash' ) . format ( query_hash = query_hash ) params = { 'limit' : limit , 'offset' : offset } if ordering : params [ 'ordering' ] = ordering req = PreparedRequest () req . prepare_url ( url , params ) url = req . url response = self . datalake_requests ( url , 'get' , headers = self . _get_headers ( output = output )) return parse_response ( response ) Classes AdvancedSearch class AdvancedSearch ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class AdvancedSearch ( Endpoint ) : ordering_list = [ 'first_seen', '-first_seen', 'last_updated', '-last_updated', 'events_count', '-events_count', 'sources_count', '-sources_count' ] @output_supported ( { Output . JSON , Output . STIX , Output . MISP , Output . CSV } ) def advanced_search_from_query_body ( self , query_body : dict , limit : int = 20 , offset : int = 0 , ordering : List [ str ] = None , output = Output . JSON ) : if not query_body : raise ValueError ( \"query_body is required\" ) if ordering and not set ( ordering ). issubset ( self . ordering_list ) : raise ValueError ( f 'ordering needs to be a list of at least one of the following str : {\", \".join(self.ordering_list)}' ) body = { 'query_body' : query_body , 'limit' : limit , 'offset' : offset , } if ordering : body [ 'ordering' ] = ordering url = self . _build_url_for_endpoint ( 'advanced-search' ) response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ( output = output )) return parse_response ( response ) @output_supported ( { Output . JSON , Output . STIX , Output . MISP , Output . CSV } ) def advanced_search_from_query_hash ( self , query_hash , limit : int = 20 , offset : int = 0 , ordering : List [ str ] = None , output = Output . JSON ) : if not query_hash : raise ValueError ( \"query_hash is required\" ) if ordering and not set ( ordering ). issubset ( self . ordering_list ) : raise ValueError ( f 'ordering needs to be a list of at least one of the following str : {\", \".join(self.ordering_list)}' ) url = self . _build_url_for_endpoint ( 'advanced-search-hash' ). format ( query_hash = query_hash ) params = { 'limit' : limit , 'offset' : offset } if ordering : params [ 'ordering' ] = ordering req = PreparedRequest () req . prepare_url ( url , params ) url = req . url response = self . datalake_requests ( url , 'get' , headers = self . _get_headers ( output = output )) return parse_response ( response ) Ancestors (in MRO) datalake.endpoints.endpoint.Endpoint Class variables OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY ordering_list Methods advanced_search_from_query_body def advanced_search_from_query_body ( * args , ** kwargs ) View Source def wrapper ( * args , ** kwargs ) : if kwargs . get ( ' output ' ) and kwargs [ ' output ' ] not in outputs : raise ValueError ( f ' {kwargs[\"output\"]} output type is not supported. ' f ' Outputs supported are: {display_outputs(outputs)} ' ) return function ( * args , ** kwargs ) advanced_search_from_query_hash def advanced_search_from_query_hash ( * args , ** kwargs ) View Source def wrapper ( * args , ** kwargs ) : if kwargs . get ( ' output ' ) and kwargs [ ' output ' ] not in outputs : raise ValueError ( f ' {kwargs[\"output\"]} output type is not supported. ' f ' Outputs supported are: {display_outputs(outputs)} ' ) return function ( * args , ** kwargs ) datalake_requests def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response","title":"Advanced Search"},{"location":"reference/datalake/endpoints/advanced_search/#module-datalakeendpointsadvanced_search","text":"None None View Source from datalake.endpoints.endpoint import Endpoint from datalake.common.ouput import parse_response , Output , output_supported from requests.sessions import PreparedRequest from typing import List class AdvancedSearch ( Endpoint ): ordering_list = [ 'first_seen' , '-first_seen' , 'last_updated' , '-last_updated' , 'events_count' , '-events_count' , 'sources_count' , '-sources_count' ] @output_supported ({ Output . JSON , Output . STIX , Output . MISP , Output . CSV }) def advanced_search_from_query_body ( self , query_body : dict , limit : int = 20 , offset : int = 0 , ordering : List [ str ] = None , output = Output . JSON ): if not query_body : raise ValueError ( \"query_body is required\" ) if ordering and not set ( ordering ) . issubset ( self . ordering_list ): raise ValueError ( f 'ordering needs to be a list of at least one of the following str : { \", \" . join ( self . ordering_list ) } ' ) body = { 'query_body' : query_body , 'limit' : limit , 'offset' : offset , } if ordering : body [ 'ordering' ] = ordering url = self . _build_url_for_endpoint ( 'advanced-search' ) response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ( output = output )) return parse_response ( response ) @output_supported ({ Output . JSON , Output . STIX , Output . MISP , Output . CSV }) def advanced_search_from_query_hash ( self , query_hash , limit : int = 20 , offset : int = 0 , ordering : List [ str ] = None , output = Output . JSON ): if not query_hash : raise ValueError ( \"query_hash is required\" ) if ordering and not set ( ordering ) . issubset ( self . ordering_list ): raise ValueError ( f 'ordering needs to be a list of at least one of the following str : { \", \" . join ( self . ordering_list ) } ' ) url = self . _build_url_for_endpoint ( 'advanced-search-hash' ) . format ( query_hash = query_hash ) params = { 'limit' : limit , 'offset' : offset } if ordering : params [ 'ordering' ] = ordering req = PreparedRequest () req . prepare_url ( url , params ) url = req . url response = self . datalake_requests ( url , 'get' , headers = self . _get_headers ( output = output )) return parse_response ( response )","title":"Module datalake.endpoints.advanced_search"},{"location":"reference/datalake/endpoints/advanced_search/#classes","text":"","title":"Classes"},{"location":"reference/datalake/endpoints/advanced_search/#advancedsearch","text":"class AdvancedSearch ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class AdvancedSearch ( Endpoint ) : ordering_list = [ 'first_seen', '-first_seen', 'last_updated', '-last_updated', 'events_count', '-events_count', 'sources_count', '-sources_count' ] @output_supported ( { Output . JSON , Output . STIX , Output . MISP , Output . CSV } ) def advanced_search_from_query_body ( self , query_body : dict , limit : int = 20 , offset : int = 0 , ordering : List [ str ] = None , output = Output . JSON ) : if not query_body : raise ValueError ( \"query_body is required\" ) if ordering and not set ( ordering ). issubset ( self . ordering_list ) : raise ValueError ( f 'ordering needs to be a list of at least one of the following str : {\", \".join(self.ordering_list)}' ) body = { 'query_body' : query_body , 'limit' : limit , 'offset' : offset , } if ordering : body [ 'ordering' ] = ordering url = self . _build_url_for_endpoint ( 'advanced-search' ) response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ( output = output )) return parse_response ( response ) @output_supported ( { Output . JSON , Output . STIX , Output . MISP , Output . CSV } ) def advanced_search_from_query_hash ( self , query_hash , limit : int = 20 , offset : int = 0 , ordering : List [ str ] = None , output = Output . JSON ) : if not query_hash : raise ValueError ( \"query_hash is required\" ) if ordering and not set ( ordering ). issubset ( self . ordering_list ) : raise ValueError ( f 'ordering needs to be a list of at least one of the following str : {\", \".join(self.ordering_list)}' ) url = self . _build_url_for_endpoint ( 'advanced-search-hash' ). format ( query_hash = query_hash ) params = { 'limit' : limit , 'offset' : offset } if ordering : params [ 'ordering' ] = ordering req = PreparedRequest () req . prepare_url ( url , params ) url = req . url response = self . datalake_requests ( url , 'get' , headers = self . _get_headers ( output = output )) return parse_response ( response )","title":"AdvancedSearch"},{"location":"reference/datalake/endpoints/advanced_search/#ancestors-in-mro","text":"datalake.endpoints.endpoint.Endpoint","title":"Ancestors (in MRO)"},{"location":"reference/datalake/endpoints/advanced_search/#class-variables","text":"OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY ordering_list","title":"Class variables"},{"location":"reference/datalake/endpoints/advanced_search/#methods","text":"","title":"Methods"},{"location":"reference/datalake/endpoints/advanced_search/#advanced_search_from_query_body","text":"def advanced_search_from_query_body ( * args , ** kwargs ) View Source def wrapper ( * args , ** kwargs ) : if kwargs . get ( ' output ' ) and kwargs [ ' output ' ] not in outputs : raise ValueError ( f ' {kwargs[\"output\"]} output type is not supported. ' f ' Outputs supported are: {display_outputs(outputs)} ' ) return function ( * args , ** kwargs )","title":"advanced_search_from_query_body"},{"location":"reference/datalake/endpoints/advanced_search/#advanced_search_from_query_hash","text":"def advanced_search_from_query_hash ( * args , ** kwargs ) View Source def wrapper ( * args , ** kwargs ) : if kwargs . get ( ' output ' ) and kwargs [ ' output ' ] not in outputs : raise ValueError ( f ' {kwargs[\"output\"]} output type is not supported. ' f ' Outputs supported are: {display_outputs(outputs)} ' ) return function ( * args , ** kwargs )","title":"advanced_search_from_query_hash"},{"location":"reference/datalake/endpoints/advanced_search/#datalake_requests","text":"def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response","title":"datalake_requests"},{"location":"reference/datalake/endpoints/bulk_search/","text":"Module datalake.endpoints.bulk_search None None View Source from http.client import ResponseNotReady from requests import Response from datalake import BulkSearchNotFound from datalake.api_objects.bulk_search_task import BulkSearchTask from datalake.common.ouput import parse_response , Output , output_supported from datalake.endpoints import Endpoint class BulkSearch ( Endpoint ): def create_task ( self , for_stix_export : bool = False , query_body : dict = None , query_hash : str = None , query_fields : list = None ) -> BulkSearchTask : if not query_body and not query_hash : raise ValueError ( \"Either a query_body or query_hash is required\" ) body = { \"query_fields\" : query_fields } if query_fields else {} if query_body : body [ 'query_body' ] = query_body else : body [ 'query_hash' ] = query_hash if for_stix_export : body [ 'for_stix_export' ] = for_stix_export url = self . _build_url_for_endpoint ( 'bulk-search' ) response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()) . json () return self . get_task ( response [ 'task_uuid' ]) def get_task ( self , task_uuid ) -> BulkSearchTask : url = self . _build_url_for_endpoint ( \"bulk-search-task\" ) body = { \"task_uuid\" : task_uuid } response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()) . json () results = response [ 'results' ] if len ( results ) != 1 : raise BulkSearchNotFound bs_as_json = results [ 0 ] return BulkSearchTask ( endpoint = self , ** bs_as_json ) @output_supported ({ Output . JSON , Output . JSON_ZIP , Output . STIX , Output . STIX_ZIP , Output . CSV , Output . CSV_ZIP }) def download ( self , task_uuid , output = Output . JSON , stream = False ): \"\"\" Download the bulk search task with the given uuid. Stream parameter enables the raw stream to be returned, allowing it to be processed by chunks. \"\"\" url = self . _build_url_for_endpoint ( 'retrieve-bulk-search' ) url = url . format ( task_uuid = task_uuid ) response : Response = self . datalake_requests ( url , 'get' , headers = self . _get_headers ( output = output ), stream = stream ) if response . status_code == 202 : raise ResponseNotReady ( response . json () . get ( 'message' , '' )) if stream : return response # Return the raw response return parse_response ( response ) Classes BulkSearch class BulkSearch ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class BulkSearch ( Endpoint ): def create_task ( self , for_stix_export : bool = False , query_body : dict = None , query_hash : str = None , query_fields : list = None ) -> BulkSearchTask : if not query_body and not query_hash : raise ValueError ( \"Either a query_body or query_hash is required\" ) body = { \"query_fields\" : query_fields } if query_fields else {} if query_body : body [ 'query_body' ] = query_body else : body [ 'query_hash' ] = query_hash if for_stix_export : body [ 'for_stix_export' ] = for_stix_export url = self . _build_url_for_endpoint ( 'bulk-search' ) response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()) . json () return self . get_task ( response [ 'task_uuid' ]) def get_task ( self , task_uuid ) -> BulkSearchTask : url = self . _build_url_for_endpoint ( \"bulk-search-task\" ) body = { \"task_uuid\" : task_uuid } response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()) . json () results = response [ 'results' ] if len ( results ) != 1 : raise BulkSearchNotFound bs_as_json = results [ 0 ] return BulkSearchTask ( endpoint = self , ** bs_as_json ) @ output_supported ({ Output . JSON , Output . JSON_ZIP , Output . STIX , Output . STIX_ZIP , Output . CSV , Output . CSV_ZIP }) def download ( self , task_uuid , output = Output . JSON , stream = False ): \"\"\" Download the bulk search task with the given uuid. Stream parameter enables the raw stream to be returned, allowing it to be processed by chunks. \"\"\" url = self . _build_url_for_endpoint ( 'retrieve-bulk-search' ) url = url . format ( task_uuid = task_uuid ) response : Response = self . datalake_requests ( url , 'get' , headers = self . _get_headers ( output = output ), stream = stream ) if response . status_code == 202 : raise ResponseNotReady ( response . json () . get ( 'message' , '' )) if stream : return response # Return the raw response return parse_response ( response ) Ancestors (in MRO) datalake.endpoints.endpoint.Endpoint Class variables OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY Methods create_task def create_task ( self , for_stix_export : bool = False , query_body : dict = None , query_hash : str = None , query_fields : list = None ) -> datalake . api_objects . bulk_search_task . BulkSearchTask View Source def create_task ( self , for_stix_export : bool = False , query_body : dict = None , query_hash : str = None , query_fields : list = None ) -> BulkSearchTask : if not query_body and not query_hash : raise ValueError ( \"Either a query_body or query_hash is required\" ) body = { \"query_fields\" : query_fields } if query_fields else {} if query_body : body [ 'query_body' ] = query_body else : body [ 'query_hash' ] = query_hash if for_stix_export : body [ 'for_stix_export' ] = for_stix_export url = self . _build_url_for_endpoint ( 'bulk-search' ) response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()) . json () return self . get_task ( response [ 'task_uuid' ]) datalake_requests def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response download def download ( * args , ** kwargs ) View Source def wrapper ( * args , ** kwargs ) : if kwargs . get ( ' output ' ) and kwargs [ ' output ' ] not in outputs : raise ValueError ( f ' {kwargs[\"output\"]} output type is not supported. ' f ' Outputs supported are: {display_outputs(outputs)} ' ) return function ( * args , ** kwargs ) get_task def get_task ( self , task_uuid ) -> datalake . api_objects . bulk_search_task . BulkSearchTask View Source def get_task ( self , task_uuid ) -> BulkSearchTask : url = self . _build_url_for_endpoint ( \"bulk-search-task\" ) body = { \"task_uuid\" : task_uuid } response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()). json () results = response [ 'results' ] if len ( results ) != 1 : raise BulkSearchNotFound bs_as_json = results [ 0 ] return BulkSearchTask ( endp oint = self , ** bs_as_json )","title":"Bulk Search"},{"location":"reference/datalake/endpoints/bulk_search/#module-datalakeendpointsbulk_search","text":"None None View Source from http.client import ResponseNotReady from requests import Response from datalake import BulkSearchNotFound from datalake.api_objects.bulk_search_task import BulkSearchTask from datalake.common.ouput import parse_response , Output , output_supported from datalake.endpoints import Endpoint class BulkSearch ( Endpoint ): def create_task ( self , for_stix_export : bool = False , query_body : dict = None , query_hash : str = None , query_fields : list = None ) -> BulkSearchTask : if not query_body and not query_hash : raise ValueError ( \"Either a query_body or query_hash is required\" ) body = { \"query_fields\" : query_fields } if query_fields else {} if query_body : body [ 'query_body' ] = query_body else : body [ 'query_hash' ] = query_hash if for_stix_export : body [ 'for_stix_export' ] = for_stix_export url = self . _build_url_for_endpoint ( 'bulk-search' ) response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()) . json () return self . get_task ( response [ 'task_uuid' ]) def get_task ( self , task_uuid ) -> BulkSearchTask : url = self . _build_url_for_endpoint ( \"bulk-search-task\" ) body = { \"task_uuid\" : task_uuid } response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()) . json () results = response [ 'results' ] if len ( results ) != 1 : raise BulkSearchNotFound bs_as_json = results [ 0 ] return BulkSearchTask ( endpoint = self , ** bs_as_json ) @output_supported ({ Output . JSON , Output . JSON_ZIP , Output . STIX , Output . STIX_ZIP , Output . CSV , Output . CSV_ZIP }) def download ( self , task_uuid , output = Output . JSON , stream = False ): \"\"\" Download the bulk search task with the given uuid. Stream parameter enables the raw stream to be returned, allowing it to be processed by chunks. \"\"\" url = self . _build_url_for_endpoint ( 'retrieve-bulk-search' ) url = url . format ( task_uuid = task_uuid ) response : Response = self . datalake_requests ( url , 'get' , headers = self . _get_headers ( output = output ), stream = stream ) if response . status_code == 202 : raise ResponseNotReady ( response . json () . get ( 'message' , '' )) if stream : return response # Return the raw response return parse_response ( response )","title":"Module datalake.endpoints.bulk_search"},{"location":"reference/datalake/endpoints/bulk_search/#classes","text":"","title":"Classes"},{"location":"reference/datalake/endpoints/bulk_search/#bulksearch","text":"class BulkSearch ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class BulkSearch ( Endpoint ): def create_task ( self , for_stix_export : bool = False , query_body : dict = None , query_hash : str = None , query_fields : list = None ) -> BulkSearchTask : if not query_body and not query_hash : raise ValueError ( \"Either a query_body or query_hash is required\" ) body = { \"query_fields\" : query_fields } if query_fields else {} if query_body : body [ 'query_body' ] = query_body else : body [ 'query_hash' ] = query_hash if for_stix_export : body [ 'for_stix_export' ] = for_stix_export url = self . _build_url_for_endpoint ( 'bulk-search' ) response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()) . json () return self . get_task ( response [ 'task_uuid' ]) def get_task ( self , task_uuid ) -> BulkSearchTask : url = self . _build_url_for_endpoint ( \"bulk-search-task\" ) body = { \"task_uuid\" : task_uuid } response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()) . json () results = response [ 'results' ] if len ( results ) != 1 : raise BulkSearchNotFound bs_as_json = results [ 0 ] return BulkSearchTask ( endpoint = self , ** bs_as_json ) @ output_supported ({ Output . JSON , Output . JSON_ZIP , Output . STIX , Output . STIX_ZIP , Output . CSV , Output . CSV_ZIP }) def download ( self , task_uuid , output = Output . JSON , stream = False ): \"\"\" Download the bulk search task with the given uuid. Stream parameter enables the raw stream to be returned, allowing it to be processed by chunks. \"\"\" url = self . _build_url_for_endpoint ( 'retrieve-bulk-search' ) url = url . format ( task_uuid = task_uuid ) response : Response = self . datalake_requests ( url , 'get' , headers = self . _get_headers ( output = output ), stream = stream ) if response . status_code == 202 : raise ResponseNotReady ( response . json () . get ( 'message' , '' )) if stream : return response # Return the raw response return parse_response ( response )","title":"BulkSearch"},{"location":"reference/datalake/endpoints/bulk_search/#ancestors-in-mro","text":"datalake.endpoints.endpoint.Endpoint","title":"Ancestors (in MRO)"},{"location":"reference/datalake/endpoints/bulk_search/#class-variables","text":"OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY","title":"Class variables"},{"location":"reference/datalake/endpoints/bulk_search/#methods","text":"","title":"Methods"},{"location":"reference/datalake/endpoints/bulk_search/#create_task","text":"def create_task ( self , for_stix_export : bool = False , query_body : dict = None , query_hash : str = None , query_fields : list = None ) -> datalake . api_objects . bulk_search_task . BulkSearchTask View Source def create_task ( self , for_stix_export : bool = False , query_body : dict = None , query_hash : str = None , query_fields : list = None ) -> BulkSearchTask : if not query_body and not query_hash : raise ValueError ( \"Either a query_body or query_hash is required\" ) body = { \"query_fields\" : query_fields } if query_fields else {} if query_body : body [ 'query_body' ] = query_body else : body [ 'query_hash' ] = query_hash if for_stix_export : body [ 'for_stix_export' ] = for_stix_export url = self . _build_url_for_endpoint ( 'bulk-search' ) response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()) . json () return self . get_task ( response [ 'task_uuid' ])","title":"create_task"},{"location":"reference/datalake/endpoints/bulk_search/#datalake_requests","text":"def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response","title":"datalake_requests"},{"location":"reference/datalake/endpoints/bulk_search/#download","text":"def download ( * args , ** kwargs ) View Source def wrapper ( * args , ** kwargs ) : if kwargs . get ( ' output ' ) and kwargs [ ' output ' ] not in outputs : raise ValueError ( f ' {kwargs[\"output\"]} output type is not supported. ' f ' Outputs supported are: {display_outputs(outputs)} ' ) return function ( * args , ** kwargs )","title":"download"},{"location":"reference/datalake/endpoints/bulk_search/#get_task","text":"def get_task ( self , task_uuid ) -> datalake . api_objects . bulk_search_task . BulkSearchTask View Source def get_task ( self , task_uuid ) -> BulkSearchTask : url = self . _build_url_for_endpoint ( \"bulk-search-task\" ) body = { \"task_uuid\" : task_uuid } response = self . datalake_requests ( url , 'post' , post_body = body , headers = self . _post_headers ()). json () results = response [ 'results' ] if len ( results ) != 1 : raise BulkSearchNotFound bs_as_json = results [ 0 ] return BulkSearchTask ( endp oint = self , ** bs_as_json )","title":"get_task"},{"location":"reference/datalake/endpoints/endpoint/","text":"Module datalake.endpoints.endpoint Base class used by all endpoints to request the API None View Source \"\"\" Base class used by all endpoints to request the API \"\"\" import json import logging import os from urllib.parse import urljoin import requests from requests import Response from datalake.common.ouput import Output from datalake.common.logger import logger from datalake.common.throttler import throttle from datalake.common.token_manager import TokenManager class Endpoint : OCD_DTL_QUOTA_TIME = int ( os . getenv ( 'OCD_DTL_QUOTA_TIME' , 1 )) OCD_DTL_REQUESTS_PER_QUOTA_TIME = int ( os . getenv ( 'OCD_DTL_REQUESTS_PER_QUOTA_TIME' , 5 )) logger . debug ( f 'Throttle selected: { OCD_DTL_REQUESTS_PER_QUOTA_TIME } queries per { OCD_DTL_QUOTA_TIME } s' ) SET_MAX_RETRY = 3 def __init__ ( self , endpoint_config : dict , environment : str , token_manager : TokenManager ): self . endpoint_config = endpoint_config self . environment = environment self . terminal_size = self . _get_terminal_size () self . token_manager = token_manager self . SET_MAX_RETRY = 3 @staticmethod def _get_terminal_size () -> int : \"\"\"Return the terminal size for pretty print\"\"\" try : terminal_size = os . get_terminal_size () if len ( terminal_size ) == 2 : return int ( terminal_size [ 1 ]) except OSError : logger . debug ( \"Couldn't get terminal size, falling back to 80 char wide\" ) return 80 @throttle ( period = OCD_DTL_QUOTA_TIME , call_per_period = OCD_DTL_REQUESTS_PER_QUOTA_TIME , ) def datalake_requests ( self , url : str , method : str , headers : dict , post_body : dict = None , stream = False , ) -> Response : \"\"\" Use it to request the API \"\"\" tries_left = self . SET_MAX_RETRY while tries_left > 0 : headers [ 'Authorization' ] = self . token_manager . access_token logger . debug ( self . _pretty_debug_request ( url , method , post_body , headers )) response = self . _send_request ( url , method , headers , post_body , stream = stream ) if logger . isEnabledFor ( logging . DEBUG ): # Don't compute response.text var if not needed, especially for streaming response logger . debug ( 'API response: \\n %s ' , response . text ) if response . status_code == 401 : logger . warning ( 'Token expired or Missing authorization header. Updating token' ) self . token_manager . process_auth_error ( response . json () . get ( 'messages' )) elif response . status_code == 422 : logger . warning ( 'Bad authorization header. Updating token' ) logger . debug ( f '422 HTTP code: { response . text } ' ) self . token_manager . process_auth_error ( response . json () . get ( 'messages' )) elif response . status_code < 200 or response . status_code > 299 : logger . error ( f 'API returned non 2xx response code : { response . status_code } \\n { response . text } \\n Retrying' ) else : return response tries_left -= 1 logger . error ( 'Request failed' ) raise ValueError ( f ' { response . status_code } : { response . text . strip () } ' ) @staticmethod def _post_headers ( output = Output . JSON ) -> dict : \"\"\"headers for POST endpoints\"\"\" return { 'Accept' : output . value , 'Content-Type' : 'application/json' } @staticmethod def _get_headers ( output = Output . JSON ) -> dict : \"\"\"headers for GET endpoints\"\"\" return { 'Accept' : output . value } @staticmethod def _send_request ( url : str , method : str , headers : dict , data : dict , stream = False ) -> Response : \"\"\" Send the correct http request to url from method [get, post, delete, patch, put]. Raise a TypeError 'Unknown method to requests {method}' when the method is not one of the above. \"\"\" common_kwargs = { 'url' : url , 'headers' : headers , 'stream' : stream , } if method == 'get' : api_response = requests . get ( ** common_kwargs ) elif method == 'post' : api_response = requests . post ( ** common_kwargs , data = json . dumps ( data )) elif method == 'delete' : api_response = requests . delete ( ** common_kwargs , data = json . dumps ( data )) elif method == 'patch' : api_response = requests . patch ( ** common_kwargs , data = json . dumps ( data )) elif method == 'put' : api_response = requests . put ( ** common_kwargs , data = json . dumps ( data )) else : logger . debug ( 'ERROR : Wrong requests, please only do [get, post, put, patch, delete] method' ) raise TypeError ( 'Unknown method to requests %s ' , method ) return api_response def _pretty_debug_request ( self , url : str , method : str , data : dict , headers : dict ): debug = ( '-' * self . terminal_size + 'DEBUG - datalake_requests: \\n ' + f ' - url: \\n { url } \\n ' + f ' - method: \\n { method } \\n ' + f ' - headers: \\n { headers } \\n ' + f ' - data: \\n { data } \\n ' + f ' - token: \\n { self . token_manager . access_token } \\n ' + f ' - refresh_token: \\n { self . token_manager . refresh_token } \\n ' + '-' * self . terminal_size ) return debug def _build_url_for_endpoint ( self , endpoint_name ): base_url = urljoin ( self . endpoint_config [ 'main' ][ self . environment ], self . endpoint_config [ 'api_version' ]) enpoints = self . endpoint_config [ 'endpoints' ] return urljoin ( base_url , enpoints [ endpoint_name ], allow_fragments = True ) Classes Endpoint class Endpoint ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class Endpoint : OCD_DTL_QUOTA_TIME = int ( os . getenv ( 'OCD_DTL_QUOTA_TIME' , 1 )) OCD_DTL_REQUESTS_PER_QUOTA_TIME = int ( os . getenv ( 'OCD_DTL_REQUESTS_PER_QUOTA_TIME' , 5 )) logger . debug ( f 'Throttle selected: {OCD_DTL_REQUESTS_PER_QUOTA_TIME} queries per {OCD_DTL_QUOTA_TIME}s' ) SET_MAX_RETRY = 3 def __init__ ( self , endpoint_config : dict , environment : str , token_manager : TokenManager ) : self . endpoint_config = endpoint_config self . environment = environment self . terminal_size = self . _get_terminal_size () self . token_manager = token_manager self . SET_MAX_RETRY = 3 @staticmethod def _get_terminal_size () -> int : \"\"\"Return the terminal size for pretty print\"\"\" try : terminal_size = os . get_terminal_size () if len ( terminal_size ) == 2 : return int ( terminal_size [ 1 ] ) except OSError : logger . debug ( \"Couldn't get terminal size, falling back to 80 char wide\" ) return 80 @throttle ( period = OCD_DTL_QUOTA_TIME , call_per_period = OCD_DTL_REQUESTS_PER_QUOTA_TIME , ) def datalake_requests ( self , url : str , method : str , headers : dict , post_body : dict = None , stream = False , ) -> Response : \"\"\" Use it to request the API \"\"\" tries_left = self . SET_MAX_RETRY while tries_left > 0 : headers [ 'Authorization' ] = self . token_manager . access_token logger . debug ( self . _pretty_debug_request ( url , method , post_body , headers )) response = self . _send_request ( url , method , headers , post_body , stream = stream ) if logger . isEnabledFor ( logging . DEBUG ) : # Don 't compute response.text var if not needed, especially for streaming response logger.debug(' API response : \\ n % s ', response.text) if response.status_code == 401: logger.warning(' Token expired or Missing authorization header . Updating token ') self.token_manager.process_auth_error(response.json().get(' messages ')) elif response.status_code == 422: logger.warning(' Bad authorization header . Updating token ') logger.debug(f' 422 HTTP code : { response . text } ') self.token_manager.process_auth_error(response.json().get(' messages ')) elif response.status_code < 200 or response.status_code > 299: logger.error( f' API returned non 2 xx response code : { response . status_code }\\ n { response . text }\\ n Retrying ' ) else: return response tries_left -= 1 logger.error(' Request failed ') raise ValueError(f' { response . status_code }: { response . text . strip () } ') @staticmethod def _post_headers(output=Output.JSON) -> dict: \"\"\"headers for POST endpoints\"\"\" return {' Accept ': output.value, ' Content - Type ': ' application / json '} @staticmethod def _get_headers(output=Output.JSON) -> dict: \"\"\"headers for GET endpoints\"\"\" return {' Accept ': output.value} @staticmethod def _send_request(url: str, method: str, headers: dict, data: dict, stream=False) -> Response: \"\"\" Send the correct http request to url from method [get, post, delete, patch, put]. Raise a TypeError ' Unknown method to requests { method } ' when the method is not one of the above. \"\"\" common_kwargs = { ' url ': url, ' headers ': headers, ' stream ': stream, } if method == ' get ': api_response = requests.get(**common_kwargs) elif method == ' post ': api_response = requests.post(**common_kwargs, data=json.dumps(data)) elif method == ' delete ': api_response = requests.delete(**common_kwargs, data=json.dumps(data)) elif method == ' patch ': api_response = requests.patch(**common_kwargs, data=json.dumps(data)) elif method == ' put ': api_response = requests.put(**common_kwargs, data=json.dumps(data)) else: logger.debug(' ERROR : Wrong requests , please only do [ get, post, put, patch, delete ] method ') raise TypeError(' Unknown method to requests % s ', method) return api_response def _pretty_debug_request(self, url: str, method: str, data: dict, headers: dict): debug = (' - ' * self.terminal_size + ' DEBUG - datalake_requests : \\ n ' + f' - url : \\ n { url }\\ n ' + f' - method : \\ n { method }\\ n ' + f' - headers : \\ n { headers }\\ n ' + f' - data : \\ n { data }\\ n ' + f' - token : \\ n { self . token_manager . access_token }\\ n ' + f' - refresh_token : \\ n { self . token_manager . refresh_token }\\ n ' + ' - ' * self.terminal_size) return debug def _build_url_for_endpoint(self, endpoint_name): base_url = urljoin(self.endpoint_config[' main '][self.environment], self.endpoint_config[' api_version ']) enpoints = self.endpoint_config[' endpoints '] return urljoin ( base_url , enpoints [ endpoint_name ] , allow_fragments = True ) Descendants datalake.endpoints.threats.Threats datalake.endpoints.bulk_search.BulkSearch datalake.endpoints.tags.Tags datalake.endpoints.advanced_search.AdvancedSearch datalake.endpoints.sightings.Sightings Class variables OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY Methods datalake_requests def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response","title":"Endpoint"},{"location":"reference/datalake/endpoints/endpoint/#module-datalakeendpointsendpoint","text":"Base class used by all endpoints to request the API None View Source \"\"\" Base class used by all endpoints to request the API \"\"\" import json import logging import os from urllib.parse import urljoin import requests from requests import Response from datalake.common.ouput import Output from datalake.common.logger import logger from datalake.common.throttler import throttle from datalake.common.token_manager import TokenManager class Endpoint : OCD_DTL_QUOTA_TIME = int ( os . getenv ( 'OCD_DTL_QUOTA_TIME' , 1 )) OCD_DTL_REQUESTS_PER_QUOTA_TIME = int ( os . getenv ( 'OCD_DTL_REQUESTS_PER_QUOTA_TIME' , 5 )) logger . debug ( f 'Throttle selected: { OCD_DTL_REQUESTS_PER_QUOTA_TIME } queries per { OCD_DTL_QUOTA_TIME } s' ) SET_MAX_RETRY = 3 def __init__ ( self , endpoint_config : dict , environment : str , token_manager : TokenManager ): self . endpoint_config = endpoint_config self . environment = environment self . terminal_size = self . _get_terminal_size () self . token_manager = token_manager self . SET_MAX_RETRY = 3 @staticmethod def _get_terminal_size () -> int : \"\"\"Return the terminal size for pretty print\"\"\" try : terminal_size = os . get_terminal_size () if len ( terminal_size ) == 2 : return int ( terminal_size [ 1 ]) except OSError : logger . debug ( \"Couldn't get terminal size, falling back to 80 char wide\" ) return 80 @throttle ( period = OCD_DTL_QUOTA_TIME , call_per_period = OCD_DTL_REQUESTS_PER_QUOTA_TIME , ) def datalake_requests ( self , url : str , method : str , headers : dict , post_body : dict = None , stream = False , ) -> Response : \"\"\" Use it to request the API \"\"\" tries_left = self . SET_MAX_RETRY while tries_left > 0 : headers [ 'Authorization' ] = self . token_manager . access_token logger . debug ( self . _pretty_debug_request ( url , method , post_body , headers )) response = self . _send_request ( url , method , headers , post_body , stream = stream ) if logger . isEnabledFor ( logging . DEBUG ): # Don't compute response.text var if not needed, especially for streaming response logger . debug ( 'API response: \\n %s ' , response . text ) if response . status_code == 401 : logger . warning ( 'Token expired or Missing authorization header. Updating token' ) self . token_manager . process_auth_error ( response . json () . get ( 'messages' )) elif response . status_code == 422 : logger . warning ( 'Bad authorization header. Updating token' ) logger . debug ( f '422 HTTP code: { response . text } ' ) self . token_manager . process_auth_error ( response . json () . get ( 'messages' )) elif response . status_code < 200 or response . status_code > 299 : logger . error ( f 'API returned non 2xx response code : { response . status_code } \\n { response . text } \\n Retrying' ) else : return response tries_left -= 1 logger . error ( 'Request failed' ) raise ValueError ( f ' { response . status_code } : { response . text . strip () } ' ) @staticmethod def _post_headers ( output = Output . JSON ) -> dict : \"\"\"headers for POST endpoints\"\"\" return { 'Accept' : output . value , 'Content-Type' : 'application/json' } @staticmethod def _get_headers ( output = Output . JSON ) -> dict : \"\"\"headers for GET endpoints\"\"\" return { 'Accept' : output . value } @staticmethod def _send_request ( url : str , method : str , headers : dict , data : dict , stream = False ) -> Response : \"\"\" Send the correct http request to url from method [get, post, delete, patch, put]. Raise a TypeError 'Unknown method to requests {method}' when the method is not one of the above. \"\"\" common_kwargs = { 'url' : url , 'headers' : headers , 'stream' : stream , } if method == 'get' : api_response = requests . get ( ** common_kwargs ) elif method == 'post' : api_response = requests . post ( ** common_kwargs , data = json . dumps ( data )) elif method == 'delete' : api_response = requests . delete ( ** common_kwargs , data = json . dumps ( data )) elif method == 'patch' : api_response = requests . patch ( ** common_kwargs , data = json . dumps ( data )) elif method == 'put' : api_response = requests . put ( ** common_kwargs , data = json . dumps ( data )) else : logger . debug ( 'ERROR : Wrong requests, please only do [get, post, put, patch, delete] method' ) raise TypeError ( 'Unknown method to requests %s ' , method ) return api_response def _pretty_debug_request ( self , url : str , method : str , data : dict , headers : dict ): debug = ( '-' * self . terminal_size + 'DEBUG - datalake_requests: \\n ' + f ' - url: \\n { url } \\n ' + f ' - method: \\n { method } \\n ' + f ' - headers: \\n { headers } \\n ' + f ' - data: \\n { data } \\n ' + f ' - token: \\n { self . token_manager . access_token } \\n ' + f ' - refresh_token: \\n { self . token_manager . refresh_token } \\n ' + '-' * self . terminal_size ) return debug def _build_url_for_endpoint ( self , endpoint_name ): base_url = urljoin ( self . endpoint_config [ 'main' ][ self . environment ], self . endpoint_config [ 'api_version' ]) enpoints = self . endpoint_config [ 'endpoints' ] return urljoin ( base_url , enpoints [ endpoint_name ], allow_fragments = True )","title":"Module datalake.endpoints.endpoint"},{"location":"reference/datalake/endpoints/endpoint/#classes","text":"","title":"Classes"},{"location":"reference/datalake/endpoints/endpoint/#endpoint","text":"class Endpoint ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class Endpoint : OCD_DTL_QUOTA_TIME = int ( os . getenv ( 'OCD_DTL_QUOTA_TIME' , 1 )) OCD_DTL_REQUESTS_PER_QUOTA_TIME = int ( os . getenv ( 'OCD_DTL_REQUESTS_PER_QUOTA_TIME' , 5 )) logger . debug ( f 'Throttle selected: {OCD_DTL_REQUESTS_PER_QUOTA_TIME} queries per {OCD_DTL_QUOTA_TIME}s' ) SET_MAX_RETRY = 3 def __init__ ( self , endpoint_config : dict , environment : str , token_manager : TokenManager ) : self . endpoint_config = endpoint_config self . environment = environment self . terminal_size = self . _get_terminal_size () self . token_manager = token_manager self . SET_MAX_RETRY = 3 @staticmethod def _get_terminal_size () -> int : \"\"\"Return the terminal size for pretty print\"\"\" try : terminal_size = os . get_terminal_size () if len ( terminal_size ) == 2 : return int ( terminal_size [ 1 ] ) except OSError : logger . debug ( \"Couldn't get terminal size, falling back to 80 char wide\" ) return 80 @throttle ( period = OCD_DTL_QUOTA_TIME , call_per_period = OCD_DTL_REQUESTS_PER_QUOTA_TIME , ) def datalake_requests ( self , url : str , method : str , headers : dict , post_body : dict = None , stream = False , ) -> Response : \"\"\" Use it to request the API \"\"\" tries_left = self . SET_MAX_RETRY while tries_left > 0 : headers [ 'Authorization' ] = self . token_manager . access_token logger . debug ( self . _pretty_debug_request ( url , method , post_body , headers )) response = self . _send_request ( url , method , headers , post_body , stream = stream ) if logger . isEnabledFor ( logging . DEBUG ) : # Don 't compute response.text var if not needed, especially for streaming response logger.debug(' API response : \\ n % s ', response.text) if response.status_code == 401: logger.warning(' Token expired or Missing authorization header . Updating token ') self.token_manager.process_auth_error(response.json().get(' messages ')) elif response.status_code == 422: logger.warning(' Bad authorization header . Updating token ') logger.debug(f' 422 HTTP code : { response . text } ') self.token_manager.process_auth_error(response.json().get(' messages ')) elif response.status_code < 200 or response.status_code > 299: logger.error( f' API returned non 2 xx response code : { response . status_code }\\ n { response . text }\\ n Retrying ' ) else: return response tries_left -= 1 logger.error(' Request failed ') raise ValueError(f' { response . status_code }: { response . text . strip () } ') @staticmethod def _post_headers(output=Output.JSON) -> dict: \"\"\"headers for POST endpoints\"\"\" return {' Accept ': output.value, ' Content - Type ': ' application / json '} @staticmethod def _get_headers(output=Output.JSON) -> dict: \"\"\"headers for GET endpoints\"\"\" return {' Accept ': output.value} @staticmethod def _send_request(url: str, method: str, headers: dict, data: dict, stream=False) -> Response: \"\"\" Send the correct http request to url from method [get, post, delete, patch, put]. Raise a TypeError ' Unknown method to requests { method } ' when the method is not one of the above. \"\"\" common_kwargs = { ' url ': url, ' headers ': headers, ' stream ': stream, } if method == ' get ': api_response = requests.get(**common_kwargs) elif method == ' post ': api_response = requests.post(**common_kwargs, data=json.dumps(data)) elif method == ' delete ': api_response = requests.delete(**common_kwargs, data=json.dumps(data)) elif method == ' patch ': api_response = requests.patch(**common_kwargs, data=json.dumps(data)) elif method == ' put ': api_response = requests.put(**common_kwargs, data=json.dumps(data)) else: logger.debug(' ERROR : Wrong requests , please only do [ get, post, put, patch, delete ] method ') raise TypeError(' Unknown method to requests % s ', method) return api_response def _pretty_debug_request(self, url: str, method: str, data: dict, headers: dict): debug = (' - ' * self.terminal_size + ' DEBUG - datalake_requests : \\ n ' + f' - url : \\ n { url }\\ n ' + f' - method : \\ n { method }\\ n ' + f' - headers : \\ n { headers }\\ n ' + f' - data : \\ n { data }\\ n ' + f' - token : \\ n { self . token_manager . access_token }\\ n ' + f' - refresh_token : \\ n { self . token_manager . refresh_token }\\ n ' + ' - ' * self.terminal_size) return debug def _build_url_for_endpoint(self, endpoint_name): base_url = urljoin(self.endpoint_config[' main '][self.environment], self.endpoint_config[' api_version ']) enpoints = self.endpoint_config[' endpoints '] return urljoin ( base_url , enpoints [ endpoint_name ] , allow_fragments = True )","title":"Endpoint"},{"location":"reference/datalake/endpoints/endpoint/#descendants","text":"datalake.endpoints.threats.Threats datalake.endpoints.bulk_search.BulkSearch datalake.endpoints.tags.Tags datalake.endpoints.advanced_search.AdvancedSearch datalake.endpoints.sightings.Sightings","title":"Descendants"},{"location":"reference/datalake/endpoints/endpoint/#class-variables","text":"OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY","title":"Class variables"},{"location":"reference/datalake/endpoints/endpoint/#methods","text":"","title":"Methods"},{"location":"reference/datalake/endpoints/endpoint/#datalake_requests","text":"def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response","title":"datalake_requests"},{"location":"reference/datalake/endpoints/sightings/","text":"Module datalake.endpoints.sightings None None View Source from datalake import ThreatType , SightingType , Visibility from datalake.endpoints.endpoint import Endpoint from datalake.common.atom_type import Atom from datetime import datetime from typing import List class Sightings ( Endpoint ): def submit_sighting ( self , start_timestamp : datetime , end_timestamp : datetime , sighting_type : SightingType , visibility : Visibility , count : int , threat_types : List [ ThreatType ] = None , tags : List [ str ] = None , description : str = None , atoms : List [ Atom ] = None , hashkeys : List [ str ] = None ): \"\"\" Submit a list of sightings. Either threat hashkeys or list of atom objects is required. Possible sightings type: POSITIVE, NEGATIVE, NEUTRAL. For POSITIVE and NEGATIVE sightings \"threat_types\" field is required. End date timestamp should always be in the past. \"\"\" payload = self . _prepare_sightings_payload ( atoms , hashkeys , start_timestamp , end_timestamp , sighting_type , visibility , count , threat_types , tags , description ) url = self . _build_url_for_endpoint ( 'submit-sightings' ) res = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) . json () return res @staticmethod def _check_sightings_payload_parameters ( atoms , hashkeys , sighting_type , visibility , count , threat_types ): if not atoms and not hashkeys : raise ValueError ( 'Either threat hashkeys or list of atom objects is required.' ) if count < 1 : raise ValueError ( 'count value minimum: 1' ) if not isinstance ( sighting_type , SightingType ): raise ValueError ( 'sighting_type has to be an instance of the SightingType class.' ) if sighting_type in ( SightingType . POSITIVE , SightingType . NEGATIVE ): if not threat_types or not all ( isinstance ( threat_type , ThreatType ) for threat_type in threat_types ): raise ValueError ( 'For POSITIVE and NEGATIVE sightings \"threat_types\" field is required and has to be ' 'an instance of the Visibility class' ) elif threat_types : raise ValueError ( \"For NEUTRAL sightings, threat_types can't be passed.\" ) if not isinstance ( visibility , Visibility ): raise ValueError ( 'visibility has to be an instance of the Visibility class.' ) def _prepare_sightings_payload ( self , atoms , hashkeys , start_timestamp , end_timestamp , sighting_type : SightingType , visibility , count , threat_types = None , tags = None , description = None ): \"\"\" Internal function to prepare a list of Atoms for sighting submission to the format the API expects. \"\"\" self . _check_sightings_payload_parameters ( atoms , hashkeys , sighting_type , visibility , count , threat_types ) payload = {} # atoms and hashkeys can both be None if atoms : for atom in atoms : if type ( atom ) == Atom or not isinstance ( atom , Atom ): raise TypeError ( \"atoms needs to be a list of Atom subclasses.\" ) atom_dict = atom . generate_atom_json ( for_sightings = True ) if not payload : payload = atom_dict else : payload = { key : payload . get ( key , []) + atom_dict . get ( key , []) for key in set ( list ( payload . keys ()) + list ( atom_dict . keys ()) ) } if hashkeys : payload [ 'hashkeys' ] = hashkeys payload [ 'start_timestamp' ] = start_timestamp . strftime ( \"%Y-%m- %d T%H:%M:%SZ\" ) payload [ 'end_timestamp' ] = end_timestamp . strftime ( \"%Y-%m- %d T%H:%M:%SZ\" ) payload [ 'visibility' ] = visibility . value payload [ 'type' ] = sighting_type . value payload [ 'count' ] = count if sighting_type in ( SightingType . POSITIVE , SightingType . NEGATIVE ): payload [ 'threat_types' ] = [ threat_type . value for threat_type in threat_types ] if tags : payload [ 'tags' ] = tags if description : payload [ 'description' ] = description return payload Classes Sightings class Sightings ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class Sightings ( Endpoint ) : def submit_sighting ( self , start_timestamp : datetime , end_timestamp : datetime , sighting_type : SightingType , visibility : Visibility , count : int , threat_types : List [ ThreatType ] = None , tags : List [ str ] = None , description : str = None , atoms : List [ Atom ] = None , hashkeys : List [ str ] = None ) : \"\"\" Submit a list of sightings. Either threat hashkeys or list of atom objects is required. Possible sightings type: POSITIVE, NEGATIVE, NEUTRAL. For POSITIVE and NEGATIVE sightings \" threat_types \" field is required. End date timestamp should always be in the past. \"\"\" payload = self . _prepare_sightings_payload ( atoms , hashkeys , start_timestamp , end_timestamp , sighting_type , visibility , count , threat_types , tags , description ) url = self . _build_url_for_endpoint ( 'submit-sightings' ) res = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ). json () return res @staticmethod def _check_sightings_payload_parameters ( atoms , hashkeys , sighting_type , visibility , count , threat_types ) : if not atoms and not hashkeys : raise ValueError ( 'Either threat hashkeys or list of atom objects is required.' ) if count < 1 : raise ValueError ( 'count value minimum: 1' ) if not isinstance ( sighting_type , SightingType ) : raise ValueError ( 'sighting_type has to be an instance of the SightingType class.' ) if sighting_type in ( SightingType . POSITIVE , SightingType . NEGATIVE ) : if not threat_types or not all ( isinstance ( threat_type , ThreatType ) for threat_type in threat_types ) : raise ValueError ( 'For POSITIVE and NEGATIVE sightings \"threat_types\" field is required and has to be ' 'an instance of the Visibility class' ) elif threat_types : raise ValueError ( \"For NEUTRAL sightings, threat_types can't be passed.\" ) if not isinstance ( visibility , Visibility ) : raise ValueError ( 'visibility has to be an instance of the Visibility class.' ) def _prepare_sightings_payload ( self , atoms , hashkeys , start_timestamp , end_timestamp , sighting_type : SightingType , visibility , count , threat_types = None , tags = None , description = None ) : \"\"\" Internal function to prepare a list of Atoms for sighting submission to the format the API expects. \"\"\" self . _check_sightings_payload_parameters ( atoms , hashkeys , sighting_type , visibility , count , threat_types ) payload = {} # atoms and hashkeys can both be None if atoms : for atom in atoms : if type ( atom ) == Atom or not isinstance ( atom , Atom ) : raise TypeError ( \"atoms needs to be a list of Atom subclasses.\" ) atom_dict = atom . generate_atom_json ( for_sightings = True ) if not payload : payload = atom_dict else : payload = { key : payload . get ( key , [] ) + atom_dict . get ( key , [] ) for key in set ( list ( payload . keys ()) + list ( atom_dict . keys ()) ) } if hashkeys : payload [ 'hashkeys' ] = hashkeys payload [ 'start_timestamp' ] = start_timestamp . strftime ( \"%Y-%m-%dT%H:%M:%SZ\" ) payload [ 'end_timestamp' ] = end_timestamp . strftime ( \"%Y-%m-%dT%H:%M:%SZ\" ) payload [ 'visibility' ] = visibility . value payload [ 'type' ] = sighting_type . value payload [ 'count' ] = count if sighting_type in ( SightingType . POSITIVE , SightingType . NEGATIVE ) : payload [ 'threat_types' ] = [ threat_type.value for threat_type in threat_types ] if tags : payload [ 'tags' ] = tags if description : payload [ 'description' ] = description return payload Ancestors (in MRO) datalake.endpoints.endpoint.Endpoint Class variables OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY Methods datalake_requests def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response submit_sighting def submit_sighting ( self , start_timestamp : datetime . datetime , end_timestamp : datetime . datetime , sighting_type : datalake . common . atom . SightingType , visibility : datalake . common . atom . Visibility , count : int , threat_types : List [ datalake . common . atom . ThreatType ] = None , tags : List [ str ] = None , description : str = None , atoms : List [ datalake . common . atom_type . Atom ] = None , hashkeys : List [ str ] = None ) Submit a list of sightings. Either threat hashkeys or list of atom objects is required. Possible sightings type: POSITIVE, NEGATIVE, NEUTRAL. For POSITIVE and NEGATIVE sightings \"threat_types\" field is required. End date timestamp should always be in the past. View Source def submit_sighting ( self , start_timestamp : datetime , end_timestamp : datetime , sighting_type : SightingType , visibility : Visibility , count : int , threat_types : List [ ThreatType ] = None , tags : List [ str ] = None , description : str = None , atoms : List [ Atom ] = None , hashkeys : List [ str ] = None ) : \"\"\" Submit a list of sightings. Either threat hashkeys or list of atom objects is required. Possible sightings type: POSITIVE, NEGATIVE, NEUTRAL. For POSITIVE and NEGATIVE sightings \" threat_types \" field is required. End date timestamp should always be in the past. \"\"\" payload = self . _prepare_sightings_payload ( atoms , hashkeys , start_timestamp , end_timestamp , sighting_type , visibility , count , threat_types , tags , description ) url = self . _build_url_for_endpoint ( 'submit-sightings' ) res = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ). json () return res","title":"Sightings"},{"location":"reference/datalake/endpoints/sightings/#module-datalakeendpointssightings","text":"None None View Source from datalake import ThreatType , SightingType , Visibility from datalake.endpoints.endpoint import Endpoint from datalake.common.atom_type import Atom from datetime import datetime from typing import List class Sightings ( Endpoint ): def submit_sighting ( self , start_timestamp : datetime , end_timestamp : datetime , sighting_type : SightingType , visibility : Visibility , count : int , threat_types : List [ ThreatType ] = None , tags : List [ str ] = None , description : str = None , atoms : List [ Atom ] = None , hashkeys : List [ str ] = None ): \"\"\" Submit a list of sightings. Either threat hashkeys or list of atom objects is required. Possible sightings type: POSITIVE, NEGATIVE, NEUTRAL. For POSITIVE and NEGATIVE sightings \"threat_types\" field is required. End date timestamp should always be in the past. \"\"\" payload = self . _prepare_sightings_payload ( atoms , hashkeys , start_timestamp , end_timestamp , sighting_type , visibility , count , threat_types , tags , description ) url = self . _build_url_for_endpoint ( 'submit-sightings' ) res = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) . json () return res @staticmethod def _check_sightings_payload_parameters ( atoms , hashkeys , sighting_type , visibility , count , threat_types ): if not atoms and not hashkeys : raise ValueError ( 'Either threat hashkeys or list of atom objects is required.' ) if count < 1 : raise ValueError ( 'count value minimum: 1' ) if not isinstance ( sighting_type , SightingType ): raise ValueError ( 'sighting_type has to be an instance of the SightingType class.' ) if sighting_type in ( SightingType . POSITIVE , SightingType . NEGATIVE ): if not threat_types or not all ( isinstance ( threat_type , ThreatType ) for threat_type in threat_types ): raise ValueError ( 'For POSITIVE and NEGATIVE sightings \"threat_types\" field is required and has to be ' 'an instance of the Visibility class' ) elif threat_types : raise ValueError ( \"For NEUTRAL sightings, threat_types can't be passed.\" ) if not isinstance ( visibility , Visibility ): raise ValueError ( 'visibility has to be an instance of the Visibility class.' ) def _prepare_sightings_payload ( self , atoms , hashkeys , start_timestamp , end_timestamp , sighting_type : SightingType , visibility , count , threat_types = None , tags = None , description = None ): \"\"\" Internal function to prepare a list of Atoms for sighting submission to the format the API expects. \"\"\" self . _check_sightings_payload_parameters ( atoms , hashkeys , sighting_type , visibility , count , threat_types ) payload = {} # atoms and hashkeys can both be None if atoms : for atom in atoms : if type ( atom ) == Atom or not isinstance ( atom , Atom ): raise TypeError ( \"atoms needs to be a list of Atom subclasses.\" ) atom_dict = atom . generate_atom_json ( for_sightings = True ) if not payload : payload = atom_dict else : payload = { key : payload . get ( key , []) + atom_dict . get ( key , []) for key in set ( list ( payload . keys ()) + list ( atom_dict . keys ()) ) } if hashkeys : payload [ 'hashkeys' ] = hashkeys payload [ 'start_timestamp' ] = start_timestamp . strftime ( \"%Y-%m- %d T%H:%M:%SZ\" ) payload [ 'end_timestamp' ] = end_timestamp . strftime ( \"%Y-%m- %d T%H:%M:%SZ\" ) payload [ 'visibility' ] = visibility . value payload [ 'type' ] = sighting_type . value payload [ 'count' ] = count if sighting_type in ( SightingType . POSITIVE , SightingType . NEGATIVE ): payload [ 'threat_types' ] = [ threat_type . value for threat_type in threat_types ] if tags : payload [ 'tags' ] = tags if description : payload [ 'description' ] = description return payload","title":"Module datalake.endpoints.sightings"},{"location":"reference/datalake/endpoints/sightings/#classes","text":"","title":"Classes"},{"location":"reference/datalake/endpoints/sightings/#sightings","text":"class Sightings ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class Sightings ( Endpoint ) : def submit_sighting ( self , start_timestamp : datetime , end_timestamp : datetime , sighting_type : SightingType , visibility : Visibility , count : int , threat_types : List [ ThreatType ] = None , tags : List [ str ] = None , description : str = None , atoms : List [ Atom ] = None , hashkeys : List [ str ] = None ) : \"\"\" Submit a list of sightings. Either threat hashkeys or list of atom objects is required. Possible sightings type: POSITIVE, NEGATIVE, NEUTRAL. For POSITIVE and NEGATIVE sightings \" threat_types \" field is required. End date timestamp should always be in the past. \"\"\" payload = self . _prepare_sightings_payload ( atoms , hashkeys , start_timestamp , end_timestamp , sighting_type , visibility , count , threat_types , tags , description ) url = self . _build_url_for_endpoint ( 'submit-sightings' ) res = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ). json () return res @staticmethod def _check_sightings_payload_parameters ( atoms , hashkeys , sighting_type , visibility , count , threat_types ) : if not atoms and not hashkeys : raise ValueError ( 'Either threat hashkeys or list of atom objects is required.' ) if count < 1 : raise ValueError ( 'count value minimum: 1' ) if not isinstance ( sighting_type , SightingType ) : raise ValueError ( 'sighting_type has to be an instance of the SightingType class.' ) if sighting_type in ( SightingType . POSITIVE , SightingType . NEGATIVE ) : if not threat_types or not all ( isinstance ( threat_type , ThreatType ) for threat_type in threat_types ) : raise ValueError ( 'For POSITIVE and NEGATIVE sightings \"threat_types\" field is required and has to be ' 'an instance of the Visibility class' ) elif threat_types : raise ValueError ( \"For NEUTRAL sightings, threat_types can't be passed.\" ) if not isinstance ( visibility , Visibility ) : raise ValueError ( 'visibility has to be an instance of the Visibility class.' ) def _prepare_sightings_payload ( self , atoms , hashkeys , start_timestamp , end_timestamp , sighting_type : SightingType , visibility , count , threat_types = None , tags = None , description = None ) : \"\"\" Internal function to prepare a list of Atoms for sighting submission to the format the API expects. \"\"\" self . _check_sightings_payload_parameters ( atoms , hashkeys , sighting_type , visibility , count , threat_types ) payload = {} # atoms and hashkeys can both be None if atoms : for atom in atoms : if type ( atom ) == Atom or not isinstance ( atom , Atom ) : raise TypeError ( \"atoms needs to be a list of Atom subclasses.\" ) atom_dict = atom . generate_atom_json ( for_sightings = True ) if not payload : payload = atom_dict else : payload = { key : payload . get ( key , [] ) + atom_dict . get ( key , [] ) for key in set ( list ( payload . keys ()) + list ( atom_dict . keys ()) ) } if hashkeys : payload [ 'hashkeys' ] = hashkeys payload [ 'start_timestamp' ] = start_timestamp . strftime ( \"%Y-%m-%dT%H:%M:%SZ\" ) payload [ 'end_timestamp' ] = end_timestamp . strftime ( \"%Y-%m-%dT%H:%M:%SZ\" ) payload [ 'visibility' ] = visibility . value payload [ 'type' ] = sighting_type . value payload [ 'count' ] = count if sighting_type in ( SightingType . POSITIVE , SightingType . NEGATIVE ) : payload [ 'threat_types' ] = [ threat_type.value for threat_type in threat_types ] if tags : payload [ 'tags' ] = tags if description : payload [ 'description' ] = description return payload","title":"Sightings"},{"location":"reference/datalake/endpoints/sightings/#ancestors-in-mro","text":"datalake.endpoints.endpoint.Endpoint","title":"Ancestors (in MRO)"},{"location":"reference/datalake/endpoints/sightings/#class-variables","text":"OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY","title":"Class variables"},{"location":"reference/datalake/endpoints/sightings/#methods","text":"","title":"Methods"},{"location":"reference/datalake/endpoints/sightings/#datalake_requests","text":"def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response","title":"datalake_requests"},{"location":"reference/datalake/endpoints/sightings/#submit_sighting","text":"def submit_sighting ( self , start_timestamp : datetime . datetime , end_timestamp : datetime . datetime , sighting_type : datalake . common . atom . SightingType , visibility : datalake . common . atom . Visibility , count : int , threat_types : List [ datalake . common . atom . ThreatType ] = None , tags : List [ str ] = None , description : str = None , atoms : List [ datalake . common . atom_type . Atom ] = None , hashkeys : List [ str ] = None ) Submit a list of sightings. Either threat hashkeys or list of atom objects is required. Possible sightings type: POSITIVE, NEGATIVE, NEUTRAL. For POSITIVE and NEGATIVE sightings \"threat_types\" field is required. End date timestamp should always be in the past. View Source def submit_sighting ( self , start_timestamp : datetime , end_timestamp : datetime , sighting_type : SightingType , visibility : Visibility , count : int , threat_types : List [ ThreatType ] = None , tags : List [ str ] = None , description : str = None , atoms : List [ Atom ] = None , hashkeys : List [ str ] = None ) : \"\"\" Submit a list of sightings. Either threat hashkeys or list of atom objects is required. Possible sightings type: POSITIVE, NEGATIVE, NEUTRAL. For POSITIVE and NEGATIVE sightings \" threat_types \" field is required. End date timestamp should always be in the past. \"\"\" payload = self . _prepare_sightings_payload ( atoms , hashkeys , start_timestamp , end_timestamp , sighting_type , visibility , count , threat_types , tags , description ) url = self . _build_url_for_endpoint ( 'submit-sightings' ) res = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ). json () return res","title":"submit_sighting"},{"location":"reference/datalake/endpoints/tags/","text":"Module datalake.endpoints.tags None None View Source from datalake.endpoints.endpoint import Endpoint from datalake.common.ouput import parse_response class Tags ( Endpoint ): def add_to_threat ( self , hashkey : str , tags : list , public = False ): \"\"\" Adds tag(s) to a threat. \"\"\" if type ( tags ) is not list or not tags : raise ValueError ( \"Tags has to be a list of string\" ) if all ( type ( tag ) is not str or not tag for tag in tags ): raise ValueError ( \"Tags has to be a list of string\" ) visibility = 'public' if public else 'organization' tags_payload = [] for tag in tags : tags_payload . append ( { 'name' : tag , 'visibility' : visibility , } ) payload = { 'tags' : tags_payload , } url = self . _build_url_for_endpoint ( 'tag' ) . format ( hashkey = hashkey ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response ) Classes Tags class Tags ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class Tags ( Endpoint ): def add_to_threat ( self , hashkey : str , tags : list , public = False ): \"\"\" Adds tag(s) to a threat. \"\"\" if type ( tags ) is not list or not tags : raise ValueError ( \"Tags has to be a list of string\" ) if all ( type ( tag ) is not str or not tag for tag in tags ): raise ValueError ( \"Tags has to be a list of string\" ) visibility = 'public' if public else 'organization' tags_payload = [] for tag in tags : tags_payload . append ( { 'name' : tag , 'visibility' : visibility , } ) payload = { 'tags' : tags_payload , } url = self . _build_url_for_endpoint ( 'tag' ) . format ( hashkey = hashkey ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response ) Ancestors (in MRO) datalake.endpoints.endpoint.Endpoint Class variables OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY Methods add_to_threat def add_to_threat ( self , hashkey : str , tags : list , public = False ) Adds tag(s) to a threat. View Source def add_to_threat ( self , hashkey : str , tags : list , public = False ): \"\"\" Adds tag(s) to a threat. \"\"\" if type ( tags ) is not list or not tags : raise ValueError ( \"Tags has to be a list of string\" ) if all ( type ( tag ) is not str or not tag for tag in tags ): raise ValueError ( \"Tags has to be a list of string\" ) visibility = 'public' if public else 'organization' tags_payload = [] for tag in tags : tags_payload . append ( { 'name' : tag , 'visibility' : visibility , } ) payload = { 'tags' : tags_payload , } url = self . _build_url_for_endpoint ( 'tag' ) . format ( hashkey = hashkey ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response ) datalake_requests def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response","title":"Tags"},{"location":"reference/datalake/endpoints/tags/#module-datalakeendpointstags","text":"None None View Source from datalake.endpoints.endpoint import Endpoint from datalake.common.ouput import parse_response class Tags ( Endpoint ): def add_to_threat ( self , hashkey : str , tags : list , public = False ): \"\"\" Adds tag(s) to a threat. \"\"\" if type ( tags ) is not list or not tags : raise ValueError ( \"Tags has to be a list of string\" ) if all ( type ( tag ) is not str or not tag for tag in tags ): raise ValueError ( \"Tags has to be a list of string\" ) visibility = 'public' if public else 'organization' tags_payload = [] for tag in tags : tags_payload . append ( { 'name' : tag , 'visibility' : visibility , } ) payload = { 'tags' : tags_payload , } url = self . _build_url_for_endpoint ( 'tag' ) . format ( hashkey = hashkey ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response )","title":"Module datalake.endpoints.tags"},{"location":"reference/datalake/endpoints/tags/#classes","text":"","title":"Classes"},{"location":"reference/datalake/endpoints/tags/#tags","text":"class Tags ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class Tags ( Endpoint ): def add_to_threat ( self , hashkey : str , tags : list , public = False ): \"\"\" Adds tag(s) to a threat. \"\"\" if type ( tags ) is not list or not tags : raise ValueError ( \"Tags has to be a list of string\" ) if all ( type ( tag ) is not str or not tag for tag in tags ): raise ValueError ( \"Tags has to be a list of string\" ) visibility = 'public' if public else 'organization' tags_payload = [] for tag in tags : tags_payload . append ( { 'name' : tag , 'visibility' : visibility , } ) payload = { 'tags' : tags_payload , } url = self . _build_url_for_endpoint ( 'tag' ) . format ( hashkey = hashkey ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response )","title":"Tags"},{"location":"reference/datalake/endpoints/tags/#ancestors-in-mro","text":"datalake.endpoints.endpoint.Endpoint","title":"Ancestors (in MRO)"},{"location":"reference/datalake/endpoints/tags/#class-variables","text":"OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY","title":"Class variables"},{"location":"reference/datalake/endpoints/tags/#methods","text":"","title":"Methods"},{"location":"reference/datalake/endpoints/tags/#add_to_threat","text":"def add_to_threat ( self , hashkey : str , tags : list , public = False ) Adds tag(s) to a threat. View Source def add_to_threat ( self , hashkey : str , tags : list , public = False ): \"\"\" Adds tag(s) to a threat. \"\"\" if type ( tags ) is not list or not tags : raise ValueError ( \"Tags has to be a list of string\" ) if all ( type ( tag ) is not str or not tag for tag in tags ): raise ValueError ( \"Tags has to be a list of string\" ) visibility = 'public' if public else 'organization' tags_payload = [] for tag in tags : tags_payload . append ( { 'name' : tag , 'visibility' : visibility , } ) payload = { 'tags' : tags_payload , } url = self . _build_url_for_endpoint ( 'tag' ) . format ( hashkey = hashkey ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response )","title":"add_to_threat"},{"location":"reference/datalake/endpoints/tags/#datalake_requests","text":"def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response","title":"datalake_requests"},{"location":"reference/datalake/endpoints/threats/","text":"Module datalake.endpoints.threats None None View Source import os from typing import List , Union , Dict from time import time , sleep from requests.sessions import PreparedRequest from datalake import AtomType , ThreatType , OverrideType , Atom from datalake.common.atom import ScoreMap from datalake.common.ouput import Output , output_supported , parse_response from datalake.common.utils import split_list , aggregate_csv_or_json_api_response from datalake.endpoints.endpoint import Endpoint class Threats ( Endpoint ): _NB_ATOMS_PER_BULK_LOOKUP = 100 OCD_DTL_MAX_BULK_THREATS_TIME = int ( os . getenv ( 'OCD_DTL_MAX_BULK_THREATS_TIME' , 600 )) OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT = int ( os . getenv ( 'OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT' , 10 )) OCD_DTL_MAX_BACK_OFF_TIME = float ( os . getenv ( 'OCD_DTL_MAX_BACK_OFF_TIME' , 30 )) OCD_DTL_MAX_EDIT_SCORE_HASHKEYS = int ( os . getenv ( 'OCD_DTL_MAX_EDIT_SCORE_HASHKEYS' , 100 )) def _bulk_lookup_batch ( self , atom_values : list , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON , return_search_hashkey = False , ) -> dict : \"\"\"Bulk lookup done on maximum _NB_ATOMS_PER_BULK_LOOKUP atoms\"\"\" typed_atoms = {} if not atom_type : atoms_values_extractor_response = self . atom_values_extract ( atom_values ) if atoms_values_extractor_response [ 'found' ] > 0 : typed_atoms = atoms_values_extractor_response [ 'results' ] else : raise ValueError ( 'none of your atoms could be typed' ) elif not isinstance ( atom_type , AtomType ): raise ValueError ( f ' { atom_type } atom_type could not be treated' ) else : typed_atoms [ atom_type . value ] = atom_values body : dict = typed_atoms body [ 'hashkey_only' ] = hashkey_only body [ 'return_search_hashkey' ] = return_search_hashkey url = self . _build_url_for_endpoint ( 'threats-bulk-lookup' ) response = self . datalake_requests ( url , 'post' , self . _post_headers ( output = output ), body ) return parse_response ( response ) @output_supported ({ Output . JSON , Output . CSV }) def bulk_lookup ( self , atom_values : list , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON , return_search_hashkey = False , ) -> Union [ dict , str ]: \"\"\" Look up multiple threats at once in the API, returning their ids and if they are present in Datalake. Compared to the lookup endpoint, it allow to lookup big batch of values faster as fewer API calls are made. However, fewer outputs types are supported as of now. \"\"\" aggregated_response = [] if output is Output . CSV else {} search_hashkey_list = [] for atom_values_batch in split_list ( atom_values , self . _NB_ATOMS_PER_BULK_LOOKUP ): batch_result = self . _bulk_lookup_batch ( atom_values_batch , atom_type , hashkey_only , output , return_search_hashkey ) if 'search_hashkey' in batch_result : search_hashkey_list . append ( batch_result . pop ( 'search_hashkey' )) aggregated_response = aggregate_csv_or_json_api_response ( aggregated_response , batch_result , ) if search_hashkey_list : aggregated_response [ 'search_hashkey' ] = search_hashkey_list if output is Output . CSV : aggregated_response = ' \\n ' . join ( aggregated_response ) # a string is expected for CSV output return aggregated_response @output_supported ({ Output . JSON , Output . CSV , Output . MISP , Output . STIX }) def lookup ( self , atom_value , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON ) -> dict : \"\"\" Look up a threat in the API, returning its id (called threat's hashkey) and if it is present in Datalake. The hashkey is also based on the threat type. If atom_type is not specified, another query to the API will be made to determine it. With hashkey_only = False, the full threat details are returned in the requested format. \"\"\" atom_type_str = None if not atom_type : threats = [ atom_value ] atoms_values_extractor_response = self . atom_values_extract ( threats ) if atoms_values_extractor_response [ 'found' ] > 0 : atom_type_str = list ( atoms_values_extractor_response [ 'results' ] . keys ())[ 0 ] else : raise ValueError ( 'atom could not be typed' ) elif not isinstance ( atom_type , AtomType ): raise ValueError ( f ' { atom_type } atom_type could not be treated' ) else : # atom_type is a valid enum passed by the user atom_type_str = atom_type . value url = self . _build_url_for_endpoint ( 'lookup' ) params = { 'atom_value' : atom_value , 'atom_type' : atom_type_str , 'hashkey_only' : hashkey_only } req = PreparedRequest () req . prepare_url ( url , params ) response = self . datalake_requests ( req . url , 'get' , self . _get_headers ( output = output )) return parse_response ( response ) def atom_values_extract ( self , untyped_atoms : List [ str ], treat_hashes_like = AtomType . FILE ) -> dict : \"\"\" Determine the types of atoms passed. values that are believed to be hashes will be returned as <treat_hashes_like> \"\"\" url = self . _build_url_for_endpoint ( 'atom-values-extract' ) payload = { 'content' : ' ' . join ( untyped_atoms ), 'treat_hashes_like' : treat_hashes_like . value } return self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) . json () def edit_score_by_hashkeys ( self , hashkeys , scores_list , override_type : OverrideType = OverrideType . TEMPORARY ): \"\"\" Edit the score of a list of threats using the API. Default is 100. This function will receive a list of hashkey to edit, a list of dictionaries defining the score the set and an override type. Can only process a limited number of hashkeys at one time. Can be modified with the environment variable OCD_DTL_MAX_EDIT_SCORE_HASHKEYS. \"\"\" if type ( hashkeys ) is not list or not hashkeys : raise ValueError ( 'Hashkeys has to be a list of string' ) if len ( hashkeys ) > self . OCD_DTL_MAX_EDIT_SCORE_HASHKEYS : raise ValueError ( f 'Can \\' t process more than { self . OCD_DTL_MAX_EDIT_SCORE_HASHKEYS } hashkeys at one time.' ) if any ( type ( hashkey ) is not str or not hashkey for hashkey in hashkeys ): raise ValueError ( 'Hashkeys has to be a list of string' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'hashkeys' : hashkeys , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response ) def edit_score_by_query_body_hash ( self , query_body_hash : str , scores_list : List [ ScoreMap ], override_type : OverrideType = OverrideType . TEMPORARY ): \"\"\" Edit the score of a list of threats using the API. This function will receive a query body hash, a list of dictonaries defining the score the set and an override type. \"\"\" if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'query_body_hash' : query_body_hash , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response ) @staticmethod def _build_scores ( scores_list : List [ ScoreMap ]): scores_body = [] for score_dict in scores_list : if not isinstance ( score_dict [ 'threat_type' ], ThreatType ): raise ValueError ( 'Invalid threat_type input' ) if score_dict [ 'score' ] > 100 or score_dict [ 'score' ] < 0 : raise ValueError ( 'Invalid score input, min: 0, max: 100' ) scores_body . append ({ 'score' : { 'risk' : score_dict [ 'score' ] }, 'threat_type' : score_dict [ 'threat_type' ] . value }) return scores_body @staticmethod def _build_whitelist_scores (): scores = [] for threat in ThreatType : scores . append ({ 'score' : { 'risk' : 0 }, 'threat_type' : threat . value }) return scores def add_threats ( self , atom_list : List [ str ], atom_type : AtomType , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , external_analysis_link : List = None , ): \"\"\" Add a list of threats to datalake using the API. The type of atom provided in the list of threats to add need to be the same, for example a list of IPs. \"\"\" self . check_add_threats_params ( atom_list , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field, default to an empty list payload = { 'override_type' : override_type . value , 'public' : public } if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) return self . _bulk_add_threat ( atom_list , atom_type , payload , tags , scores , external_analysis_link ) @staticmethod def check_add_threats_params ( atom_list , override_type , threat_types , whitelist ): if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) if any ( len ( atom ) < 1 for atom in atom_list ): raise ValueError ( 'Empty atom in atom_list' ) def _bulk_add_threat ( self , atom_list : List , atom_type : AtomType , payload : Dict , tags : List , scores : List [ Dict ], external_analysis_link : List = None ): url = self . _build_url_for_endpoint ( 'threats-manual-bulk' ) payload [ 'atom_type' ] = atom_type . value payload [ 'scores' ] = scores payload [ 'tags' ] = tags if external_analysis_link : payload [ 'threat_data' ] = { 'content' : { f ' { atom_type . value } _content' : { 'external_analysis_link' : external_analysis_link } } } hashkey_created = self . queue_bulk_threats ( atom_list , payload , url ) return hashkey_created def queue_bulk_threats ( self , atom_list , payload , url ): bulk_response = [] bulk_in_flight = [] # bulk task uuid unchecked failed_batch = [] for batch in split_list ( atom_list , 100 ): if len ( bulk_in_flight ) >= self . OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT : bulk_threat_task_uuid = bulk_in_flight . pop ( 0 ) bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) payload [ 'atom_values' ] = ' \\n ' . join ( batch ) # Raw csv expected response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) response = parse_response ( response ) task_uid = response . get ( 'task_uuid' ) if task_uid : bulk_in_flight . append ( response [ 'task_uuid' ]) else : failed_batch . append ( batch ) if failed_batch : bulk_response . append ({ 'success' : { 'created_hashkeys' : [], 'created_atom_values' : [] }, 'failed' : { 'failed_hashkeys' : [], 'failed_atom_values' : failed_batch } }) # Finish to check the other bulk tasks for bulk_threat_task_uuid in bulk_in_flight : bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) return bulk_response def check_bulk_threats_added ( self , bulk_threat_task_uuid ) -> dict : \"\"\"Check if the bulk manual threat submission completed successfully and if so return the hashkeys created\"\"\" success = [] failed = [] url = self . _build_url_for_endpoint ( 'retrieve-threats-manual-bulk' ) try : response = self . _handle_bulk_task ( bulk_threat_task_uuid , url , timeout = self . OCD_DTL_MAX_BULK_THREATS_TIME , ) except TimeoutError : response = {} hashkeys = response . get ( 'hashkeys' ) atom_values = response . get ( 'atom_values' ) if hashkeys and response . get ( 'state' , 'CANCELLED' ) == 'DONE' : hashkey_created = [] atom_val_created = [] for hashkey in hashkeys : hashkey_created . append ( hashkey ) for atom_value in atom_values : atom_val_created . append ( atom_value ) success . append ({ 'created_hashkeys' : hashkey_created , 'created_atom_values' : atom_val_created }) else : # if the state is not DONE we consider the batch a failure # default values in case the json is missing some fields hashkeys = hashkeys or [ '<missing value>' ] atom_values = atom_values or [ '<missing value>' ] failed . append ({ 'failed_hashkeys' : hashkeys , 'failed_atom_values' : atom_values }) response_dict = { 'success' : success , 'failed' : failed } return response_dict def _handle_bulk_task ( self , task_uuid , retrieve_bulk_result_url , timeout ): retrieve_bulk_result_url = retrieve_bulk_result_url . format ( task_uuid = task_uuid ) start_time = time () back_off_time = 1 json_response = None while not json_response : response = self . datalake_requests ( retrieve_bulk_result_url , 'get' , self . _get_headers ()) if response . status_code == 200 : potential_json_response = response . json () if not potential_json_response [ 'state' ] in ( 'DONE' , 'CANCELLED' ): continue json_response = response . json () elif time () - start_time + back_off_time < timeout : sleep ( back_off_time ) back_off_time = min ( back_off_time * 2 , self . OCD_DTL_MAX_BACK_OFF_TIME ) else : raise TimeoutError ( f 'No bulk result after waiting { timeout / 60 : .0f } mins \\n ' f 'task_uuid: \" { task_uuid } \"' ) return json_response @staticmethod def check_add_threat_params ( atom , override_type , threat_types , whitelist ): if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) if type ( atom ) == Atom or not isinstance ( atom , Atom ): raise TypeError ( \"atom needs to be an Atom subclasses.\" ) def add_threat ( self , atom : Atom , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , ): \"\"\" Add a single threat to datalake using the API. This method is slower than add_threats for submitting a large number of threats but allows to provide greater details \"\"\" self . check_add_threat_params ( atom , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field, default to an empty list if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) payload = { 'override_type' : override_type . value , 'public' : public , 'threat_data' : { 'scores' : scores , 'tags' : tags , 'content' : atom . generate_atom_json (), } } url = self . _build_url_for_endpoint ( 'threats-manual' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response ) Classes Threats class Threats ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class Threats ( Endpoint ): _NB_ATOMS_PER_BULK_LOOKUP = 100 OCD_DTL_MAX_BULK_THREATS_TIME = int ( os . getenv ( 'OCD_DTL_MAX_BULK_THREATS_TIME' , 600 )) OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT = int ( os . getenv ( 'OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT' , 10 )) OCD_DTL_MAX_BACK_OFF_TIME = float ( os . getenv ( 'OCD_DTL_MAX_BACK_OFF_TIME' , 30 )) OCD_DTL_MAX_EDIT_SCORE_HASHKEYS = int ( os . getenv ( 'OCD_DTL_MAX_EDIT_SCORE_HASHKEYS' , 100 )) def _bulk_lookup_batch ( self , atom_values : list , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON , return_search_hashkey = False , ) -> dict : \"\"\"Bulk lookup done on maximum _NB_ATOMS_PER_BULK_LOOKUP atoms\"\"\" typed_atoms = {} if not atom_type : atoms_values_extractor_response = self . atom_values_extract ( atom_values ) if atoms_values_extractor_response [ 'found' ] > 0 : typed_atoms = atoms_values_extractor_response [ 'results' ] else : raise ValueError ( 'none of your atoms could be typed' ) elif not isinstance ( atom_type , AtomType ): raise ValueError ( f '{atom_type} atom_type could not be treated' ) else : typed_atoms [ atom_type . value ] = atom_values body : dict = typed_atoms body [ 'hashkey_only' ] = hashkey_only body [ 'return_search_hashkey' ] = return_search_hashkey url = self . _build_url_for_endpoint ( 'threats-bulk-lookup' ) response = self . datalake_requests ( url , 'post' , self . _post_headers ( output = output ), body ) return parse_response ( response ) @ output_supported ({ Output . JSON , Output . CSV }) def bulk_lookup ( self , atom_values : list , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON , return_search_hashkey = False , ) -> Union [ dict , str ]: \"\"\" Look up multiple threats at once in the API, returning their ids and if they are present in Datalake. Compared to the lookup endpoint, it allow to lookup big batch of values faster as fewer API calls are made. However, fewer outputs types are supported as of now. \"\"\" aggregated_response = [] if output is Output . CSV else {} search_hashkey_list = [] for atom_values_batch in split_list ( atom_values , self . _NB_ATOMS_PER_BULK_LOOKUP ): batch_result = self . _bulk_lookup_batch ( atom_values_batch , atom_type , hashkey_only , output , return_search_hashkey ) if 'search_hashkey' in batch_result : search_hashkey_list . append ( batch_result . pop ( 'search_hashkey' )) aggregated_response = aggregate_csv_or_json_api_response ( aggregated_response , batch_result , ) if search_hashkey_list : aggregated_response [ 'search_hashkey' ] = search_hashkey_list if output is Output . CSV : aggregated_response = ' \\n ' . join ( aggregated_response ) # a string is expected for CSV output return aggregated_response @ output_supported ({ Output . JSON , Output . CSV , Output . MISP , Output . STIX }) def lookup ( self , atom_value , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON ) -> dict : \"\"\" Look up a threat in the API, returning its id (called threat's hashkey) and if it is present in Datalake. The hashkey is also based on the threat type. If atom_type is not specified, another query to the API will be made to determine it. With hashkey_only = False, the full threat details are returned in the requested format. \"\"\" atom_type_str = None if not atom_type : threats = [ atom_value ] atoms_values_extractor_response = self . atom_values_extract ( threats ) if atoms_values_extractor_response [ 'found' ] > 0 : atom_type_str = list ( atoms_values_extractor_response [ 'results' ] . keys ())[ 0 ] else : raise ValueError ( 'atom could not be typed' ) elif not isinstance ( atom_type , AtomType ): raise ValueError ( f '{atom_type} atom_type could not be treated' ) else : # atom_type is a valid enum passed by the user atom_type_str = atom_type . value url = self . _build_url_for_endpoint ( 'lookup' ) params = { 'atom_value' : atom_value , 'atom_type' : atom_type_str , 'hashkey_only' : hashkey_only } req = PreparedRequest () req . prepare_url ( url , params ) response = self . datalake_requests ( req . url , 'get' , self . _get_headers ( output = output )) return parse_response ( response ) def atom_values_extract ( self , untyped_atoms : List [ str ], treat_hashes_like = AtomType . FILE ) -> dict : \"\"\" Determine the types of atoms passed. values that are believed to be hashes will be returned as <treat_hashes_like> \"\"\" url = self . _build_url_for_endpoint ( 'atom-values-extract' ) payload = { 'content' : ' ' . join ( untyped_atoms ), 'treat_hashes_like' : treat_hashes_like . value } return self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) . json () def edit_score_by_hashkeys ( self , hashkeys , scores_list , override_type : OverrideType = OverrideType . TEMPORARY ): \"\"\" Edit the score of a list of threats using the API. Default is 100. This function will receive a list of hashkey to edit, a list of dictionaries defining the score the set and an override type. Can only process a limited number of hashkeys at one time. Can be modified with the environment variable OCD_DTL_MAX_EDIT_SCORE_HASHKEYS. \"\"\" if type ( hashkeys ) is not list or not hashkeys : raise ValueError ( 'Hashkeys has to be a list of string' ) if len ( hashkeys ) > self . OCD_DTL_MAX_EDIT_SCORE_HASHKEYS : raise ValueError ( f 'Can \\' t process more than {self.OCD_DTL_MAX_EDIT_SCORE_HASHKEYS} hashkeys at one time.' ) if any ( type ( hashkey ) is not str or not hashkey for hashkey in hashkeys ): raise ValueError ( 'Hashkeys has to be a list of string' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'hashkeys' : hashkeys , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response ) def edit_score_by_query_body_hash ( self , query_body_hash : str , scores_list : List [ ScoreMap ], override_type : OverrideType = OverrideType . TEMPORARY ): \"\"\" Edit the score of a list of threats using the API. This function will receive a query body hash, a list of dictonaries defining the score the set and an override type. \"\"\" if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'query_body_hash' : query_body_hash , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response ) @ staticmethod def _build_scores ( scores_list : List [ ScoreMap ]): scores_body = [] for score_dict in scores_list : if not isinstance ( score_dict [ 'threat_type' ], ThreatType ): raise ValueError ( 'Invalid threat_type input' ) if score_dict [ 'score' ] > 100 or score_dict [ 'score' ] < 0 : raise ValueError ( 'Invalid score input, min: 0, max: 100' ) scores_body . append ({ 'score' : { 'risk' : score_dict [ 'score' ] }, 'threat_type' : score_dict [ 'threat_type' ] . value }) return scores_body @ staticmethod def _build_whitelist_scores (): scores = [] for threat in ThreatType : scores . append ({ 'score' : { 'risk' : 0 }, 'threat_type' : threat . value }) return scores def add_threats ( self , atom_list : List [ str ], atom_type : AtomType , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , external_analysis_link : List = None , ): \"\"\" Add a list of threats to datalake using the API. The type of atom provided in the list of threats to add need to be the same, for example a list of IPs. \"\"\" self . check_add_threats_params ( atom_list , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field, default to an empty list payload = { 'override_type' : override_type . value , 'public' : public } if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) return self . _bulk_add_threat ( atom_list , atom_type , payload , tags , scores , external_analysis_link ) @ staticmethod def check_add_threats_params ( atom_list , override_type , threat_types , whitelist ): if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) if any ( len ( atom ) < 1 for atom in atom_list ): raise ValueError ( 'Empty atom in atom_list' ) def _bulk_add_threat ( self , atom_list : List , atom_type : AtomType , payload : Dict , tags : List , scores : List [ Dict ], external_analysis_link : List = None ): url = self . _build_url_for_endpoint ( 'threats-manual-bulk' ) payload [ 'atom_type' ] = atom_type . value payload [ 'scores' ] = scores payload [ 'tags' ] = tags if external_analysis_link : payload [ 'threat_data' ] = { 'content' : { f '{atom_type.value}_content' : { 'external_analysis_link' : external_analysis_link } } } hashkey_created = self . queue_bulk_threats ( atom_list , payload , url ) return hashkey_created def queue_bulk_threats ( self , atom_list , payload , url ): bulk_response = [] bulk_in_flight = [] # bulk task uuid unchecked failed_batch = [] for batch in split_list ( atom_list , 100 ): if len ( bulk_in_flight ) >= self . OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT : bulk_threat_task_uuid = bulk_in_flight . pop ( 0 ) bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) payload [ 'atom_values' ] = ' \\n ' . join ( batch ) # Raw csv expected response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) response = parse_response ( response ) task_uid = response . get ( 'task_uuid' ) if task_uid : bulk_in_flight . append ( response [ 'task_uuid' ]) else : failed_batch . append ( batch ) if failed_batch : bulk_response . append ({ 'success' : { 'created_hashkeys' : [], 'created_atom_values' : [] }, 'failed' : { 'failed_hashkeys' : [], 'failed_atom_values' : failed_batch } }) # Finish to check the other bulk tasks for bulk_threat_task_uuid in bulk_in_flight : bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) return bulk_response def check_bulk_threats_added ( self , bulk_threat_task_uuid ) -> dict : \"\"\"Check if the bulk manual threat submission completed successfully and if so return the hashkeys created\"\"\" success = [] failed = [] url = self . _build_url_for_endpoint ( 'retrieve-threats-manual-bulk' ) try : response = self . _handle_bulk_task ( bulk_threat_task_uuid , url , timeout = self . OCD_DTL_MAX_BULK_THREATS_TIME , ) except TimeoutError : response = {} hashkeys = response . get ( 'hashkeys' ) atom_values = response . get ( 'atom_values' ) if hashkeys and response . get ( 'state' , 'CANCELLED' ) == 'DONE' : hashkey_created = [] atom_val_created = [] for hashkey in hashkeys : hashkey_created . append ( hashkey ) for atom_value in atom_values : atom_val_created . append ( atom_value ) success . append ({ 'created_hashkeys' : hashkey_created , 'created_atom_values' : atom_val_created }) else : # if the state is not DONE we consider the batch a failure # default values in case the json is missing some fields hashkeys = hashkeys or [ '<missing value>' ] atom_values = atom_values or [ '<missing value>' ] failed . append ({ 'failed_hashkeys' : hashkeys , 'failed_atom_values' : atom_values }) response_dict = { 'success' : success , 'failed' : failed } return response_dict def _handle_bulk_task ( self , task_uuid , retrieve_bulk_result_url , timeout ): retrieve_bulk_result_url = retrieve_bulk_result_url . format ( task_uuid = task_uuid ) start_time = time () back_off_time = 1 json_response = None while not json_response : response = self . datalake_requests ( retrieve_bulk_result_url , 'get' , self . _get_headers ()) if response . status_code == 200 : potential_json_response = response . json () if not potential_json_response [ 'state' ] in ( 'DONE' , 'CANCELLED' ): continue json_response = response . json () elif time () - start_time + back_off_time < timeout : sleep ( back_off_time ) back_off_time = min ( back_off_time * 2 , self . OCD_DTL_MAX_BACK_OFF_TIME ) else : raise TimeoutError ( f 'No bulk result after waiting {timeout / 60:.0f} mins \\n ' f 'task_uuid: \"{task_uuid}\"' ) return json_response @ staticmethod def check_add_threat_params ( atom , override_type , threat_types , whitelist ): if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) if type ( atom ) == Atom or not isinstance ( atom , Atom ): raise TypeError ( \"atom needs to be an Atom subclasses.\" ) def add_threat ( self , atom : Atom , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , ): \"\"\" Add a single threat to datalake using the API. This method is slower than add_threats for submitting a large number of threats but allows to provide greater details \"\"\" self . check_add_threat_params ( atom , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field, default to an empty list if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) payload = { 'override_type' : override_type . value , 'public' : public , 'threat_data' : { 'scores' : scores , 'tags' : tags , 'content' : atom . generate_atom_json (), } } url = self . _build_url_for_endpoint ( 'threats-manual' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response ) Ancestors (in MRO) datalake.endpoints.endpoint.Endpoint Class variables OCD_DTL_MAX_BACK_OFF_TIME OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT OCD_DTL_MAX_BULK_THREATS_TIME OCD_DTL_MAX_EDIT_SCORE_HASHKEYS OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY Static methods check_add_threat_params def check_add_threat_params ( atom , override_type , threat_types , whitelist ) View Source @staticmethod def check_add_threat_params ( atom , override_type , threat_types , whitelist ) : if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ) : raise ValueError ( 'Invalid OverrideType input' ) if type ( atom ) == Atom or not isinstance ( atom , Atom ) : raise TypeError ( \"atom needs to be an Atom subclasses.\" ) check_add_threats_params def check_add_threats_params ( atom_list , override_type , threat_types , whitelist ) View Source @staticmethod def check_add_threats_params ( atom_list , override_type , threat_types , whitelist ) : if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ) : raise ValueError ( 'Invalid OverrideType input' ) if any ( len ( atom ) < 1 for atom in atom_list ) : raise ValueError ( 'Empty atom in atom_list' ) Methods add_threat def add_threat ( self , atom : datalake . common . atom_type . Atom , threat_types : List [ Dict [ str , Union [ int , datalake . common . atom . ThreatType ]]] = None , override_type : datalake . common . atom . OverrideType = < OverrideType . TEMPORARY : 'temporary' > , whitelist : bool = False , public : bool = True , tags : List = None ) Add a single threat to datalake using the API. This method is slower than add_threats for submitting a large number of threats but allows to provide greater details View Source def add_threat ( self , atom : Atom , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , ) : \"\"\" Add a single threat to datalake using the API. This method is slower than add_threats for submitting a large number of threats but allows to provide greater details \"\"\" self . check_add_threat_params ( atom , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field , default to an empty list if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) payload = { 'override_type' : override_type . value , 'public' : public , 'threat_data' : { 'scores' : scores , 'tags' : tags , 'content' : atom . generate_atom_json (), } } url = self . _build_url_for_endpoint ( 'threats-manual' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response ) add_threats def add_threats ( self , atom_list : List [ str ], atom_type : datalake . common . atom . AtomType , threat_types : List [ Dict [ str , Union [ int , datalake . common . atom . ThreatType ]]] = None , override_type : datalake . common . atom . OverrideType = < OverrideType . TEMPORARY : 'temporary' > , whitelist : bool = False , public : bool = True , tags : List = None , external_analysis_link : List = None ) Add a list of threats to datalake using the API. The type of atom provided in the list of threats to add need to be the same, for example a list of IPs. View Source def add_threats ( self , atom_list : List [ str ] , atom_type : AtomType , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , external_analysis_link : List = None , ) : \"\"\" Add a list of threats to datalake using the API. The type of atom provided in the list of threats to add need to be the same, for example a list of IPs. \"\"\" self . check_add_threats_params ( atom_list , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field , default to an empty list payload = { 'override_type' : override_type . value , 'public' : public } if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) return self . _bulk_add_threat ( atom_list , atom_type , payload , tags , scores , external_analysis_link ) atom_values_extract def atom_values_extract ( self , untyped_atoms : List [ str ], treat_hashes_like =< AtomType . FILE : 'file' > ) -> dict Determine the types of atoms passed. values that are believed to be hashes will be returned as View Source def atom_values_extract ( self , untyped_atoms : List [ str ] , treat_hashes_like = AtomType . FILE ) -> dict : \"\"\" Determine the types of atoms passed. values that are believed to be hashes will be returned as <treat_hashes_like> \"\"\" url = self . _build_url_for_endpoint ( 'atom-values-extract' ) payload = { 'content' : ' ' . join ( untyped_atoms ), 'treat_hashes_like' : treat_hashes_like . value } return self . datalake_requests ( url , 'post' , self . _post_headers (), payload ). json () bulk_lookup def bulk_lookup ( * args , ** kwargs ) View Source def wrapper ( * args , ** kwargs ) : if kwargs . get ( ' output ' ) and kwargs [ ' output ' ] not in outputs : raise ValueError ( f ' {kwargs[\"output\"]} output type is not supported. ' f ' Outputs supported are: {display_outputs(outputs)} ' ) return function ( * args , ** kwargs ) check_bulk_threats_added def check_bulk_threats_added ( self , bulk_threat_task_uuid ) -> dict Check if the bulk manual threat submission completed successfully and if so return the hashkeys created View Source def check_bulk_threats_added ( self , bulk_threat_task_uuid ) -> dict : \"\"\"Check if the bulk manual threat submission completed successfully and if so return the hashkeys created\"\"\" success = [] failed = [] url = self . _build_url_for_endpoint ( 'retrieve-threats-manual-bulk' ) try : response = self . _handle_bulk_task ( bulk_threat_task_uuid , url , timeout = self . OCD_DTL_MAX_BULK_THREATS_TIME , ) except TimeoutError : response = {} hashkeys = response . get ( 'hashkeys' ) atom_values = response . get ( 'atom_values' ) if hashkeys and response . get ( 'state' , 'CANCELLED' ) == 'DONE' : hashkey_created = [] atom_val_created = [] for hashkey in hashkeys : hashkey_created . append ( hashkey ) for atom_value in atom_values : atom_val_created . append ( atom_value ) success . append ({ 'created_hashkeys' : hashkey_created , 'created_atom_values' : atom_val_created }) else : # if the state is not DONE we consider the batch a failure # default values in case the json is missing some fields hashkeys = hashkeys or [ '<missing value>' ] atom_values = atom_values or [ '<missing value>' ] failed . append ({ 'failed_hashkeys' : hashkeys , 'failed_atom_values' : atom_values }) response_dict = { 'success' : success , 'failed' : failed } return response_dict datalake_requests def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response edit_score_by_hashkeys def edit_score_by_hashkeys ( self , hashkeys , scores_list , override_type : datalake . common . atom . OverrideType = < OverrideType . TEMPORARY : 'temporary' > ) Edit the score of a list of threats using the API. Default is 100. This function will receive a list of hashkey to edit, a list of dictionaries defining the score the set and an override type. Can only process a limited number of hashkeys at one time. Can be modified with the environment variable OCD_DTL_MAX_EDIT_SCORE_HASHKEYS. View Source def edit_score_by_hashkeys ( self , hashkeys , scores_list , override_type : OverrideType = OverrideType . TEMPORARY ): \"\"\" Edit the score of a list of threats using the API. Default is 100. This function will receive a list of hashkey to edit, a list of dictionaries defining the score the set and an override type. Can only process a limited number of hashkeys at one time. Can be modified with the environment variable OCD_DTL_MAX_EDIT_SCORE_HASHKEYS. \"\"\" if type ( hashkeys ) is not list or not hashkeys : raise ValueError ( 'Hashkeys has to be a list of string' ) if len ( hashkeys ) > self . OCD_DTL_MAX_EDIT_SCORE_HASHKEYS : raise ValueError ( f 'Can \\' t process more than {self.OCD_DTL_MAX_EDIT_SCORE_HASHKEYS} hashkeys at one time.' ) if any ( type ( hashkey ) is not str or not hashkey for hashkey in hashkeys ): raise ValueError ( 'Hashkeys has to be a list of string' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'hashkeys' : hashkeys , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response ) edit_score_by_query_body_hash def edit_score_by_query_body_hash ( self , query_body_hash : str , scores_list : List [ Dict [ str , Union [ int , datalake . common . atom . ThreatType ]]], override_type : datalake . common . atom . OverrideType = < OverrideType . TEMPORARY : 'temporary' > ) Edit the score of a list of threats using the API. This function will receive a query body hash, a list of dictonaries defining the score the set and an override type. View Source def edit_score_by_query_body_hash ( self , query_body_hash : str , scores_list : List [ ScoreMap ] , override_type : OverrideType = OverrideType . TEMPORARY ) : \"\"\" Edit the score of a list of threats using the API. This function will receive a query body hash, a list of dictonaries defining the score the set and an override type. \"\"\" if not isinstance ( override_type , OverrideType ) : raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'query_body_hash' : query_body_hash , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response ) lookup def lookup ( * args , ** kwargs ) View Source def wrapper ( * args , ** kwargs ) : if kwargs . get ( ' output ' ) and kwargs [ ' output ' ] not in outputs : raise ValueError ( f ' {kwargs[\"output\"]} output type is not supported. ' f ' Outputs supported are: {display_outputs(outputs)} ' ) return function ( * args , ** kwargs ) queue_bulk_threats def queue_bulk_threats ( self , atom_list , payload , url ) View Source def queue_bulk_threats ( self , atom_list , payload , url ): bulk_response = [] bulk_in_flight = [] # bulk task uuid unchecked failed_batch = [] for batch in split_list ( atom_list , 100 ): if len ( bulk_in_flight ) >= self . OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT : bulk_threat_task_uuid = bulk_in_flight . pop ( 0 ) bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) payload [ 'atom_values' ] = ' \\n ' . join ( batch ) # Raw csv expected response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) response = parse_response ( response ) task_uid = response . get ( 'task_uuid' ) if task_uid : bulk_in_flight . append ( response [ 'task_uuid' ]) else : failed_batch . append ( batch ) if failed_batch : bulk_response . append ({ 'success' : { 'created_hashkeys' : [], 'created_atom_values' : [] }, 'failed' : { 'failed_hashkeys' : [], 'failed_atom_values' : failed_batch } }) # Finish to check the other bulk tasks for bulk_threat_task_uuid in bulk_in_flight : bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) return bulk_response","title":"Threats"},{"location":"reference/datalake/endpoints/threats/#module-datalakeendpointsthreats","text":"None None View Source import os from typing import List , Union , Dict from time import time , sleep from requests.sessions import PreparedRequest from datalake import AtomType , ThreatType , OverrideType , Atom from datalake.common.atom import ScoreMap from datalake.common.ouput import Output , output_supported , parse_response from datalake.common.utils import split_list , aggregate_csv_or_json_api_response from datalake.endpoints.endpoint import Endpoint class Threats ( Endpoint ): _NB_ATOMS_PER_BULK_LOOKUP = 100 OCD_DTL_MAX_BULK_THREATS_TIME = int ( os . getenv ( 'OCD_DTL_MAX_BULK_THREATS_TIME' , 600 )) OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT = int ( os . getenv ( 'OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT' , 10 )) OCD_DTL_MAX_BACK_OFF_TIME = float ( os . getenv ( 'OCD_DTL_MAX_BACK_OFF_TIME' , 30 )) OCD_DTL_MAX_EDIT_SCORE_HASHKEYS = int ( os . getenv ( 'OCD_DTL_MAX_EDIT_SCORE_HASHKEYS' , 100 )) def _bulk_lookup_batch ( self , atom_values : list , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON , return_search_hashkey = False , ) -> dict : \"\"\"Bulk lookup done on maximum _NB_ATOMS_PER_BULK_LOOKUP atoms\"\"\" typed_atoms = {} if not atom_type : atoms_values_extractor_response = self . atom_values_extract ( atom_values ) if atoms_values_extractor_response [ 'found' ] > 0 : typed_atoms = atoms_values_extractor_response [ 'results' ] else : raise ValueError ( 'none of your atoms could be typed' ) elif not isinstance ( atom_type , AtomType ): raise ValueError ( f ' { atom_type } atom_type could not be treated' ) else : typed_atoms [ atom_type . value ] = atom_values body : dict = typed_atoms body [ 'hashkey_only' ] = hashkey_only body [ 'return_search_hashkey' ] = return_search_hashkey url = self . _build_url_for_endpoint ( 'threats-bulk-lookup' ) response = self . datalake_requests ( url , 'post' , self . _post_headers ( output = output ), body ) return parse_response ( response ) @output_supported ({ Output . JSON , Output . CSV }) def bulk_lookup ( self , atom_values : list , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON , return_search_hashkey = False , ) -> Union [ dict , str ]: \"\"\" Look up multiple threats at once in the API, returning their ids and if they are present in Datalake. Compared to the lookup endpoint, it allow to lookup big batch of values faster as fewer API calls are made. However, fewer outputs types are supported as of now. \"\"\" aggregated_response = [] if output is Output . CSV else {} search_hashkey_list = [] for atom_values_batch in split_list ( atom_values , self . _NB_ATOMS_PER_BULK_LOOKUP ): batch_result = self . _bulk_lookup_batch ( atom_values_batch , atom_type , hashkey_only , output , return_search_hashkey ) if 'search_hashkey' in batch_result : search_hashkey_list . append ( batch_result . pop ( 'search_hashkey' )) aggregated_response = aggregate_csv_or_json_api_response ( aggregated_response , batch_result , ) if search_hashkey_list : aggregated_response [ 'search_hashkey' ] = search_hashkey_list if output is Output . CSV : aggregated_response = ' \\n ' . join ( aggregated_response ) # a string is expected for CSV output return aggregated_response @output_supported ({ Output . JSON , Output . CSV , Output . MISP , Output . STIX }) def lookup ( self , atom_value , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON ) -> dict : \"\"\" Look up a threat in the API, returning its id (called threat's hashkey) and if it is present in Datalake. The hashkey is also based on the threat type. If atom_type is not specified, another query to the API will be made to determine it. With hashkey_only = False, the full threat details are returned in the requested format. \"\"\" atom_type_str = None if not atom_type : threats = [ atom_value ] atoms_values_extractor_response = self . atom_values_extract ( threats ) if atoms_values_extractor_response [ 'found' ] > 0 : atom_type_str = list ( atoms_values_extractor_response [ 'results' ] . keys ())[ 0 ] else : raise ValueError ( 'atom could not be typed' ) elif not isinstance ( atom_type , AtomType ): raise ValueError ( f ' { atom_type } atom_type could not be treated' ) else : # atom_type is a valid enum passed by the user atom_type_str = atom_type . value url = self . _build_url_for_endpoint ( 'lookup' ) params = { 'atom_value' : atom_value , 'atom_type' : atom_type_str , 'hashkey_only' : hashkey_only } req = PreparedRequest () req . prepare_url ( url , params ) response = self . datalake_requests ( req . url , 'get' , self . _get_headers ( output = output )) return parse_response ( response ) def atom_values_extract ( self , untyped_atoms : List [ str ], treat_hashes_like = AtomType . FILE ) -> dict : \"\"\" Determine the types of atoms passed. values that are believed to be hashes will be returned as <treat_hashes_like> \"\"\" url = self . _build_url_for_endpoint ( 'atom-values-extract' ) payload = { 'content' : ' ' . join ( untyped_atoms ), 'treat_hashes_like' : treat_hashes_like . value } return self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) . json () def edit_score_by_hashkeys ( self , hashkeys , scores_list , override_type : OverrideType = OverrideType . TEMPORARY ): \"\"\" Edit the score of a list of threats using the API. Default is 100. This function will receive a list of hashkey to edit, a list of dictionaries defining the score the set and an override type. Can only process a limited number of hashkeys at one time. Can be modified with the environment variable OCD_DTL_MAX_EDIT_SCORE_HASHKEYS. \"\"\" if type ( hashkeys ) is not list or not hashkeys : raise ValueError ( 'Hashkeys has to be a list of string' ) if len ( hashkeys ) > self . OCD_DTL_MAX_EDIT_SCORE_HASHKEYS : raise ValueError ( f 'Can \\' t process more than { self . OCD_DTL_MAX_EDIT_SCORE_HASHKEYS } hashkeys at one time.' ) if any ( type ( hashkey ) is not str or not hashkey for hashkey in hashkeys ): raise ValueError ( 'Hashkeys has to be a list of string' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'hashkeys' : hashkeys , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response ) def edit_score_by_query_body_hash ( self , query_body_hash : str , scores_list : List [ ScoreMap ], override_type : OverrideType = OverrideType . TEMPORARY ): \"\"\" Edit the score of a list of threats using the API. This function will receive a query body hash, a list of dictonaries defining the score the set and an override type. \"\"\" if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'query_body_hash' : query_body_hash , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response ) @staticmethod def _build_scores ( scores_list : List [ ScoreMap ]): scores_body = [] for score_dict in scores_list : if not isinstance ( score_dict [ 'threat_type' ], ThreatType ): raise ValueError ( 'Invalid threat_type input' ) if score_dict [ 'score' ] > 100 or score_dict [ 'score' ] < 0 : raise ValueError ( 'Invalid score input, min: 0, max: 100' ) scores_body . append ({ 'score' : { 'risk' : score_dict [ 'score' ] }, 'threat_type' : score_dict [ 'threat_type' ] . value }) return scores_body @staticmethod def _build_whitelist_scores (): scores = [] for threat in ThreatType : scores . append ({ 'score' : { 'risk' : 0 }, 'threat_type' : threat . value }) return scores def add_threats ( self , atom_list : List [ str ], atom_type : AtomType , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , external_analysis_link : List = None , ): \"\"\" Add a list of threats to datalake using the API. The type of atom provided in the list of threats to add need to be the same, for example a list of IPs. \"\"\" self . check_add_threats_params ( atom_list , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field, default to an empty list payload = { 'override_type' : override_type . value , 'public' : public } if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) return self . _bulk_add_threat ( atom_list , atom_type , payload , tags , scores , external_analysis_link ) @staticmethod def check_add_threats_params ( atom_list , override_type , threat_types , whitelist ): if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) if any ( len ( atom ) < 1 for atom in atom_list ): raise ValueError ( 'Empty atom in atom_list' ) def _bulk_add_threat ( self , atom_list : List , atom_type : AtomType , payload : Dict , tags : List , scores : List [ Dict ], external_analysis_link : List = None ): url = self . _build_url_for_endpoint ( 'threats-manual-bulk' ) payload [ 'atom_type' ] = atom_type . value payload [ 'scores' ] = scores payload [ 'tags' ] = tags if external_analysis_link : payload [ 'threat_data' ] = { 'content' : { f ' { atom_type . value } _content' : { 'external_analysis_link' : external_analysis_link } } } hashkey_created = self . queue_bulk_threats ( atom_list , payload , url ) return hashkey_created def queue_bulk_threats ( self , atom_list , payload , url ): bulk_response = [] bulk_in_flight = [] # bulk task uuid unchecked failed_batch = [] for batch in split_list ( atom_list , 100 ): if len ( bulk_in_flight ) >= self . OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT : bulk_threat_task_uuid = bulk_in_flight . pop ( 0 ) bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) payload [ 'atom_values' ] = ' \\n ' . join ( batch ) # Raw csv expected response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) response = parse_response ( response ) task_uid = response . get ( 'task_uuid' ) if task_uid : bulk_in_flight . append ( response [ 'task_uuid' ]) else : failed_batch . append ( batch ) if failed_batch : bulk_response . append ({ 'success' : { 'created_hashkeys' : [], 'created_atom_values' : [] }, 'failed' : { 'failed_hashkeys' : [], 'failed_atom_values' : failed_batch } }) # Finish to check the other bulk tasks for bulk_threat_task_uuid in bulk_in_flight : bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) return bulk_response def check_bulk_threats_added ( self , bulk_threat_task_uuid ) -> dict : \"\"\"Check if the bulk manual threat submission completed successfully and if so return the hashkeys created\"\"\" success = [] failed = [] url = self . _build_url_for_endpoint ( 'retrieve-threats-manual-bulk' ) try : response = self . _handle_bulk_task ( bulk_threat_task_uuid , url , timeout = self . OCD_DTL_MAX_BULK_THREATS_TIME , ) except TimeoutError : response = {} hashkeys = response . get ( 'hashkeys' ) atom_values = response . get ( 'atom_values' ) if hashkeys and response . get ( 'state' , 'CANCELLED' ) == 'DONE' : hashkey_created = [] atom_val_created = [] for hashkey in hashkeys : hashkey_created . append ( hashkey ) for atom_value in atom_values : atom_val_created . append ( atom_value ) success . append ({ 'created_hashkeys' : hashkey_created , 'created_atom_values' : atom_val_created }) else : # if the state is not DONE we consider the batch a failure # default values in case the json is missing some fields hashkeys = hashkeys or [ '<missing value>' ] atom_values = atom_values or [ '<missing value>' ] failed . append ({ 'failed_hashkeys' : hashkeys , 'failed_atom_values' : atom_values }) response_dict = { 'success' : success , 'failed' : failed } return response_dict def _handle_bulk_task ( self , task_uuid , retrieve_bulk_result_url , timeout ): retrieve_bulk_result_url = retrieve_bulk_result_url . format ( task_uuid = task_uuid ) start_time = time () back_off_time = 1 json_response = None while not json_response : response = self . datalake_requests ( retrieve_bulk_result_url , 'get' , self . _get_headers ()) if response . status_code == 200 : potential_json_response = response . json () if not potential_json_response [ 'state' ] in ( 'DONE' , 'CANCELLED' ): continue json_response = response . json () elif time () - start_time + back_off_time < timeout : sleep ( back_off_time ) back_off_time = min ( back_off_time * 2 , self . OCD_DTL_MAX_BACK_OFF_TIME ) else : raise TimeoutError ( f 'No bulk result after waiting { timeout / 60 : .0f } mins \\n ' f 'task_uuid: \" { task_uuid } \"' ) return json_response @staticmethod def check_add_threat_params ( atom , override_type , threat_types , whitelist ): if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) if type ( atom ) == Atom or not isinstance ( atom , Atom ): raise TypeError ( \"atom needs to be an Atom subclasses.\" ) def add_threat ( self , atom : Atom , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , ): \"\"\" Add a single threat to datalake using the API. This method is slower than add_threats for submitting a large number of threats but allows to provide greater details \"\"\" self . check_add_threat_params ( atom , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field, default to an empty list if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) payload = { 'override_type' : override_type . value , 'public' : public , 'threat_data' : { 'scores' : scores , 'tags' : tags , 'content' : atom . generate_atom_json (), } } url = self . _build_url_for_endpoint ( 'threats-manual' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response )","title":"Module datalake.endpoints.threats"},{"location":"reference/datalake/endpoints/threats/#classes","text":"","title":"Classes"},{"location":"reference/datalake/endpoints/threats/#threats","text":"class Threats ( endpoint_config : dict , environment : str , token_manager : datalake . common . token_manager . TokenManager ) View Source class Threats ( Endpoint ): _NB_ATOMS_PER_BULK_LOOKUP = 100 OCD_DTL_MAX_BULK_THREATS_TIME = int ( os . getenv ( 'OCD_DTL_MAX_BULK_THREATS_TIME' , 600 )) OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT = int ( os . getenv ( 'OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT' , 10 )) OCD_DTL_MAX_BACK_OFF_TIME = float ( os . getenv ( 'OCD_DTL_MAX_BACK_OFF_TIME' , 30 )) OCD_DTL_MAX_EDIT_SCORE_HASHKEYS = int ( os . getenv ( 'OCD_DTL_MAX_EDIT_SCORE_HASHKEYS' , 100 )) def _bulk_lookup_batch ( self , atom_values : list , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON , return_search_hashkey = False , ) -> dict : \"\"\"Bulk lookup done on maximum _NB_ATOMS_PER_BULK_LOOKUP atoms\"\"\" typed_atoms = {} if not atom_type : atoms_values_extractor_response = self . atom_values_extract ( atom_values ) if atoms_values_extractor_response [ 'found' ] > 0 : typed_atoms = atoms_values_extractor_response [ 'results' ] else : raise ValueError ( 'none of your atoms could be typed' ) elif not isinstance ( atom_type , AtomType ): raise ValueError ( f '{atom_type} atom_type could not be treated' ) else : typed_atoms [ atom_type . value ] = atom_values body : dict = typed_atoms body [ 'hashkey_only' ] = hashkey_only body [ 'return_search_hashkey' ] = return_search_hashkey url = self . _build_url_for_endpoint ( 'threats-bulk-lookup' ) response = self . datalake_requests ( url , 'post' , self . _post_headers ( output = output ), body ) return parse_response ( response ) @ output_supported ({ Output . JSON , Output . CSV }) def bulk_lookup ( self , atom_values : list , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON , return_search_hashkey = False , ) -> Union [ dict , str ]: \"\"\" Look up multiple threats at once in the API, returning their ids and if they are present in Datalake. Compared to the lookup endpoint, it allow to lookup big batch of values faster as fewer API calls are made. However, fewer outputs types are supported as of now. \"\"\" aggregated_response = [] if output is Output . CSV else {} search_hashkey_list = [] for atom_values_batch in split_list ( atom_values , self . _NB_ATOMS_PER_BULK_LOOKUP ): batch_result = self . _bulk_lookup_batch ( atom_values_batch , atom_type , hashkey_only , output , return_search_hashkey ) if 'search_hashkey' in batch_result : search_hashkey_list . append ( batch_result . pop ( 'search_hashkey' )) aggregated_response = aggregate_csv_or_json_api_response ( aggregated_response , batch_result , ) if search_hashkey_list : aggregated_response [ 'search_hashkey' ] = search_hashkey_list if output is Output . CSV : aggregated_response = ' \\n ' . join ( aggregated_response ) # a string is expected for CSV output return aggregated_response @ output_supported ({ Output . JSON , Output . CSV , Output . MISP , Output . STIX }) def lookup ( self , atom_value , atom_type : AtomType = None , hashkey_only = False , output = Output . JSON ) -> dict : \"\"\" Look up a threat in the API, returning its id (called threat's hashkey) and if it is present in Datalake. The hashkey is also based on the threat type. If atom_type is not specified, another query to the API will be made to determine it. With hashkey_only = False, the full threat details are returned in the requested format. \"\"\" atom_type_str = None if not atom_type : threats = [ atom_value ] atoms_values_extractor_response = self . atom_values_extract ( threats ) if atoms_values_extractor_response [ 'found' ] > 0 : atom_type_str = list ( atoms_values_extractor_response [ 'results' ] . keys ())[ 0 ] else : raise ValueError ( 'atom could not be typed' ) elif not isinstance ( atom_type , AtomType ): raise ValueError ( f '{atom_type} atom_type could not be treated' ) else : # atom_type is a valid enum passed by the user atom_type_str = atom_type . value url = self . _build_url_for_endpoint ( 'lookup' ) params = { 'atom_value' : atom_value , 'atom_type' : atom_type_str , 'hashkey_only' : hashkey_only } req = PreparedRequest () req . prepare_url ( url , params ) response = self . datalake_requests ( req . url , 'get' , self . _get_headers ( output = output )) return parse_response ( response ) def atom_values_extract ( self , untyped_atoms : List [ str ], treat_hashes_like = AtomType . FILE ) -> dict : \"\"\" Determine the types of atoms passed. values that are believed to be hashes will be returned as <treat_hashes_like> \"\"\" url = self . _build_url_for_endpoint ( 'atom-values-extract' ) payload = { 'content' : ' ' . join ( untyped_atoms ), 'treat_hashes_like' : treat_hashes_like . value } return self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) . json () def edit_score_by_hashkeys ( self , hashkeys , scores_list , override_type : OverrideType = OverrideType . TEMPORARY ): \"\"\" Edit the score of a list of threats using the API. Default is 100. This function will receive a list of hashkey to edit, a list of dictionaries defining the score the set and an override type. Can only process a limited number of hashkeys at one time. Can be modified with the environment variable OCD_DTL_MAX_EDIT_SCORE_HASHKEYS. \"\"\" if type ( hashkeys ) is not list or not hashkeys : raise ValueError ( 'Hashkeys has to be a list of string' ) if len ( hashkeys ) > self . OCD_DTL_MAX_EDIT_SCORE_HASHKEYS : raise ValueError ( f 'Can \\' t process more than {self.OCD_DTL_MAX_EDIT_SCORE_HASHKEYS} hashkeys at one time.' ) if any ( type ( hashkey ) is not str or not hashkey for hashkey in hashkeys ): raise ValueError ( 'Hashkeys has to be a list of string' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'hashkeys' : hashkeys , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response ) def edit_score_by_query_body_hash ( self , query_body_hash : str , scores_list : List [ ScoreMap ], override_type : OverrideType = OverrideType . TEMPORARY ): \"\"\" Edit the score of a list of threats using the API. This function will receive a query body hash, a list of dictonaries defining the score the set and an override type. \"\"\" if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'query_body_hash' : query_body_hash , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response ) @ staticmethod def _build_scores ( scores_list : List [ ScoreMap ]): scores_body = [] for score_dict in scores_list : if not isinstance ( score_dict [ 'threat_type' ], ThreatType ): raise ValueError ( 'Invalid threat_type input' ) if score_dict [ 'score' ] > 100 or score_dict [ 'score' ] < 0 : raise ValueError ( 'Invalid score input, min: 0, max: 100' ) scores_body . append ({ 'score' : { 'risk' : score_dict [ 'score' ] }, 'threat_type' : score_dict [ 'threat_type' ] . value }) return scores_body @ staticmethod def _build_whitelist_scores (): scores = [] for threat in ThreatType : scores . append ({ 'score' : { 'risk' : 0 }, 'threat_type' : threat . value }) return scores def add_threats ( self , atom_list : List [ str ], atom_type : AtomType , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , external_analysis_link : List = None , ): \"\"\" Add a list of threats to datalake using the API. The type of atom provided in the list of threats to add need to be the same, for example a list of IPs. \"\"\" self . check_add_threats_params ( atom_list , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field, default to an empty list payload = { 'override_type' : override_type . value , 'public' : public } if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) return self . _bulk_add_threat ( atom_list , atom_type , payload , tags , scores , external_analysis_link ) @ staticmethod def check_add_threats_params ( atom_list , override_type , threat_types , whitelist ): if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) if any ( len ( atom ) < 1 for atom in atom_list ): raise ValueError ( 'Empty atom in atom_list' ) def _bulk_add_threat ( self , atom_list : List , atom_type : AtomType , payload : Dict , tags : List , scores : List [ Dict ], external_analysis_link : List = None ): url = self . _build_url_for_endpoint ( 'threats-manual-bulk' ) payload [ 'atom_type' ] = atom_type . value payload [ 'scores' ] = scores payload [ 'tags' ] = tags if external_analysis_link : payload [ 'threat_data' ] = { 'content' : { f '{atom_type.value}_content' : { 'external_analysis_link' : external_analysis_link } } } hashkey_created = self . queue_bulk_threats ( atom_list , payload , url ) return hashkey_created def queue_bulk_threats ( self , atom_list , payload , url ): bulk_response = [] bulk_in_flight = [] # bulk task uuid unchecked failed_batch = [] for batch in split_list ( atom_list , 100 ): if len ( bulk_in_flight ) >= self . OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT : bulk_threat_task_uuid = bulk_in_flight . pop ( 0 ) bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) payload [ 'atom_values' ] = ' \\n ' . join ( batch ) # Raw csv expected response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) response = parse_response ( response ) task_uid = response . get ( 'task_uuid' ) if task_uid : bulk_in_flight . append ( response [ 'task_uuid' ]) else : failed_batch . append ( batch ) if failed_batch : bulk_response . append ({ 'success' : { 'created_hashkeys' : [], 'created_atom_values' : [] }, 'failed' : { 'failed_hashkeys' : [], 'failed_atom_values' : failed_batch } }) # Finish to check the other bulk tasks for bulk_threat_task_uuid in bulk_in_flight : bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) return bulk_response def check_bulk_threats_added ( self , bulk_threat_task_uuid ) -> dict : \"\"\"Check if the bulk manual threat submission completed successfully and if so return the hashkeys created\"\"\" success = [] failed = [] url = self . _build_url_for_endpoint ( 'retrieve-threats-manual-bulk' ) try : response = self . _handle_bulk_task ( bulk_threat_task_uuid , url , timeout = self . OCD_DTL_MAX_BULK_THREATS_TIME , ) except TimeoutError : response = {} hashkeys = response . get ( 'hashkeys' ) atom_values = response . get ( 'atom_values' ) if hashkeys and response . get ( 'state' , 'CANCELLED' ) == 'DONE' : hashkey_created = [] atom_val_created = [] for hashkey in hashkeys : hashkey_created . append ( hashkey ) for atom_value in atom_values : atom_val_created . append ( atom_value ) success . append ({ 'created_hashkeys' : hashkey_created , 'created_atom_values' : atom_val_created }) else : # if the state is not DONE we consider the batch a failure # default values in case the json is missing some fields hashkeys = hashkeys or [ '<missing value>' ] atom_values = atom_values or [ '<missing value>' ] failed . append ({ 'failed_hashkeys' : hashkeys , 'failed_atom_values' : atom_values }) response_dict = { 'success' : success , 'failed' : failed } return response_dict def _handle_bulk_task ( self , task_uuid , retrieve_bulk_result_url , timeout ): retrieve_bulk_result_url = retrieve_bulk_result_url . format ( task_uuid = task_uuid ) start_time = time () back_off_time = 1 json_response = None while not json_response : response = self . datalake_requests ( retrieve_bulk_result_url , 'get' , self . _get_headers ()) if response . status_code == 200 : potential_json_response = response . json () if not potential_json_response [ 'state' ] in ( 'DONE' , 'CANCELLED' ): continue json_response = response . json () elif time () - start_time + back_off_time < timeout : sleep ( back_off_time ) back_off_time = min ( back_off_time * 2 , self . OCD_DTL_MAX_BACK_OFF_TIME ) else : raise TimeoutError ( f 'No bulk result after waiting {timeout / 60:.0f} mins \\n ' f 'task_uuid: \"{task_uuid}\"' ) return json_response @ staticmethod def check_add_threat_params ( atom , override_type , threat_types , whitelist ): if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) if type ( atom ) == Atom or not isinstance ( atom , Atom ): raise TypeError ( \"atom needs to be an Atom subclasses.\" ) def add_threat ( self , atom : Atom , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , ): \"\"\" Add a single threat to datalake using the API. This method is slower than add_threats for submitting a large number of threats but allows to provide greater details \"\"\" self . check_add_threat_params ( atom , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field, default to an empty list if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) payload = { 'override_type' : override_type . value , 'public' : public , 'threat_data' : { 'scores' : scores , 'tags' : tags , 'content' : atom . generate_atom_json (), } } url = self . _build_url_for_endpoint ( 'threats-manual' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response )","title":"Threats"},{"location":"reference/datalake/endpoints/threats/#ancestors-in-mro","text":"datalake.endpoints.endpoint.Endpoint","title":"Ancestors (in MRO)"},{"location":"reference/datalake/endpoints/threats/#class-variables","text":"OCD_DTL_MAX_BACK_OFF_TIME OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT OCD_DTL_MAX_BULK_THREATS_TIME OCD_DTL_MAX_EDIT_SCORE_HASHKEYS OCD_DTL_QUOTA_TIME OCD_DTL_REQUESTS_PER_QUOTA_TIME SET_MAX_RETRY","title":"Class variables"},{"location":"reference/datalake/endpoints/threats/#static-methods","text":"","title":"Static methods"},{"location":"reference/datalake/endpoints/threats/#check_add_threat_params","text":"def check_add_threat_params ( atom , override_type , threat_types , whitelist ) View Source @staticmethod def check_add_threat_params ( atom , override_type , threat_types , whitelist ) : if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ) : raise ValueError ( 'Invalid OverrideType input' ) if type ( atom ) == Atom or not isinstance ( atom , Atom ) : raise TypeError ( \"atom needs to be an Atom subclasses.\" )","title":"check_add_threat_params"},{"location":"reference/datalake/endpoints/threats/#check_add_threats_params","text":"def check_add_threats_params ( atom_list , override_type , threat_types , whitelist ) View Source @staticmethod def check_add_threats_params ( atom_list , override_type , threat_types , whitelist ) : if not threat_types and not whitelist : raise ValueError ( 'threat_types is required if the atom is not for whitelisting' ) if not isinstance ( override_type , OverrideType ) : raise ValueError ( 'Invalid OverrideType input' ) if any ( len ( atom ) < 1 for atom in atom_list ) : raise ValueError ( 'Empty atom in atom_list' )","title":"check_add_threats_params"},{"location":"reference/datalake/endpoints/threats/#methods","text":"","title":"Methods"},{"location":"reference/datalake/endpoints/threats/#add_threat","text":"def add_threat ( self , atom : datalake . common . atom_type . Atom , threat_types : List [ Dict [ str , Union [ int , datalake . common . atom . ThreatType ]]] = None , override_type : datalake . common . atom . OverrideType = < OverrideType . TEMPORARY : 'temporary' > , whitelist : bool = False , public : bool = True , tags : List = None ) Add a single threat to datalake using the API. This method is slower than add_threats for submitting a large number of threats but allows to provide greater details View Source def add_threat ( self , atom : Atom , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , ) : \"\"\" Add a single threat to datalake using the API. This method is slower than add_threats for submitting a large number of threats but allows to provide greater details \"\"\" self . check_add_threat_params ( atom , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field , default to an empty list if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) payload = { 'override_type' : override_type . value , 'public' : public , 'threat_data' : { 'scores' : scores , 'tags' : tags , 'content' : atom . generate_atom_json (), } } url = self . _build_url_for_endpoint ( 'threats-manual' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) return parse_response ( response )","title":"add_threat"},{"location":"reference/datalake/endpoints/threats/#add_threats","text":"def add_threats ( self , atom_list : List [ str ], atom_type : datalake . common . atom . AtomType , threat_types : List [ Dict [ str , Union [ int , datalake . common . atom . ThreatType ]]] = None , override_type : datalake . common . atom . OverrideType = < OverrideType . TEMPORARY : 'temporary' > , whitelist : bool = False , public : bool = True , tags : List = None , external_analysis_link : List = None ) Add a list of threats to datalake using the API. The type of atom provided in the list of threats to add need to be the same, for example a list of IPs. View Source def add_threats ( self , atom_list : List [ str ] , atom_type : AtomType , threat_types : List [ ScoreMap ] = None , override_type : OverrideType = OverrideType . TEMPORARY , whitelist : bool = False , public : bool = True , tags : List = None , external_analysis_link : List = None , ) : \"\"\" Add a list of threats to datalake using the API. The type of atom provided in the list of threats to add need to be the same, for example a list of IPs. \"\"\" self . check_add_threats_params ( atom_list , override_type , threat_types , whitelist ) tags = tags or [] # API requires a tag field , default to an empty list payload = { 'override_type' : override_type . value , 'public' : public } if whitelist : scores = self . _build_whitelist_scores () else : scores = self . _build_scores ( threat_types ) return self . _bulk_add_threat ( atom_list , atom_type , payload , tags , scores , external_analysis_link )","title":"add_threats"},{"location":"reference/datalake/endpoints/threats/#atom_values_extract","text":"def atom_values_extract ( self , untyped_atoms : List [ str ], treat_hashes_like =< AtomType . FILE : 'file' > ) -> dict Determine the types of atoms passed. values that are believed to be hashes will be returned as View Source def atom_values_extract ( self , untyped_atoms : List [ str ] , treat_hashes_like = AtomType . FILE ) -> dict : \"\"\" Determine the types of atoms passed. values that are believed to be hashes will be returned as <treat_hashes_like> \"\"\" url = self . _build_url_for_endpoint ( 'atom-values-extract' ) payload = { 'content' : ' ' . join ( untyped_atoms ), 'treat_hashes_like' : treat_hashes_like . value } return self . datalake_requests ( url , 'post' , self . _post_headers (), payload ). json ()","title":"atom_values_extract"},{"location":"reference/datalake/endpoints/threats/#bulk_lookup","text":"def bulk_lookup ( * args , ** kwargs ) View Source def wrapper ( * args , ** kwargs ) : if kwargs . get ( ' output ' ) and kwargs [ ' output ' ] not in outputs : raise ValueError ( f ' {kwargs[\"output\"]} output type is not supported. ' f ' Outputs supported are: {display_outputs(outputs)} ' ) return function ( * args , ** kwargs )","title":"bulk_lookup"},{"location":"reference/datalake/endpoints/threats/#check_bulk_threats_added","text":"def check_bulk_threats_added ( self , bulk_threat_task_uuid ) -> dict Check if the bulk manual threat submission completed successfully and if so return the hashkeys created View Source def check_bulk_threats_added ( self , bulk_threat_task_uuid ) -> dict : \"\"\"Check if the bulk manual threat submission completed successfully and if so return the hashkeys created\"\"\" success = [] failed = [] url = self . _build_url_for_endpoint ( 'retrieve-threats-manual-bulk' ) try : response = self . _handle_bulk_task ( bulk_threat_task_uuid , url , timeout = self . OCD_DTL_MAX_BULK_THREATS_TIME , ) except TimeoutError : response = {} hashkeys = response . get ( 'hashkeys' ) atom_values = response . get ( 'atom_values' ) if hashkeys and response . get ( 'state' , 'CANCELLED' ) == 'DONE' : hashkey_created = [] atom_val_created = [] for hashkey in hashkeys : hashkey_created . append ( hashkey ) for atom_value in atom_values : atom_val_created . append ( atom_value ) success . append ({ 'created_hashkeys' : hashkey_created , 'created_atom_values' : atom_val_created }) else : # if the state is not DONE we consider the batch a failure # default values in case the json is missing some fields hashkeys = hashkeys or [ '<missing value>' ] atom_values = atom_values or [ '<missing value>' ] failed . append ({ 'failed_hashkeys' : hashkeys , 'failed_atom_values' : atom_values }) response_dict = { 'success' : success , 'failed' : failed } return response_dict","title":"check_bulk_threats_added"},{"location":"reference/datalake/endpoints/threats/#datalake_requests","text":"def datalake_requests ( * args , ** kwargs ) View Source def wrapped ( * args , ** kwargs ) : previous_call = THROTTLE_QUEUE . get ( f , [] ) call_time = time () # Clear calls made older than the given period while previous_call and previous_call [ 0 ] + period <= call_time : previous_call . pop ( 0 ) # Check if the number of call for the period don 't allow the function to be called if len(previous_call) >= call_per_period: time_since_first_call = (call_time - previous_call[0]) sleep(max(period - time_since_first_call, 0.1)) # Wait until a call has been made ' period ' ago assert previous_call [ 0 ] + period <= time () previous_call . pop ( 0 ) previous_call . append ( time ()) THROTTLE_QUEUE [ f ] = previous_call response = f ( * args , ** kwargs ) return response","title":"datalake_requests"},{"location":"reference/datalake/endpoints/threats/#edit_score_by_hashkeys","text":"def edit_score_by_hashkeys ( self , hashkeys , scores_list , override_type : datalake . common . atom . OverrideType = < OverrideType . TEMPORARY : 'temporary' > ) Edit the score of a list of threats using the API. Default is 100. This function will receive a list of hashkey to edit, a list of dictionaries defining the score the set and an override type. Can only process a limited number of hashkeys at one time. Can be modified with the environment variable OCD_DTL_MAX_EDIT_SCORE_HASHKEYS. View Source def edit_score_by_hashkeys ( self , hashkeys , scores_list , override_type : OverrideType = OverrideType . TEMPORARY ): \"\"\" Edit the score of a list of threats using the API. Default is 100. This function will receive a list of hashkey to edit, a list of dictionaries defining the score the set and an override type. Can only process a limited number of hashkeys at one time. Can be modified with the environment variable OCD_DTL_MAX_EDIT_SCORE_HASHKEYS. \"\"\" if type ( hashkeys ) is not list or not hashkeys : raise ValueError ( 'Hashkeys has to be a list of string' ) if len ( hashkeys ) > self . OCD_DTL_MAX_EDIT_SCORE_HASHKEYS : raise ValueError ( f 'Can \\' t process more than {self.OCD_DTL_MAX_EDIT_SCORE_HASHKEYS} hashkeys at one time.' ) if any ( type ( hashkey ) is not str or not hashkey for hashkey in hashkeys ): raise ValueError ( 'Hashkeys has to be a list of string' ) if not isinstance ( override_type , OverrideType ): raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'hashkeys' : hashkeys , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response )","title":"edit_score_by_hashkeys"},{"location":"reference/datalake/endpoints/threats/#edit_score_by_query_body_hash","text":"def edit_score_by_query_body_hash ( self , query_body_hash : str , scores_list : List [ Dict [ str , Union [ int , datalake . common . atom . ThreatType ]]], override_type : datalake . common . atom . OverrideType = < OverrideType . TEMPORARY : 'temporary' > ) Edit the score of a list of threats using the API. This function will receive a query body hash, a list of dictonaries defining the score the set and an override type. View Source def edit_score_by_query_body_hash ( self , query_body_hash : str , scores_list : List [ ScoreMap ] , override_type : OverrideType = OverrideType . TEMPORARY ) : \"\"\" Edit the score of a list of threats using the API. This function will receive a query body hash, a list of dictonaries defining the score the set and an override type. \"\"\" if not isinstance ( override_type , OverrideType ) : raise ValueError ( 'Invalid OverrideType input' ) req_body = { 'override_type' : override_type . value , 'query_body_hash' : query_body_hash , 'scores' : self . _build_scores ( scores_list ) } url = self . _build_url_for_endpoint ( 'bulk-scoring-edits' ) response = self . datalake_requests ( url , 'post' , self . _post_headers (), req_body ) return parse_response ( response )","title":"edit_score_by_query_body_hash"},{"location":"reference/datalake/endpoints/threats/#lookup","text":"def lookup ( * args , ** kwargs ) View Source def wrapper ( * args , ** kwargs ) : if kwargs . get ( ' output ' ) and kwargs [ ' output ' ] not in outputs : raise ValueError ( f ' {kwargs[\"output\"]} output type is not supported. ' f ' Outputs supported are: {display_outputs(outputs)} ' ) return function ( * args , ** kwargs )","title":"lookup"},{"location":"reference/datalake/endpoints/threats/#queue_bulk_threats","text":"def queue_bulk_threats ( self , atom_list , payload , url ) View Source def queue_bulk_threats ( self , atom_list , payload , url ): bulk_response = [] bulk_in_flight = [] # bulk task uuid unchecked failed_batch = [] for batch in split_list ( atom_list , 100 ): if len ( bulk_in_flight ) >= self . OCD_DTL_MAX_BULK_THREATS_IN_FLIGHT : bulk_threat_task_uuid = bulk_in_flight . pop ( 0 ) bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) payload [ 'atom_values' ] = ' \\n ' . join ( batch ) # Raw csv expected response = self . datalake_requests ( url , 'post' , self . _post_headers (), payload ) response = parse_response ( response ) task_uid = response . get ( 'task_uuid' ) if task_uid : bulk_in_flight . append ( response [ 'task_uuid' ]) else : failed_batch . append ( batch ) if failed_batch : bulk_response . append ({ 'success' : { 'created_hashkeys' : [], 'created_atom_values' : [] }, 'failed' : { 'failed_hashkeys' : [], 'failed_atom_values' : failed_batch } }) # Finish to check the other bulk tasks for bulk_threat_task_uuid in bulk_in_flight : bulk_response . append ( self . check_bulk_threats_added ( bulk_threat_task_uuid )) return bulk_response","title":"queue_bulk_threats"}]}